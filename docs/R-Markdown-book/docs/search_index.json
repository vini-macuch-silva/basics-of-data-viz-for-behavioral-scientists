[["basics-of-data-wrangling-in-r.html", "Basics of data visualization for behavioral scientists Session 1 Basics of data wrangling in R 1.1 Installing R and RStudio 1.2 Creating data structures 1.3 Subsetting and manipulating data 1.4 Session summary 1.5 Exercises", " Basics of data visualization for behavioral scientists Vinicius Macuch Silva Session 1 Basics of data wrangling in R 1.1 Installing R and RStudio We will start by installing R  which is the programming language we will be using to learn how to visualize data  as well as RStudio  which is an integrated development environment for R. In case youre not sure what the difference between one thing and the other is, think of R as the actual computer or engine behind all the calculations and operations you will be performing, while RStudio is the beautiful and very helpful interface that makes it all much more enjoyable and streamlined as a programming/ analysis experience. To install R, go to the following webpage &gt; https://cloud.r-project.org/ &lt; and select your operating system as well as your R version. As the page will tell you, you want to go for base R if this is your first install. Once youve installed R, open it and lets get familiarized with some basic concepts and operations, as well as with the way R syntax generally works. R serves first and foremost as a computing tool, that is, as a tool for calculating things. To get started, type in your R console 2 + 2, as shown in the code box below, and see what happens. To run the code in R, either press Enter or select the bit of code and press Ctrl + Enter. Below the inputted code you should see the result of your operation, which in this case is straightforward and not in any sense unexpected. 2 + 2 ## [1] 4 In R you are able to perform the same arithmetic operations youd be able to perform in any programming language, such as subtraction 2 - 2, multiplication 2 * 2, or division 2 / 2. You are also able to use logical operators to check whether a given relation between two or more elements is true or false. Suppose you want to know whether 2 is larger than 3, you can check that by running the code below. 2 &gt; 3 ## [1] FALSE Unsurprisingly, the output of the code above tells us that 2 is not greater than 3. Other important logical operations include checking whether two elements are equal to one another 2 == 3, not equal to another 2 != 3, or whether the relation between multiple sets of elements or multiple individual operations is true or false. For instance, notice the difference between the output of the two code snippets below. 2 &gt;= 3 | 3 &gt;= 3 ## [1] TRUE 2 &gt;= 3 &amp; 3 &gt;= 3 ## [1] FALSE While in the first case the output of the joint logical operation is true, in the second case it is false, and this has to do with the usage of the operators | and &amp;. What these operators do is to evaluate different combinations of individual outputs. In both cases, the individual operations which get evaluated consist of checking whether a given value is equal to or higher than 3. | indicates that the output of either one or the other individual operation should be equal to or greater than 3, while &amp; indicates that the output of both individual operations should be equal to or greater than 3. These sorts of logical operations will come in very handy later when we manipulate data structures consisting of lists of several values. Now, other than performing basic operations such as the ones above, more importantly for our purposes we will use R to manipulate and organize data in ways which help us discover and highlight patterns which we deem interesting as data analysts. More than that, we will use R to create visual representations of data so as to help us communicate any interesting data patterns to an external audience. In this course, we will learn how to communicate data to an audience which is academic or to some extent or another familiar with the goals and interests of scientific communication. But first lets learn some more about the basics in R. This is a good point to bring in RStudio, which will only enhance our experience as R data analysts. You can download RStudio here &gt; https://rstudio.com/products/rstudio/download/. Note that you need to have base R installed to be able to run RStudio. Once youve installed RStudio, lets have a look at the interface it offers us. RStudio interface In a nutshell, RStudio has 4 panels or regions of interest, as shown in the picture above. At the top left you have the region where you will usually have an R script, which is the document in which you type and record your R commands, the bits of code that will generate an output after the R engine has done its magic. This panel need not necessarily house a plain code script, as we will see it can also contain a document which gets converted into a reader-friendly file, such as a PDF or an HTML. But thats where you will type things and organize the logic of your analyses/ programs. Right below that is the RStudio console, which, much like the console in base R, showcases the output of whatever chunks of code you run. Any time you ask R to output some data summary or to run an operation this is where you will see it, usually in a condensed format given the running window character of the panel. Use the console to inspect what your code does but do not type your commands straight into it as the console will not save anything that is displayed in it. At the bottom right you have a multi-purpose panel which is usually used to view any graphical representations of your data, your plots, but also to view and locate any files or to read the always-handy entries from the different sources of R documentation. Last, at the top right you will find a panel where you can see any data structures you create or load during your R sessions, including data sets, R variables, temporary functions and shortcuts you create and much more. This is where most of your data management will occur, so eventually you will learn to keep your environment minimally clean and tidy. 1.2 Creating data structures Now that we know where to write code in RStudio and where to look for the output and representation of our operations, lets have a look at how we can create and manipulate different types of data in R. Lets bring back our basic arithmetic operation from above, 2 + 2. Its fine simply running the operation as this chunk of code currently does, but usually you will want to access the output of one operation and feed it to another operation, or maybe you will want to store it locally for purposes of record-keeping and data management. Lets then store the result of 2 + 2 in a new variable, which we can call op1, short for operation one. You can do that in R by assigning an operation or an already existing code output to a label, an unique identifier which stands for your newly created variable. Simply type the name of your new variable followed by an arrow and the operation or code output you wish to assign to it. op1 &lt;- 2 + 2 Now, in order to check if the assignment was sucessful, you can try calling your new variable. Type in op1 and run the code to see what happens. You should see the result of the operation 2 + 2, which is what you stored under the label op1. Notice that calling op1 only shows the actual output of the operation, and not the source of that output. This is an important aspect of variable assignment. op1 ## [1] 4 Before we move on to more interesting cases of data manipulation, lets have a look at what happens if you try to manipulate the variable you created. Theres two things we can do here. We can use op1 in a new operation without modifying the variable itself, which works by using the value associated with op1 without changing its representation: op1 + 2 ## [1] 6 Alternatively, we can store the result of our manipulation of op1 in a new variable, which we can call op2. Now we have two variables which we can manipulate and call back whenever we want. Notice how those two variables are now visible in your environment panel on the right, which means that they are retrievable within your current R session. If you wish to store your progress for future work, however, you should save an R script with the code used to create op1 and op2. When you open that script in a future R session and run the respective code, the variables will be created anew and will become visible in your environment panel. op2 &lt;- op1 + 2 op2 ## [1] 6 As you can imagine, storing the output of simple operations like the one above might not be that useful in itself. Usually, you will want to manipulate more complex variables consisting of lists of several elements, for instance several numbers or tokens of a given category. Lets create a list of numbers and do some more basic calculations with it. To create a list of elements, called a vector in R, you can use the command c() as shown below: numbers &lt;- c(2, 3, 5, 9) numbers ## [1] 2 3 5 9 As per usual, calling your variable shows the elements stored in it. In our toy example, this is a vector with four numbers. You can inspect any R object, whether a single variable or more complex data structures as we will construct later, by using the function str(). Try that yourself and see what it tells you about the vector numbers. str(numbers) ## num [1:4] 2 3 5 9 str() is useful in two ways: first, it tells you what type of an object your variable is. This is something that you will usually know if you create the variable yourself  in fact, it is something that you should know, as it will make your life easier later on  but in case you dont know or cant remember, str() is a good method of finding it out. Then, str() will also show you the length of your object, in this case a vector of length 4, i.e. a vector composed of 4 elements. Lets turn our focus to object types. In the case of numbers, str() diagnoses the vector as being numeric, which of course means that it is composed of numbers. But numeric is not the only vector type in R, there is in fact a number of other vector types, all of which are primitive data structures in R, which is to say that they act as building blocks for more complex data structures. The most important primitives for us are: Logical vectors, which contain elements that are marked as either true or false, most likely stored as the output of some logical operation or as a reference to a binary category (lv &lt;- c(TRUE, FALSE, FALSE, TRUE)); Numeric vectors, which contain elements that are numbers, including numbers with decimals (nv &lt;- c(10, 15.6, 4, 7.9)); Integer vectors, which contain elements that are whole numbers (iv &lt;- c(7, 15, 2, 57)); Character vectors, which contain elements that are strings of text, including orthographic representations of numbers and Arabic numerals without the underlying number representation in R (cv &lt;- c(\"butterfly\", \"5\", \"four\", \"false\")). One important and useful characteristic of vectors in R is that they can be converted into a nominal variable regardless of the actual vector type. Factors as they are called in R are vectors that have labels associated with each unique element stored in them. This might not seem particularly useful at this point, but it will be when we start dealing with meaningful data. For now lets just remember that a factor consists of a mapping between nominal values, which are arbitrary labels, and a set of elements. Notice that there might be more elements than unique labels, depending on the number of repeated elements. The code below converts numbers into a factor and checks the structure of the converted vector. numbers &lt;- as.factor(numbers) str(numbers) ## Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;5&quot;,&quot;9&quot;: 1 2 3 4 Vectors can be combined to create larger R objects. If multiple vectors of the same type are combined, the result is a matrix, a two-dimensional data structure with multiple rows and columns. If multiple vectors of different types are combined, the result is a data frame, which is also a two-dimensional data structure. Another useful type of data structure is an array, which is similar to a matrix, i.e. it consists of a single element type, but can have an n number of dimensions. The code below creates a new vector composed of letters, a character vector called letters. When you create a character vector, you need to use quotation marks to demarcate each individual element, as shown below. We then check the structure of the newly created vector in order to make sure that the object type is correct. Finally, we combine numbers and letters into a data frame which we call df1, and use str() to get some more details about the newly created data frame. letters &lt;- c(&quot;a&quot;, &quot;f&quot;, &quot;m&quot;, &quot;t&quot;) str(letters) ## chr [1:4] &quot;a&quot; &quot;f&quot; &quot;m&quot; &quot;t&quot; df1 &lt;- data.frame(numbers, letters) str(df1) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ numbers: Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;5&quot;,&quot;9&quot;: 1 2 3 4 ## $ letters: Factor w/ 4 levels &quot;a&quot;,&quot;f&quot;,&quot;m&quot;,&quot;t&quot;: 1 2 3 4 By inspecting our new data frame we can see that letters, originally a character vector, is now a factor, each individual letter consisting of a label, or as R calls it, a level, of its own. There is an explanation for this: when creating a data frame R converts character vectors to factors by default. This can lead to confusion later on when you manipulate your variable, now part of a data frame. In case the automatic conversion is unintended, you can avoid it by specifying the argument stringsAsFactors in the data.frame() function. NOTE: Starting with R version 4.0, released in June 2020, data.frame() does not convert character vectors to factors by default anymore. If you have installed R from scratch right before working on this session, you will be using R version 4.0 or newer, which means your default is stringsAsFactors = FALSE. Functions in R, like data.frame(), have arguments, which are slots that get filled with both mandatory and optional information. Think of arguments as switches that can be manipulated in order to produce slightly different outputs with the same function call. In the code below, we change stringAsFactors from its default TRUE to FALSE in order to avoid the conversion of our character vector. Notice the difference in the output. df1 &lt;- data.frame(numbers, letters, stringsAsFactors = FALSE) str(df1) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ numbers: Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;5&quot;,&quot;9&quot;: 1 2 3 4 ## $ letters: chr &quot;a&quot; &quot;f&quot; &quot;m&quot; &quot;t&quot; 1.3 Subsetting and manipulating data Having created our toy data frame df1 lets play around with it for a little while. Lets start by calling our data frame, this time without using the usual str() command. Given that df1 is a minuscule data set, it should be no problem visualizing it straight in the console. Alternatively, you could click on the data frame object in your environment panel to have a full view of the data, which should come in handy when we deal with larger data sets with dozens or hundreds of rows/ columns. df1 ## numbers letters ## 1 2 a ## 2 3 f ## 3 5 m ## 4 9 t As you can see, our two variables, numbers and letters, are organized as columns in the data frame, with the individual values organized vertically bellow their respective variable names. You have probably noticed this before, but data frames have a tabular format, which is only possible given that each of our variables has the exact same number of elements or values. Now suppose we want to add more information to our data set, suppose we want to record whether the values associated with our two existing variables are presented in the right order, numbers in ascending order and letters in alphabetical order. We can start by creating a variable called order_numb, which indicates whether the number value of a given observation is given in the correct ascending order. Lets then merge that with df1 and save the resulting data frame with the same name, df1. order_numb &lt;- c(TRUE, TRUE, TRUE, TRUE) df1 &lt;- data.frame(df1, order_numb) df1 ## numbers letters order_numb ## 1 2 a TRUE ## 2 3 f TRUE ## 3 5 m TRUE ## 4 9 t TRUE Now lets create a fourth variable called order_let, which indicates whether the letter value of a given observation is given in the correct alphabetical order. This time, however, lets use an alternative to merging the vector and the data frame together, which is what data.frame() does, and so lets add the vector order_let straight to the already existing df1 data frame. Notice the usage of the operator $. order_let &lt;- c(TRUE, TRUE, TRUE, TRUE) df1$order_let &lt;- order_let df1 ## numbers letters order_numb order_let ## 1 2 a TRUE TRUE ## 2 3 f TRUE TRUE ## 3 5 m TRUE TRUE ## 4 9 t TRUE TRUE We now have order_numb and order_let, two logical vectors, indicating whether the number and letter values of a given observation are ordered properly. However, if we think about it, these two variables are in a sense instances of the same superordinate variable, something we could call order. This is a very good moment to introduce the concept of tidy data, which will be very important for us as we get our hands on real data. The picture below illustrates what tidy data looks like. Tidy data As we can see, aside from each row reflecting an individual observation and each cell consisting of an individual value, in a tidy data set the columns relate to individual variables. That is not really the case in df1, if we consider that order_numb and order_let both relate to the same variable. Lets then change how our data frame is organized. For that, we will need special tools, which we will borrow from a very handy R toolkit known as the tidyverse. The tidyverse is a collection of packages  small extensions to base R which feature extended or improved function(alities)s  which greatly improve the tasks of everyday data analysis. Some of its packages, including dplyr, tidyr, and ggplot2, will be the bread and butter of practically any data analytic endeavor in R. To install the tidyverse, call install.packages(\"tidyverse\"). Once installed, any R package can be loaded by calling library(). Packages need only be installed once, however, each new session they need to be loaded anew if to be used in that session. A best practice in R programming is to load any necessary packages right at the beginning of an R session/ script, before they are actually needed later in the body of the script. For now, we will need the function pivot_longer() from the tidyr package, but we will load the tidyverse as a whole, which will prevent us from having to load other individual tidyverse packages later. Lets then use pivot_longer() to reshape our data frame, moving our two order_ variables and their respective values into a new arrangement that reflects the common basis between order_numb and order_let. pivot_longer() allows us to elongate our data set, increasing the number of rows and decreasing the number of columns. In this particular case, we end up with the same number of columns as we started, but had we had more than two order_ variables the resulting number of columns in df1_long would have been smaller than in df1. library(tidyverse) ## -- Attaching packages --------- tidyverse 1.3.0 -- ## v ggplot2 3.3.2 v purrr 0.3.4 ## v tibble 3.0.3 v dplyr 1.0.2 ## v tidyr 1.1.2 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.5.0 ## -- Conflicts ------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() df1_long &lt;- pivot_longer(df1, cols = starts_with(&quot;order&quot;), names_to = &quot;order&quot;, values_to = &quot;correct&quot;) df1_long ## # A tibble: 8 x 4 ## numbers letters order correct ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 2 a order_numb TRUE ## 2 2 a order_let TRUE ## 3 3 f order_numb TRUE ## 4 3 f order_let TRUE ## 5 5 m order_numb TRUE ## 6 5 m order_let TRUE ## 7 9 t order_numb TRUE ## 8 9 t order_let TRUE As you will notice as you start coding in R, usually there are multiple alternative ways of achieving the same desired end result. One alternative to our reshaping operation as coded above involves using the so-called pipe operator %&gt;%, which is one of the many handy tools that come with the tidyverse. If we inspect our code, more specifically the arguments in the pivot_longer() function, we see that the original data frame, df1, is called inside the function as its first argument. What that means is that pivot_longer needs to know which data frame is to be reshaped, the specific instructions of how exactly to reshape it being given in the arguments names_to and values_to. Now, in many functions you will need to specify which data source is the object of whatever operation the function is supposed to perform. If you are performing a single operation at a time, specifying the data source in the function argument is fine, however, if you are to perform several distinct operations, all sharing the same data source, chaining those operations might be a more convenient way of doing things. This is where the pipe operator comes in. Using %&gt;% to pipe a data source into one or more operations is a great way of saving yourself typing and writing a more fluid, though arguably less explicit, chunk of code. Run the code below and check whether the output differs from the code chunk above. df1_long &lt;- df1 %&gt;% pivot_longer(cols = starts_with(&quot;order&quot;), names_to = &quot;order&quot;, values_to = &quot;correct&quot;) df1_long ## # A tibble: 8 x 4 ## numbers letters order correct ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 2 a order_numb TRUE ## 2 2 a order_let TRUE ## 3 3 f order_numb TRUE ## 4 3 f order_let TRUE ## 5 5 m order_numb TRUE ## 6 5 m order_let TRUE ## 7 9 t order_numb TRUE ## 8 9 t order_let TRUE Now lets learn how to subset our data frame. Depending on what it is we want to do, we might want to extract a whole variable, a certain number of observations, or a single data value. Suppose we want to inspect the numbers variable without calling the whole data frame. We can that do by using either of the options below. df1_long$numbers ## [1] 2 2 3 3 5 5 9 9 ## Levels: 2 3 5 9 df1_long[, 1] ## # A tibble: 8 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 ## 3 3 ## 4 3 ## 5 5 ## 6 5 ## 7 9 ## 8 9 Note how the subsetted values are the same, though the output is presented in a slightly different way. In the first case we resorted to the $ operator, which we had used before already to add a variable to our data frame. Here, the $ operator is used to single out an individual variable from the data frame. In the second case we resorted to squared brackets [,], which are used to specify which rows and columns from our data frame, a tabular data structure, we want to extract. The 1 after the comma stands for the column: that tells R to extract the 1st column, i.e. the numbers column, from df1_long. Notice how before the comma nothing is specified: that tells R to extract all rows from the data frame. An alternative to that would be to explicitly specify rows 1 to 8 df1_long[1:8, 1], which would result in all 8 rows being extracted, exactly the same result as above. Now, suppose you only want to extract the first two rows of numbers. For that, we would specify our rows [1:2, x] and then our column [x, 1]. df1_long[1:2, 1] ## # A tibble: 2 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 What if we only want to extract a single value from df1_long? For that, we should specify the precise coordinates of that value, say 1st column and 4th row [4, 1]. df1_long[4, 1] ## # A tibble: 1 x 1 ## numbers ## &lt;fct&gt; ## 1 3 These are all the basic ways of extracting bits of data from a data table in R. In fact, these are the base R ways of doing it. But we also have the tidyverse tools at our disposal, which make some of these operations more streamlined. Lets repeat the same operations from the code chunks above but now using functions from the dplyr package. First, lets extract the numbers variable from df1_long, incorporating the %&gt;% operator as well. Notice how you can do that by either specifying the column number or the column name. df1_long %&gt;% select(1) ## # A tibble: 8 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 ## 3 3 ## 4 3 ## 5 5 ## 6 5 ## 7 9 ## 8 9 df1_long %&gt;% select(numbers) ## # A tibble: 8 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 ## 3 3 ## 4 3 ## 5 5 ## 6 5 ## 7 9 ## 8 9 You can, of course, also select more than a single variable at a time. Suppose we wanted to extract both numbers and letters. For that we could use either of the options below. df1_long %&gt;% select(1:2) ## # A tibble: 8 x 2 ## numbers letters ## &lt;fct&gt; &lt;chr&gt; ## 1 2 a ## 2 2 a ## 3 3 f ## 4 3 f ## 5 5 m ## 6 5 m ## 7 9 t ## 8 9 t df1_long %&gt;% select(numbers, letters) ## # A tibble: 8 x 2 ## numbers letters ## &lt;fct&gt; &lt;chr&gt; ## 1 2 a ## 2 2 a ## 3 3 f ## 4 3 f ## 5 5 m ## 6 5 m ## 7 9 t ## 8 9 t Now lets use a dplyr function to extract only values that meet a certain criterion. Say we want to extract only the observations that have a number value above 2. We can try the code below, using the filter() function. df1_long %&gt;% filter(numbers &gt; 2) ## Warning in Ops.factor(numbers, 2): &#39;&gt;&#39; not meaningful for factors ## # A tibble: 0 x 4 ## # ... with 4 variables: numbers &lt;fct&gt;, letters &lt;chr&gt;, order &lt;chr&gt;, ## # correct &lt;lgl&gt; This didnt work the way we wanted, but luckily R throws error warnings to help us understand what might have gone awry. According to the warning, the greater-than operator &gt; is not meaningful for factors. Lets try to make sense of that. First, recall that the numbers variable is a factor, which means that the numbers are only stored as labels and not as actual numeric representations. Now the operation we are trying to perform only makes sense when applied to numeric or integer vectors, given that a nominal value cannot be greater than another nominal value. Lets then try to convert numbers back to a numeric vector and attempt to perform the operation again. as.numeric(df1_long$numbers) ## [1] 1 1 2 2 3 3 4 4 Even before we have the chance to try the extraction operation again, we can see that something went wrong with the factor conversion. The resulting vector does not seem to show the original numeric values but rather the labels associated with them in the factor representation. If we look for help in the documentation on factor, we find the following note: as.numeric applied to a factor is meaningless, and may happen by implicit coercion. To transform a factor f to approximately its original numeric values, as.numeric(levels(f))[f] is recommended Lets then do what the R documentation recommends before we attempt to perform our original extraction operation again. as.numeric(levels(df1_long$numbers))[df1_long$numbers] ## [1] 2 2 3 3 5 5 9 9 df1_long$numbers &lt;- as.numeric(levels(df1_long$numbers))[df1_long$numbers] df1_long %&gt;% filter(numbers &gt; 2) ## # A tibble: 6 x 4 ## numbers letters order correct ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 3 f order_numb TRUE ## 2 3 f order_let TRUE ## 3 5 m order_numb TRUE ## 4 5 m order_let TRUE ## 5 9 t order_numb TRUE ## 6 9 t order_let TRUE Now the factor conversion works seamlessly, and so does the subsequent extraction operation. Our failed attempts should serve as a reminder that one should always know what type of data they are trying to manipulate. In many cases, we wont exactly know, for example, why a given operation wont work on a vector of type factor or numeric. Such situations will usually require some research about the exact operator or function were trying to use, but overall knowing how our variables of interest are stored and represented in R is a must to any process of data manipulation and analysis. This will be particularly true when the data we deal with have meaningful counterparts in a study design, such as a group of participants or a particular type of measurement. Lets try one more trick before summarizing what weve learned so far. Currently, numbers is organized in an ascending order. But what if we wanted to display our number values in a descending order? We could create a new numbers vector with the reverse order, and then replace the old vector with the new one, but that is a pretty roundabout solution. Instead, we can use the dplyr function arrange(), as shown below. df1_long %&gt;% arrange(desc(numbers)) ## # A tibble: 8 x 4 ## numbers letters order correct ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 9 t order_numb TRUE ## 2 9 t order_let TRUE ## 3 5 m order_numb TRUE ## 4 5 m order_let TRUE ## 5 3 f order_numb TRUE ## 6 3 f order_let TRUE ## 7 2 a order_numb TRUE ## 8 2 a order_let TRUE This seems to work just fine. Of course not always will we have a handy tidyverse function like arrange() to quickly solve our problems, but many recurring wrangling operations have been turned into functions to simplify data analysis in R. This is the exact reason why we use the tidyverse, as its packages make our job as data analysts much easier in many instances. This is not to say that one needs to rely on these packages and functions, in fact we can even create our own functions or use coding solutions that we might know from other programming languages. The point is to realize which options work best for our individual purposes, considering also the constrains of the data we have at hand. In the next session of our course, before we learn how to plot data, we will talk about how to map data reflecting some of the measures typically found in behavioral research onto the data structures we can create and manipulate in R. For now, lets recapitulate the key points that we learned in todays session. 1.4 Session summary In order to create a variable or assign something to an already existing variable we use the arrow operator &lt;-, e.g. op1 &lt;- 2 + 2; In order to create a vector we can use the function c(), where c() contains the values or elements to be created, e.g. nv &lt;- c(10, 15.6, 4, 7.9); In order to quickly gather information about a vector or any other data structure we can use the function str(), e.g. str(nv); In order to convert any vector into a factor, which stores individual values as unique labels, we can use the function as.factor(), e.g. as.factor(nv); In order to create a data frame, a tabular data structure made up of vectors of various types, we can use the function data.frame(), e.g. data.frame(nv, cv); We learned that we should strive to keep our data tidy. In order to do that, we can perform several operations, such as elongating a data table with pivot_longer, selecting and filtering variables with select() and filter(), or better arranging variables with arrange(); Finally, we learned that we can feed a data source into one or more operations using the pipe operator %&gt;%, e.g. nv %&gt;% filter( &gt; 4). 1.5 Exercises Convert df1_long back to its original, wide format. Try using the pivot_wider() function from the tidyr package. Remove the variable letters from df1. Try using select(), and for that inspect the function documentation using the Help tab on the lower right panel of RStudio. Find observations that have a numbers value above 4 and then add 4 to those values. Chain those two operations together using the pipe %&gt;%. Add a new variable to the data frame called order_both. This should be a logical vector that indicates whether both order_numb and order_let are true. If so, order_both should be TRUE; if not, then FALSE. Check the function mutate() and use it to create the new variable. "],["plotting-and-visualizing-data-in-r.html", "Session 2 Plotting and visualizing data in R 2.1 Measurement levels in behavioral research 2.2 Why visualize data? 2.3 Plotting data using ggplot2 2.4 Session summary 2.5 Exercises", " Session 2 Plotting and visualizing data in R 2.1 Measurement levels in behavioral research Before we start plotting data using R, it is important to discuss how the different types of data objects found in a programming language such as R might relate to the levels of measurement usually found in behavioral research. Widely diverse strands of research, conducted across different fields of the social and cognitive sciences, rely on behavioral data as their main object of measurement and analysis. Depending on the specific research area and methodology employed, such data can be collected using experiments, questionnaires and surveys, as well as a myriad of other methods. Regardless of the specific data collection method employed, the considerations we discuss below apply to various measures found in behavioral research, from decision-making to information and linguistic processing, encompassing both measures of real-time processing as well as measures of offline judgment and interpretation. The examples we will be using in the remainder of this course come from psycholinguistics, more specifically from two studies on non-literal language, and feature both online and offline measures. In behavioral research, traditionally a distinction has been made between four basic levels of measurement, namely nominal, ordinal, interval, and ratio. Just by looking at the names of the measures themselves, we can see that these levels have rather direct counterparts in the data types found in R (and in other programming languages for that matter). In practice, however, it is important to realize that the mapping between data objects in R and different levels of measurement is not necessarily straightforward, as these four basic measure types can be extended to many other categories and sub-types, which may bring with them assumptions of their own. One must always understand what measure scale their data best corresponds to, so as to encode it appropriately both when creating data, for example for simulation purposes, and when working with data imported from external sources. Lets start by distinguishing between the four basic types of measures. Nominal measures are best understood as categories or instances of any sort of classification where each value represents one category or class. The most basic type of nominal measures consists of dichotomous data, where there are only two categories and every token must be classified according to either one or the other category. There can also be multiple categories, each of them identified by a unique identifier, a label which usually consists of a name or a string of letters or numbers. Example in study design: In an experimental study, individual participants as well as individual stimuli items are understood as nominal measures for the purposes of data and statistical analysis. Individual conditions reflecting a certain experimental manipulation also consist of nominal measures, as well as questions in a questionnaire or survey. Representation in R: Nominal measures should be coded as unordered factors, the labels reflecting unique strings. No arithmetic operations can be performed on nominal measures, yet, one can perform logical operations like checking whether two values are the same or whether a certain token is included in a given category. Ordinal measures are similar to nominal insofar as they represent categories, however, these categories are naturally ordered or ranked, as in a grading or rating scale. Even though the order of the categories is important, only the actual points on the scale are meaningful, the intervals between them not being captured by the scale. Despite ongoing discussion about the matter as well as variation across disciplines, the median is regarded as the most appropriate measure of central tendency to be applied to ordinal data. Example in study design: In an experimental study, any sort of scale presented to a participant or respondent consists of an ordinal measure. Each point on the scale is represented by a label, the labels being organized in an order which is either theoretically or methodologically meaningful. Scales need not necessarily consist of several points, they can also be dichotomous, as in judgments of truth value. Representation in R: Ordinal measures should be coded as ordered factors, the labels following a specific order which can be specified by the analyst. The same sorts of operations that can be performed on nominal measures can be performed on ordinal measures. Interval measures reflect an underlying scale where the degree of difference between the individual points on the scale is meaningful. Intervals may contain arbitrary reference points, such as a zero/ starting point, which can be considered an absolute minimum. Example in study design: In an experimental study, reaction times are interval measures. Zero is the absolute minimum, as the measurement begins only when a certain stimulus or motor/ verbal action is triggered and the measured reaction times cannot be negative. Representation in R: Interval measures should be coded as numeric. Any sort of arithmetic operation can be performed on interval measures. The mean is the most commonly applied measure of central tendency to interval measures. Ratio measures reflect the relationship between two units of measurement, for example a count of a certain type of token in a given time window or its proportion to another type of token. Ratios contain non-arbitrary zero values, which makes it meaningful to say, for instance, that there are are twice as as many tokens of event or category x compared to event or category y. Example in study design: In an experimental study, counts or proportions of responses are ratio measures. Representation in R: Ratio measures can be coded as either numeric or integer depending on the corresponding variable in the study design. These four basic types of measurement illustrate only a portion of the possible measure scales found in behavioral study designs. The coarse-grained distinction presented here suffices for the purposes of our course, however, these considerations should also serve to highlight how important it is to understand how the variables and measurements of a study relate to the objects used in a programming language to represent and analyze the data originating from the measurements made in the study. So, in a nutshell, knowing your data is crucial to any attempt to analyze. 2.2 Why visualize data? Our main goal for today is to learn how to produce visual representations of data. As researchers concerned with analyzing empirical data, we are interested, primarily, in two things: first, we want to understand what the meaningful and relevant patterns in a given data set are; then, we want to be able to highlight and communicate those patterns to an audience who is not directly involved in the process of analyzing the data, be it our peers or other parties involved in the larger process of scientific communication. These two aspects of the data analytic process can be regarded as distinct steps or stages in any analysis pipeline, as they involve different goals and practices. Lets call the first of them data exploration and the second data communication. Data exploration involves wrangling and plotting the data in different ways, according to different dimensions, in an attempt to uncover as many facets of the data as possible. In the case of confirmatory research, it might involve investigating the data beyond the scope of the originally formulated hypotheses. The main point at this stage is to gather as much and as detailed information about the data as possible, without placing much emphasis on making plots visually or conceptually transparent nor on refining them aesthetically. As a result of exploring the data, one should be be able to tell, qualitatively at least, what the main aspects of interest of a data set are, and which of these should be further analyzed by means of statistical analyses or techniques other than visualization proper. Data communication, on the other hand, involves taking the insights gained from the exploration of the data  and possibly too from any statistical modeling  and packaging it in such a way that people not involved in the analysis of the data are able to understand what it says about the investigated research questions. This involves highlighting the most important aspects of the data and making sure the main (differences between) measurements are presented in a visually salient and understandable way. Communicating data visually, however, is not an easy task: there will always be a trade-off between clarity and fidelity to the data, especially in scenarios where the underlying data is complex and the data patterns of interest are convoluted or non-trivial. Often enough, complex study designs will necessitate complex visualizations, though any data set, no matter how simple, will demand optimizing and refining visualizations. Having said that, making decisions about what aspects of the data to highlight or omit in a visualization is an integral part of the data analytic process, and as such should be regarded as one of many possible degrees of freedom that a researcher has, along with aspects related to the design of the study, the choice of a data elicitation and data collection method, as well as any decisions related to the statistical modeling of the data. Lets keep in mind these considerations as we get familiar with the data we are starting to plot in the section below. 2.3 Plotting data using ggplot2 Now that weve discussed what we might be aiming for as researchers visualizing data, lets get familiar with the actual data we are going to be using throughout our course. The data we are visualizing comes from two psycholinguistic studies on metonymy and metaphor. Both metonomy and metaphor are forms of non-literal language which, despite their common idiomatic nature, differ from one another conceptually as well as in terms of their degree of non-literalness. To begin with, lets take a look at what such idiomatic expressions might look like. As the author of the studies has put it: In a metaphor such as to swim against the current, what is said (current in a body of water) stands for something literally unrelated (a general trend). In a metonymy such as to have an eye for detail, eye refers to something that is literally or immanently related or part of the same concept (i.e., ability to see details). (Michl, 2019a, pp. 98-99) The studies we are concerned with thus include two types of idiomatic expressions: Metonymic expressions etw. unter vier Augen besprechen (to discuss something among four eyes = to discuss something in private) mit leeren Händen kommen (to come with empty hands = to arrive empty-handed, without contribution) Metaphoric expressions die Hand für jmd. ins Feuer legen (to put ones hand in fire for somebody = to vouchsafe for somebodys virtue) noch feucht hinter den Ohren sein (to still be moist behind the ears = to be young and inexperienced) In the first of the two studies, Michl (2019a) investigated how non-literalness might be linked to other properties relevant to the linguistic and cognitive characterization of idioms, such as transparency and familiarity. For that, she had participants rate different German idioms along a number of dimensions, later using those ratings to predict idiom type in an inferential statistical model. In the second study, Michl (2019b) had participants read some of the idioms used in her first study in a task that involved reading the stimuli one chunk at a time. The idioms used in the second study were first rated on their non-literalness by an independent sample of participants, and that is what we turn our attention to now. In order to understand the extent to which the expressions presented to participants in the reading experiment were perceived as non-literal, Michl (2019b) first had an independent pool of participants rate the experimental items according to their degree of non-literalness. As such, the author asked participants to rate both literal expressions as well as the target idiomatic expressions on a scale from 1 to 4, where 1 stood for an expression that was not at all literal and 4 for an expression that was completely literal. There were two actual studies: pre-study 1 focused on the metaphoric expressions while pre-study 2 focused on the metonymic expressions. Lets get started with plotting the results, first the ones from pre-study 1. For that, we are using the tidyverse package ggpplot2. In order to produce a visual object with ggplot2, all we have to do is to provide a data source, then tell ggplot2 how to map the variables of interest to the plot aesthetics, and finally tell ggplot2 which graphical primitives to use. You can think of ggplot2 plotting as a layered process which requires all these basic elements just described. We start with the blank canvas onto which we will be adding elements to build our graphical representations of the data. Lets call an empty ggplot() function and see what it does. ggplot() You might be surprised at the output, but this is as much as our chunk of code does: it lays out a blank canvas. What we need to do in order to add content to this blank plot is to specify our coordinate system, or in other words, which variables should be plotted where. Lets then try to add the ratings to the plot. For that, lets add our data source to the code above and lets specify which axis we want to assign the rating variable to  lets say the x-axis. In order to do this, we need to specify some aesthetics within the ggplot() call. The aesthetics tell ggplot() which variable to map onto which coordinate. ggplot(data = pre1, aes(x = rat)) As we can see, we now have the x-coordinate in place  we can already see the rating scale with the variable name rat as the axis title, as well as some of the points on the scale plotted beneath the axis, with vertical lines extending from them. Now lets add the last essential element to the plot, a geometric object, or geom in R lingo. Aside from aesthetic mappings, ggplot() needs geoms in order to plot data onto the canvas. Once all three elements are in place, the basic requirements for the call are met, and the plot is built. ggplot(data = pre1, aes(x = rat)) + geom_bar() Here we used the geom geom_bar(), which plots the data using a bar chart. Notice how ggplot() generated the chart by using the counts of responses for each point on the scale. However, as it stands, the plot only tells us about the responses as an overall function of the points on the scale, aggregated over all data, and not about their relation to the two types of expressions we are interested in. Remember that what we were interested in knowing was what the distribution of responses would look like when taking into account the type of expression rated, either metaphoric or literal. Lets then add that information to the plot. What we are going to do is to specify another aesthetic, this time within our geom_bar() call. More specifically, we will add color to our plot to differentiate between the two expression types, and we will do so by using fill. All there is to do here is to map a variable onto the fill aesthetic, which is embedded within the geom_bar geom. ggplot(data = pre1, aes(x = rat)) + geom_bar(aes(fill = typ)) We now see the same response distribution as above with the addition of color sections to the count bars, each color referring to one expression type. This allows us to distinguish what the relevant patterns in the data are, namely that people rated the metaphoric expressions, plotted in red/ pink, mostly as not at all literal, while rating the literal expressions, plotted in blue, mostly as completely literal. Despite now being more nuanced and informative than before, arguably the graph has also become somewhat convoluted: one needs to parse both the height of the bars as well as their color composition in order to understand what their relation to each point on the rating scale is. One simple way to modify this default rendition while keeping the relevant color distinction in place is to plot the individually colored sections of the bars as individual bars. Lets tell ggplot() to do exactly that by dodging the position of the bars. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(fill = typ), position = position_dodge()) In this re-designed plot, we now see the initial bars split in terms of the type of expression they represent. This way, it is easy to inspect not only the overall distribution of responses per expression type but also the differences in terms of particular points on the scale. Next, what well do to this already quite informative plot is to change the y-axis. Instead of showing the overall frequency of responses, lets plot their relative frequency, that is, as proportional to the number of responses for each expression type. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;, position = position_dodge()) This seems to have worked the way we intended: the proportions are being calculated relative to each expression type. In this plot, one can associate a visual pattern to a quantity which is immediately meaningful. Just by glancing at the plot one can quite easily see that people rated metaphoric expressions as not at all literal in about 80% of the cases. We could also have plotted the ratings proportional to the overall number of responses, though this is not what we were interested in, from the point of view of our empirical and data analytic goals. This might be useful, however, in other cases. Notice the differences in distribution if we had opted for this alternative plotting. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = (..count..)/sum(..count..), fill = typ), position = position_dodge()) Now, usually there are several ways of achieving the same, or at least similar, ends in R. In the graph sequence above, we used color as well dodged or split bars to visualize the ratings in terms of our two groups of interest, the metaphoric and the literal expressions. We opted for plotting the responses in a single graph, which means that the data points are plotted sharing the same axes as wel as the same visual real estate at the center of the plot. Notice how we also relied on a legend to be able to tell which expression type was which. One alternative to this rendition is to use facets, an option provided in ggplot2 for splitting observations or groups of observations into different windows or panels within a single graph. facets work in a very simple way, you simply specify what variable(s) the data should be split by, and how the facets are to be plotted, horizontal or vertical to one another. Here we tell ggplot() to split the data by the expression type and to plot the facets horizontal to one another. pre1 %&gt;% ggplot(aes(rat)) + geom_bar() + facet_wrap( ~ typ) This rendition offers a different way of visualizing the same results as above: we can see the distribution of ratings for both the metaphoric and the literal expressions. With the facets it is easy to see that the metaphoric expressions are rated mostly as not at all literal while the literal expressions are rated mostly as completely literal. Although the two facets share the same y-axis, they are clearly divided in terms of expression type, which is indicated on top of each facet by means of a label. Even if we are not exploring any customization options right now, you can probably already tell that with ggplot2 virtually any aspect of a visualization can be altered or enhanced: here, for example, we could change the position and aesthetics of the facet labels, or the range and aesthetics of the axes and ticks. More importantly for now, notice how we are facetting our data in terms of a single variable, type, though facetting is also possible with two variables, in which case there are more degrees of freedom in terms of how exactly to render the visualization in spatial terms. Much like we did before, lets convert the y-axis to represent the proportion of responses by expression type instead of the raw response frequency. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop..), stat = &quot;count&quot;) + facet_wrap( ~ typ) Currently, our graph does not contain any color coding, which we replaced earlier with the facetting. This is not to say, however, that color cannot be added to a facetted plot. Lets map the variable type onto a fill aesthetic, much like we did before. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;) + facet_wrap( ~ typ) Lets now turn our attention to the data from pre-study 2, which compared metonymic and literal expressions. Lets plot the ratings using our latest ggplot() code. pre2 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ, group = typ), stat = &quot;count&quot;) + facet_wrap( ~ typ) The plot looks just like the one we had for pre-study 1, except, of course, that the data patterns are different here: while still rating the literal expressions mostly as completely literal, people rated the metonymic expressions mostly as fairly literal, in contrast to metaphors which were rated mostly as not at all literal. Now, in order to get a global picture of the combined results of the two norming studies, we would like to compare the results of pre-study 2 to those of pre-study 1. Lets then plot the data together. In order to do that, we will have to merge the two individual data sets together, something which we can easily do in R using, for example, tools from the tidyverse package dplyr. We are using a function called bind_rows, which adds one data set right on top of the other. Notice how in the function call, in addition to the two data frames, we specify the argument .id, which gives us a label we can refer back to to know which data set each individual observation comes from. norms &lt;- bind_rows(pre1, pre2, .id = &quot;study&quot;) We successfully joined the two data sets, but we also got an error that due to unequal factor levels one of our resulting factors was coerced into character. Luckily for us, we know where this error might come from: while in the data set from pre-study 1 the factor type had the levels meta and woe, in the date set from pre-study 2 it had the levels meto and woe, which means that the resulting data frame has a factor with three levels as opposed to two. That is all fine, we just have to convert the variable type in the the new data frame to factor again. norms$typ &lt;- as.factor(norms$typ) Now that we have a single data set with observations from both the pre-study on metaphors and and the pre-study on metonomies, lets plot the combined results. Lets go back to facet_wrap() and try to change our code from earlier to produce a plot with four facets. norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;) + facet_wrap(study ~ typ) We were able to achieve what we wanted by instructing ggplot2 to facet the data by both the original data set, study, and the expression type, type. Notice how the order of the arguments in the facet_wrap() call impacted the resulting plot: study was plotted along the x-axis while type was plotted along the y-axis. The graph is already good as it is, but lets redesign the facet labels to aid visualization. For that, well create two vectors with the new names we want to assign to the labels. Then, well use the labeller() function from ggplot2 to apply those new labels to the already existing plot. study_labels &lt;- c( `1` = &quot;Study 1&quot;, `2` = &quot;Study 2&quot; ) type_labels &lt;- c( meta = &quot;Metaphoric&quot;, meto = &quot;Metonymic&quot;, woe = &quot;Literal&quot; ) norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;) + facet_wrap(study ~ typ, labeller = labeller(study = study_labels, typ = type_labels)) In order to conclude our introduction to ggplot2, lets attempt to visualize the combined results with our split bar technique from earlier, exploring an alternative visualization possibility. norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, labeller = labeller(study = study_labels, typ = type_labels)) This is an interesting plot: it subsumes the ratings for each expression type within a single facet which tells us which study the ratings come from. Despite the usage of the split bars though, the plot is somewhat convoluted, as there is a lot of visual information in each facet. Lets try to modify this graph so as to keep both distributions but only focus on a single expression type per facet. For that, well actually plot the graph twice, once highlighting the literal results and once highlighting the non-literal results. First, lets create two vectors with the color highlight information: for the literal facets, we color the literal responses in their original color and the non-literal responses in gray; for the non-literal facets, we color the non-literal responses in their original color and the literal responses in gray. In both cases, we follow the original coloring scheme, plotting our graph once with each coloring information. literal &lt;- c( &quot;woe&quot; = &quot;grey&quot;, &quot;meta&quot; = &quot;#F8766D&quot;, &quot;meto&quot; = &quot;#00BA38&quot;) non_literal &lt;- c( &quot;woe&quot; = &quot;#619CFF&quot;, &quot;meta&quot; = &quot;grey&quot;, &quot;meto&quot; = &quot;grey&quot;) literal_norms &lt;- norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, ncol = 1, labeller = labeller(study = study_labels, typ = type_labels)) + scale_fill_manual(values = non_literal) print(literal_norms) non_literal_norms &lt;- norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, ncol = 1, labeller = labeller(study = study_labels, typ = type_labels)) + scale_fill_manual(values = literal) print(non_literal_norms) Now that we have two plots, each with their own color highlight, we merge them together into a single plot. This time, we will use a function from an external package, namely the cowplot package, as ggplot2 doesnt offer us exactly the tools we need. We use a function called plot_grid(), which is being called from the package cowplot via cowplot::plot_grid(), and to it we add the two plots we just created. Notice how within the call we specify that no legend should be plotted. This is the case as well generate a common legend below. plot &lt;- cowplot::plot_grid( non_literal_norms + theme(legend.position = &quot;none&quot;), literal_norms + theme(legend.position = &quot;none&quot;), nrow = 1) print(plot) Now, all we need to do is to produce a common legend for our new plot, and then to combine the plot with its legend. In order to do that, we generate the plot we started with, where there was no custom color information, only so we can get the legend with the right colors in it. We then use the useful get_legend() function from cowplot to extract the legend. Finally, we produce our final plot using plot_grid() again, now with the legend included. legend_norms &lt;- norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, labeller = labeller(study = study_labels, typ = type_labels)) print(legend_norms) legend &lt;- cowplot::get_legend(legend_norms) cowplot::plot_grid(plot, legend, rel_widths = c(2, .3)) 2.4 Session summary In order to produce a plot with ggplot2 three basic elements are needed: the ggplot() call which sets the canvas, a data source and the basic coordinate aesthetics, and a geometric object, also known as a geom; geoms are graphical primitives which are used to plot different types of graphs. In todays session, we only used geom_bar(), which plots bar plots, but ggplot2 offers a myriad of other geoms, some of the most useful for our purposes being geom_point() for scatter plots, geom_box() for box plots, and geom_violin() for violin plots. We will use some of these other types of geoms later in our course. As we will see, different geoms allow us to plot different types of variables as well as their corresponding interactions; aesthetics allow us to add more elements to a plot, on top of the base geom specifications. They also allow us to customize our plots in different ways. We saw how to add color to a plot using fill but also how to create subplots using facet_wrap(). We also made some adjustments to our base plots specifying the position and labeller aesthetics; We learned that with ggplot2 there are usually several ways of achieving the same desired ends. We will talk more about clarity and relevance in data visualization in the next sessions, but already at this point its good to realize that the same data, with the same intended visualization goal, can be visualized in a variety of ways, often to the same utility to the target audience. Although there are some principles which are generally preferred across different visualization contexts, ultimately many of the design choices are dependent on the personal goals and taste of the analyst. Standards also vary from one field of research to another, but the main takeaway is that we should aim at a trade-off between clarity and functionality that best suits our particular needs as analysts visualizing a specific data sets. You can find a summary of the essential ggplot2 functions on this Cheatsheet. 2.5 Exercises fill is not the only way to add color to plots using ggplot2. Using the documentation, check what the aesthetic color does and then try to replace fill with color in one of the plots above. If youre feeling curious, try using both fill and color. In the plots where we plotted proportions, the scale on the y-axis shows ticks at every increment of 0.2, starting at 0. Replot one of the graphs above such that there are ticks at every increment of 0.1 instead. Try using the function scale_y_continuous, and read on the options for the break aesthetic. There are two functions used for facetting in ggplot2: facet_wrap(), which weve already used, and facet_grid(). Read on facet_grid() and try to use it to replot one of the graphs from above. "],["exploring-and-communicating-patterns-in-rating-data.html", "Session 3 Exploring and communicating patterns in rating data 3.1 Laying out the data 3.2 Exploring the data 3.3 Enriching the base visualizations 3.4 Session summary", " Session 3 Exploring and communicating patterns in rating data 3.1 Laying out the data Last session we got an introduction on how to use ggplot2 to plot data in R. We saw that with the basics in place, which are the data to be plotted as well as at least one geom, a graphical primitive in R, we can create graphs which allow us to visualize our data, being up to us as analysts to decide what specific format and what design choices best suit our visualization needs. Today we will dig deeper into how to visualize data using gpplot2, first by seeing some examples of how to explore different patterns of interest in a data set and then by creating visualizations which not only highlight the most relevant patterns in the data but which also look refined and visually pleasing. We will be visualizing data from the first of the two studies introduced last session, in which the primary question of interest was whether metaphors and metonomies, both of which forms of non-literal language, differ in their perceived degree of literalness. Recall that in this study the author asked groups of people to rate both types of idiomatic expressions along a number of dimensions tied to literalness, namely familiarity, transparency, and non-literalness. Lets focus our attention on the ratings for transparency, which encompasses two distinct properties, comprehensibility and relation. As the author of the study puts it, comprehensibility, as a feature of transparency, relates to [] the ease with which the meaning of an idiomatic unit can be recovered (Michl, 2019a). In addition to comprehensibility, the relatedness between the literal and non-literal meaning of an idiom also matters for its perceived transparency. Indeed, as the author explains in the paper, the notion of relation [] defines the strength of the semantic or conceptual link between the literal and the idiomatic meaning of an idiom. With these two notions in mind, lets then turn to the ratings themselves. As we can see, comprehensibility and relation are theoretical constructs which capture the nature of idiom transparency from two different angles, yet they may or may not overlap with one another in the ratings measured empirically. In the study we are concerned with, people were asked to rate 244 different idioms. As a reminder of what these idioms might look like, lets take a look at an example of each type of expression, taken from the sample used in the study. Metaphor - das Eis brechen - to break the ice = to relax a socially stiff/ uncomfortable situation Metonomy - die Nase rümpfen - to wrinkle ones nose = to be contemptuous or disgusted Now, lets start plotting the comprehensibility ratings, which range from 1, extremely difficult to understand, to 5, extremely easy to understand. For that, lets recycle some code from last session, where we already plotted similar ratings using colored bars to indicate the different expression types. Instead of plotting the raw numbers, lets make sure to plot the proportion of ratings as a function of the expression types, which is a more reader-friendly plotting option compared to using raw counts. rat_comp %&gt;% ggplot(aes(x = rat)) + geom_bar(aes(y = ..prop.., fill = type, group = type), stat = &quot;count&quot;, position = position_dodge()) As the plot shows us, at least on the surface, the two types of expression look very similar in terms of their perceived comprehensibility. Given the selected sample of idioms, respondents seem to find, on average, both metaphors and metonomies quite easy to understand. And yet this finding is is not necessarily transparent when one looks at the plot. On the one hand, it is obvious by looking at it that most of the rating mass is concentrated on the upper end of the scale, over points 4 and 5. On the other hand, however, it is not immediately evident what the central tendency of the response distribution is. Since we are talking about ordinal data here, our preferred measure of central tendency is the median, which seems to be around 4, but this is a guess based on eyeballing the data and roughly estimating it. Before assuming that 4 is the actual median rating, lets calculate the median directly. For that we will turn to our trusted dplyr package from the tidyverse, which weve already used to wrangle data before. So what we want to do here is to calculate the median for each group of interest. Medians can be calculated in R using the function median(), which according to the documentation can be used to compute the sample median and is to be applied to a numeric vector containing the values whose median is to be computed. Now, since we want the median of each group, and not just the overall sample median, we need to group the data. Thanks to dplyr, that is easily done using group_by(), inside of which we specify our grouping variable, in this case type. Next, what we need to do is to call our summary statistic function, median(), inside of a summarize() call, which creates a new data frame with our summary of interest. Lets try that. rat_comp %&gt;% group_by(type) %&gt;% summarize(median = median(rat)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## type median ## &lt;chr&gt; &lt;dbl&gt; ## 1 metaphoric 4 ## 2 metonymic 4 The calculated summary statistic confirms our rough guess that the median rating of both metaphors and metonomies is 4. But this still leaves us with the question of how to improve our visualization such that this pattern of interest becomes more noticeable. There might be ways of changing the current plot so as to make the medians stand out, but one more practical solution is to completely re-think our choice of a graph. Notice how weve been using the geom_bar() geom so far, but maybe there are better alternatives in the geoms available in ggplot2. Lets explore a little. What we have done so far is to plot one variable, namely rat, the resulting graph showing bars either with the raw response counts or with different proportions of responses according to each expression type, which we achieved using fill(). What if we tried to assign type to one of the axes instead of using fill() to add it to the graph? Lets try that, and lets use another geom instead, since geom_bar() only plots one variable. We can start by giving geom_point() a try, which plots each response as a point. Lets assign type to the x-axis and rat to the y-axis. rat_comp %&gt;% ggplot(aes(type, rat)) + geom_point() Our code does exactly what we tell it to do: it plots the ratings on the y-axis and the groups as two distinct columns on the x-axis. But why does the plot look so weird, as if there was only one data point for each point on the rating scale? Well, the reason is that geom_point() is useful for displaying the relationship between two continuous variables, as would be the case for interval or ratio data. What we have at hand is two discrete variables, one ordinal and one dichotomous, so our data points are in fact all there, but, since our rating scale consists of discrete points, the responses are overplotted. Lets use another geom to declutter the overplotted graph and reveal the underlying response points. Well use geom_jitter(), which according to the documentation adds a small amount of random variation to the location of each plotted point, such that it is a useful way of handling overplotting caused by discreteness. rat_comp %&gt;% ggplot(aes(type, rat)) + geom_jitter() With geom_jitter() were able to see all the responses, which are clustered around each point on the rating scale. Remember that we were interested in being able to spot where most of the rating mass lies on the scale, so it might make sense to plot the scale horizontally, so one can easily notice the data spread for each expression type as well any differences between the two groups, which will be plotted one on top of the other. Lets try to achieve that by reversing our x and y arguments in the ggplot() function. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_jitter() This jittered plot looks more intuitive than the previous one, but even after reversing the axes we still end up with a plot which is not particularly informative; if anything our original barplot was more informative. Lets try another solution then, namely another geom, geom_count(). According to the documentation, geom_count() is a variant of geom_point() which counts the number of observations of each location and then maps the count to a point area. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_count() This again somewhat resembles our original barplot insofar as we get a visual correlate of the amount of responses for each point on the rating scale. Still, this plot is arguably much less informative than the original barplot. So do we settle for a barplot or do we try some more alternatives? Lets try some more alternatives. So far weve tried geoms which take either two continuous variables or two discrete variables. ggplot2 also has geoms which take one discrete and one continuous variable. Even though the ratings we are plotting are ordinal in nature, they are represented in R as a numeric variable, which we can double-check via a str() call. str(rat_comp$rat) ## num [1:16492] 4 2 2 1 1 2 3 3 2 1 ... R represents vectors containing numbers as numeric by default, regardless of what these numbers actually represent. We could have converted rat to a factor to reflect the nature of its original measurement type, but keeping it as a numeric vector was useful for calculating the median earlier on. Still, what that means is that we are plotting rat as a continuous variable, which is useful for the geom were using next, geom_boxplot(). Boxplots, or box-and-whisker plots as they are also called, are a very useful way of graphically representing numeric data when one is interested in the quartiles or percentiles of the data distribution. The median itself is the 50th percentile, that is, the point which divides the data distribution in two. The lower and uppers quartiles represent the 25th and the 75th percentiles, respectively. Lets take a look at our graph and see how all that is represented in a boxplot. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot() What this plot tells us is that for both expression types 4 corresponds to the 50th percentile of the data, that is, to the median point in the rating distribution. This is marked by a thick vertical black line in the plot. Moreover, the plot tells us that 3 and 5 are the 25th and 75th percentiles, respectively, which means that half of the data mass lies between these two points. These percentiles are marked by thin vertical black lines, which enclose the area colored in white and halved by the median line. The lines extending from the boxes in the graph, the so-called whiskers, indicate values that lie within a deviation of 1.5 x the range marked by the 25th and 75th percentiles, which is called the interquartile range. Importantly for our current purposes, such a plot gives us both our desired measure of central tendency as well as information about the spread of the data, all readily available as part of the geom itself. Before we move on, lets look at one last geom, geom_violin(). A plot produced with geom_violin() resembles a boxplot except that it has the shape of a density distribution, which may or may not come in handy. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_violin() In our case, such a violin plot is not particularly informative, and that is because our data, despite being represented as continuous, actually consists of discrete response bins clustered around each of the five points of the rating scale. We will therefore stick with our boxplot from above, which was the most informative graph for our purposes, perhaps alongside the barplot. Now that we have settled on the type of graph we will be using for our visualization, lets plot the other transparency measure, the relation ratings. rat_rel %&gt;% ggplot(aes(rat, type)) + geom_boxplot() Given the selected sample of idioms, respondents seem to find that both in the case of metaphors and metonomies their literal and non-literal meanings are neither extremely distantly or not at all related nor extremely closely related. That is to say that, on average, the literal and non-literal meanings of both types of expressions is perceived as not particularly closely or distantly related, which may be expected given the large number of items in the study and inherent variability between items. Now that we have both the comprehensibility and the relation ratings, we might want to combine them in a single plot, given that both measures relate to the notion transparency. In order to do that, we will create a graph that shows the correlation between the two response distributions on the basis of visual overlap between the data. First, we need to combine the two data sets, so we can have a single data frame to feed to ggplot(). rat_combi &lt;- bind_rows(rat_comp, rat_rel, .id = &quot;study&quot;) Now that we have a combined data frame, lets plot it using a boxplot, just like we did above. In order to highlight the different measures, we will need to add the variable study to the plot, which we created in our bind_rows() call above. If we inspect rat_combi, well see that study has 1 and 2 as default IDs for our two transparency measures, so we should probably change that to something more meaningful, to make our lives easier. Were using a function called fct_recode() from the tidyverse package forcats. Note that study is not actually coded as a factor in our data frame but rather as character, which is the R default for vectors containing strings. That is not a problem in itself as we can still use fct_recode() to change the labels associated with the values recorded in study. Lets call the comprehensibility measure comp and the relation measure rel. str(rat_combi$study) ## chr [1:37138] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... rat_combi$study &lt;- fct_recode(rat_combi$study, comp = &quot;1&quot;, rel = &quot;2&quot;) str(rat_combi$study) ## Factor w/ 2 levels &quot;comp&quot;,&quot;rel&quot;: 1 1 1 1 1 1 1 1 1 1 ... Now lets plot the combined ratings, just like we did above with only one measure. Here, we will add the information about the measures as a fill() within our geom_boxplot() call. Note how we are using color here to highlight the source of the ratings, or in other words, what measure they refer to, while we are mapping our expression type directly onto the y-axis. rat_combi %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = study)) With this plot, we can directly compare our two transparency measures. Of course, we could also reverse the current visual scheme and map the study source to the y-axis and use color to indicate the expression type, like below. This second rendition might be more informative if we would like to highlight both the contrast between the expressions types and the one between the measures themselves. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type)) One thing left to do before we move on is to make some minor visual improvements to the plot. Later today we will make further changes to these plots to increase their legibility and clarity, but for now lets simply dodge the boxes a little bit, so theyre not so close to one another. Within our geom, we can specify the same argument we specified last session when working with barplots, namely position = position_dodge(). Since the boxes in boxplots are already slightly dodged by default, we can increase the distance between them by modifying the argument width within position_dodge(). rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), position = position_dodge(width = .9)) Weve increased the distance between the boxes in each group, but what if we also wanted to increase the distance between the two groups? Well, we can then modify the width argument within geom_boxplot(), which is a separate argument as the one nested within position_dodge(). Lets try that out. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), width = .9, position = position_dodge(width = .9)) This didnt work the way we wanted to, and thats because were now dealing with two width parameters, so we need to adjust them accordingly until we find a balance that suits our needs. Lets reduce the distance between groups. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), width = .5, position = position_dodge(width = .9)) Were getting there, but we should probably reduce the distance between the paired boxes a tiny bit as well. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), width = .5, position = position_dodge(width = .7)) This plot looks good. By inspecting it, one can see that there are no differences in how the two idiom types are perceived, neither in terms of their comprehensibility nor in terms of the relation between their literal and non-literal meanings. There is, however, a difference between the two transparency measures, such that both metaphors and metonomies are perceived, on average, as easy to understand despite their literal and non-literal meanings not being perceived as particularly related. Lets now explore the data a bit more to see if there are other interesting patterns worth highlighting and communicating. 3.2 Exploring the data Before working on improving our plots, well explore our data some more. A detailed account of the hows and whys of exploring data, as part of a data analytic pipeline, is something that goes much beyond the scope of this course and which is directly tied to the specific practices and standards of a research field as well as to specific methods employed to measure and analyze data. Having said that, its important to understand what the goal of data exploration might be, and how it might fit within the larger picture of data analysis. Analysis of empirical data can be roughly divided into exploratory and confirmatory strands. With exploratory analyses one attempts to describe and summarize the main characteristics of a data set, as a representative snapshot of a given phenomenon, possibly with the intention of generating hypotheses for further empirical testing. In a study which is exploratory in nature, the researchers do not state specific hypotheses about the data they are analyzing, despite having a research question and specific empirical and possibly theoretical goals. With confirmatory analyses, on the other hand, also known as hypothesis testing, one attempts to answer a research question which is formulated in terms of a specific hypothesis or sets of hypotheses, usually accompanied by an [explicit] alternative hypothesis or sets of alternative hypotheses. In a study which is confirmatory in nature, the researchers attempt to confirm hypotheses which they formulate before collecting and analyzing the data. In practice, data exploration and hypothesis testing are complementary aspects of empirical research, such that exploring one data set might lead to generating hypotheses, and possibly building theories, which can be tested explicitly using another data set. While testing hypotheses which are different from the ones formulated beforehand is considered bad practice in confirmatory research, data exploration is something that can and is expected to happen in addition to hypothesis testing, either cumulatively across different studies which rely on different data sets or as part of a single study before any hypotheses are generated and tested. The study we are focusing on today can be characterized as exploratory, and as such exploring the data beyond the main contrasts of interest might be particularly fruitful. Given that we are not experts in the particular empirical/ theoretical domain the study is situated in, generating hypotheses for future empirical testing would probably be wishful thinking, and it also escapes the purpose of this exercise. We can, nevertheless, look at the data and see if theres any interesting descriptive pattern worth highlighting. Given that we have already explored the transparency ratings to some degree in the previous section, lets look at some of the other variables which are recorded in the data set. Lets use our trusted str() function to generate an overview of the data frame. Alternatively, we can also view the data by clicking on it in our environment panel on the top right of RStudio. str(rat_combi) ## tibble [37,138 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ study : Factor w/ 2 levels &quot;comp&quot;,&quot;rel&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ part_id: num [1:37138] 460 460 460 460 460 460 460 460 460 460 ... ## $ gender : chr [1:37138] &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ age : num [1:37138] 31 31 31 31 31 31 31 31 31 31 ... ## $ state : chr [1:37138] &quot;Niedersachen&quot; &quot;Niedersachen&quot; &quot;Niedersachen&quot; &quot;Niedersachen&quot; ... ## $ educ : chr [1:37138] &quot;Hoch/Fachhochschul&quot; &quot;Hoch/Fachhochschul&quot; &quot;Hoch/Fachhochschul&quot; &quot;Hoch/Fachhochschul&quot; ... ## $ item : num [1:37138] 1 2 3 4 5 6 7 8 9 10 ... ## $ rat : num [1:37138] 4 2 2 1 1 2 3 3 2 1 ... ## $ type : chr [1:37138] &quot;metaphoric&quot; &quot;metaphoric&quot; &quot;metaphoric&quot; &quot;metaphoric&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. part_id = col_double(), ## .. gender = col_character(), ## .. age = col_double(), ## .. state = col_character(), ## .. educ = col_character(), ## .. item = col_double(), ## .. rat = col_double(), ## .. type = col_character() ## .. ) In addition to our main variables of interest rat and type, which we are already familiar with, we also see other variables which relate to the study per se, such as part_id and item, as well as basic demographic information about the respondents, like age, gender, and educ. Even though we dont know what a theory of non-literal language might say about the relation between perceived idiom transparency and traits like gender or age of a language user, we might expect there to be differences in how, for example, young and older language users perceive the comprehensibility of metaphors, or in how users with a higher or lower level of formal education perceive the relation between the literal and non-literal meaning of such expressions. Lets then start by plotting our ratings from above in terms of different sub-populations of our sample. To simplify our job for now, lets plot only the comprehensibility ratings, and lets use color to help us visualize any differences between our two expression types. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) To this plot, were adding a facet_wrap() with gender as a grouping variable. Lets also move the legend to underneath the plot, so it doesnt take up so much space on the right-hand side. For that we need to add a new layer to our ggplot() call, one in which we can specify details about the plot. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ gender) + theme(legend.position = &quot;bottom&quot;) As the graph suggests, there seem to be no systematic differences in terms of how men and women perceive the comprehensibility of both metaphors and metonomies. Lets now plot the ratings in terms of age. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ age) + theme(legend.position = &quot;bottom&quot;) This is an interesting plot: what we see is the ratings facetted in terms of all the 34 different ages reported by respondents. Just by glancing at the graph, we see that there is variability in terms of how people of different ages rated the idioms, on average metaphors and metonomies being rated similarly, despite different patterns in some of the facets. Lets now collapse the ages into more representative age groups, such as how they are usually presented in surveys. For that, we will need to wrangle our data some more. What well do is that well create a new variable, called age_group, which is based on groupings of younger, middle-aged, and older respondents. For that, were using the function mutate() from dplyr, which we first encountered in the exercises from session 1. Within our mutate() call were using another function called case_when(), which basically allows us to vectorize multiple if-else statements. Lets see how that works. rat_comp &lt;- rat_comp %&gt;% mutate(age_group = case_when(age &gt;= 18 &amp; age &lt;= 24 ~ &quot;18-24&quot;, age &gt;= 25 &amp; age &lt;= 54 ~ &quot;25-54&quot;, age &gt;= 55 &amp; age &lt;= 64 ~ &quot;55-64&quot;, age &gt;= 65 ~ &quot;65+&quot;)) What we did was to create age_group on the basis of grouping of values from age, such as specified in the call. We can now replace age with age_group in our code and see how the new plot looks like. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ age_group) + theme(legend.position = &quot;bottom&quot;) This plot shows that, despite variability in some of the facets in the plot above, the ratings are the same across three of the four binned age groups, the exception being adults who are older than 65. All in all, the median comprehensibility of both metaphors and metonomies is still 4 on the 5-point scale. Lets now try to combine this plot with the earlier plot where we grouped participants by gender. For that, we need to add gender to our facet_wrap() call. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap(gender ~ age_group) + theme(legend.position = &quot;bottom&quot;) This plot confirms what we had already seen separately in the gender and age plots, namely that, on average, the ratings are consistent across men and women as well as across different age groups. The outliers seem to be men who are older than 65. The current plot works well for the purposes of exploring the data, but in case we would like to have something which is a bit more transparent to other viewers, we might want to line the facets for each gender in their own rows. Lets change that by specifying the number of rows in our facet_wrap() call. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap(gender ~ age_group, nrow = 2) + theme(legend.position = &quot;bottom&quot;) Note that if we had used facet_grid(), which is an alternative to facet_wrap(), we would have had achieved a similar end result without having to specify the number of desired rows, as the two functions work differently and have different arguments. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_grid(gender ~ age_group) + theme(legend.position = &quot;bottom&quot;) Just before we move on to improving the aesthetics of our plots, lets try to explore the data in terms of one last variable, namely the specific idioms that participants rated. Before we plot the data, lets double-check how many items there were in total. Lets use the function unique(), which can be applied to our vector item to single out all unique values. unique(rat_comp$item) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 ## [217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 ## [235] 235 236 237 238 239 240 241 242 243 244 That means that we have 244 items in total, which might be complicated to plot. Lets give it a go in any case and see what we get. Were adding item as a grouping variable within facet_wrap(). rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ item) + theme(legend.position = &quot;bottom&quot;) The resulting plot is, as expected, simply impossible to parse. We will have to find some alternative way of exploring the items. What we are interested in, ultimately, is in having an overview of the amount of variability in the rated idioms. Since we know that the median rating is 4, perhaps we can use that to break down the responses into smaller groups. Lets calculate, for each expression type, the relative number of items rated as more than 4 and less than 4. For that, we first group the items by type. Then, we create a variable called median_split, which indicates whether the rating for a given item is equal to or smaller/ larger than the median. We then group our data by type and our new variable median_split and summarize the relative number of occurrences. rat_comp %&gt;% group_by(type) %&gt;% mutate(median_split = case_when(rat &gt; 4 ~ &quot;5&quot;, rat &lt; 4 ~ &quot;1,2,3&quot;, rat == 4 ~ &quot;4&quot;)) %&gt;% group_by(type, median_split) %&gt;% summarize(n = n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` regrouping output by &#39;type&#39; (override with `.groups` argument) ## # A tibble: 6 x 4 ## # Groups: type [2] ## type median_split n freq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 metaphoric 1,2,3 3768 0.355 ## 2 metaphoric 4 2950 0.278 ## 3 metaphoric 5 3908 0.368 ## 4 metonymic 1,2,3 2064 0.352 ## 5 metonymic 4 1648 0.281 ## 6 metonymic 5 2154 0.367 The information we generated tells us, in numbers, what we had already inferred from our graphical representation of the data, namely that most of the rating mass lies on the upper end of the scale, over points 4 and 5. In order to get more detailed information about each item, wed have to add item to our grouping call. rat_comp %&gt;% group_by(type) %&gt;% mutate(median_split = case_when(rat &gt; 4 ~ &quot;5&quot;, rat &lt; 4 ~ &quot;1,2,3&quot;, rat == 4 ~ &quot;4&quot;)) %&gt;% group_by(item, type, median_split) %&gt;% summarize(n = n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` regrouping output by &#39;item&#39;, &#39;type&#39; (override with `.groups` argument) ## # A tibble: 732 x 5 ## # Groups: item, type [244] ## item type median_split n freq ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 metaphoric 1,2,3 3 0.0469 ## 2 1 metaphoric 4 13 0.203 ## 3 1 metaphoric 5 48 0.75 ## 4 2 metaphoric 1,2,3 11 0.169 ## 5 2 metaphoric 4 26 0.4 ## 6 2 metaphoric 5 28 0.431 ## 7 3 metaphoric 1,2,3 26 0.4 ## 8 3 metaphoric 4 19 0.292 ## 9 3 metaphoric 5 20 0.308 ## 10 4 metaphoric 1,2,3 12 0.185 ## # ... with 722 more rows What we now get is a detailed view of how each item was rated according to our tripartite division of the data around the median. From here on, in order to further explore the items, wed have to conduct a more thorough inspection of the numbers in the table. We could look, for instance, for those items that received similar ratings, and then inspect the actual idioms to see if there are any commonalities between them. This is, of course, something that requires not only a qualitative view of the data but also expert knowledge about the actual target domain. Because of that, were not exploring this data any further, but this should serve to show that data exploration consists of a mix between visualization, wrangling, and numerical/ statistical summarizing. The best place to start is, just liked we did, with plots and graphical representations of the data, but usually several iterations between different techniques will be required to get at interesting and relevant data patterns. 3.3 Enriching the base visualizations Having laid out some base plots and explored the data along some of its secondary dimensions, were now in the position to decide what aspects of our data we want to highlight and communicate to our audience. In order to do that, we are going to go through the steps of producing camera-ready visualizations of the data, taking into account what we learned about it and what we deem interesting to be communicated. As a reminder, we are dealing with categorical data, where each value consists of a category, in our case one point on a 5-point psychometric scale measuring properties related to idiom literalness. More specifically, our data is ordinal, meaning that the different categories are ranked, as the task participants were asked to perform consisted of rating, for example, the comprehensibility of each idiom they read using the provided 5-point scale. Now, our main goal is to highlight any potentially systematic differences between the participant ratings as a function of the experimental conditions. As we saw, the main contrast between metaphors and metonomies seems to hold across different sub-populations of our sample, the minor differences found in terms of age perhaps not being relevant to the point that they deserve being communicated in a plot of its own. Because of that, lets focus on communicating the main results, namely the lack of systematic difference in comprehensibility between metaphors and metonomies, and the fact that both were rated, on average, as quite easy to understand, a 4 on the 5-point scale. Before we plot our data, lets double-check how its represented in R. str(rat_comp$rat) ## num [1:16492] 4 2 2 1 1 2 3 3 2 1 ... As we can see, the ratings are coded as numeric, although we know that they represent, in fact, ordinal data. We managed to plot the data before without any problems, but lets see what happens if we convert rat to a factor and try to plot it then. rat_comp$rat &lt;- as.factor(rat_comp$rat) Lets start by re-plotting the comprehensibility ratings using our basic and straightforward ggplot2 code from above. Remember that all we have to do to create a basic graphical representation of our data is to specify the required arguments in the ggplot() function, namely aes(), which requires a variable to be plotted to the x-axis  our experimental conditions, that is, the two idiom types  and one variable to be plotted to the y-axis  our ratings. We subsequently tell ggplot() to plot the data using boxplots, which we decided was going to be our geom of choice to highlight the patterns of interest in the data. rat_comp %&gt;% ggplot(aes(type, rat)) + geom_boxplot() That does not look good at all. The reason why we get this odd result is because were feeding geom_boxplot(), which takes a continuous and a discrete variable, with two discrete variables, somewhat of a similar problem to what happened to us earlier when we tried feeding geom_point() with the ratings. Lets change the ratings back to numeric and try again. rat_comp$rat &lt;- as.numeric(as.character(rat_comp$rat)) rat_comp %&gt;% ggplot(aes(type, rat)) + geom_boxplot() This works just fine. Now that we have the basic data patterns laid out, lets start refining our plot, little by little, so it looks increasingly better, both in terms of transparency and effectiveness but also in terms of aesthetics. The first thing we are going to do is to flip the boxplots, so they are plotted horizontally as opposed to vertically. Since our main goal with this visualization is to allow our audience to notice where most of the rating mass lies on the scale, it makes sense to plot the scale horizontally, so one can easily notice the data spread for each group but also any differences between groups, which will be plotted one on top of the other. Lets do that by reversing our x and y arguments in the ggplot() function, just like we did earlier. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot() This works just as intended. In some cases, however, reverting the axes in the ggplot() function will not produce the intended result. Luckily, we can also flip the x, y coordinates without changing the order of the arguments within the ggplot() call. To understand how flipping coordinates works, notice what happens if we add the coord_flip() call to our code. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot() + coord_flip() 3.3.1 Modifying major elements of the plot Now that we have our data plotted the way we would like it to be, lets work on some further adjustments to the base plot which will greatly improve the effectiveness of our data communication. Remember that there are a couple of useful ways of highlight and contrasting visual elements. Relevant for this case is the manipulation of color and fill scales. First, lets assign different fills to each experimental condition, which is the variable assigned to the y-axis. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot() Now each condition is represented by a different color. Lets change the color palette and slightly increase the transparency of the filled colors. For the color itself, we need to specify a new palette in a function that refers to our fill scale, namely scale_fill_brewer(). For the transparency, we modify the alpha parameter within our geom. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7) + scale_fill_brewer(palette = &quot;Set2&quot;) Now lets remove the gray background so as to make the plot somewhat cleaner. Lets also thicken the lines which surround and extend from the boxplots, the boxplot whiskers, in order to create a sharper contrast between the boxes and the white background. For the lines, we need to adjust the lwd argument within geom_boxplot(), while for the background were applying a new theme to the plot, namely them_minimal(), which, among other things, removes the gray background. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() 3.3.2 Refining minor elements of the plot As it is, the plot looks pretty good already, the most important aspects of the underlying data appropriately represented and well clearly visible. Now we can fix minor issues which arguably only contribute cosmetically to the plot, though when considered together with the previous changes make an astounding impact to the overall quality of the resulting visualization. Firs, lets get rid of unnecessary elements. Lets remove the grid lines on the y-axis extending horizontally from the condition names. Lets also get rid of the axis titles, which only clutter the plot, as well as the legend that appeared when we added fills to the plot. Were doing all of those things using the theme() function, which we had already encountered before. theme() takes several arguments which relate to, for example, the legend of the plot, the scales, as well as the labels and titles. Were specifying that wed like to remove the title axes, as in axis.title.x = element_blank(), and also telling ggplot() to remove the major grids of the y-axis, which are always plotted by default and not always necessary. Finally, were also calling legend_position(), and specifying that the legend should be removed altogether. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;) Now lets improve the quality of the labels on the different axes. Lets make them more visible by increasing their size and making them bold. Notice how we specify axis.text.x = element_text(), and within that function we modify the relevant parameters. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14), axis.text.x = element_text(face = &quot;bold&quot;, size = 14)) The next step is to increase the distance between the labels and the actual graph. Again notice the usage of the arguments in the element_text() functions. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0))) Now, lets capitalize the labels on the y-axis. Theres at least two options to do that. We could recode the underlying factor in rat_comp, though that would change the actual data representation in R. A less permanent solution is to tell ggplot() to replace the existing labels with new ones, which will not change the underlying data representation. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0))) + scale_y_discrete(labels = c(&quot;metonymic&quot; = &quot;Metonymic&quot;, &quot;metaphoric&quot; = &quot;Metaphoric&quot;)) Last, lets add a title to the plot. Note that in many cases titles need not be added to plots, as that can be done straight in the text/ presentation editor one is using (including R, as we will see later). rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;metonymic&quot; = &quot;Metonymic&quot;, &quot;metaphoric&quot; = &quot;Metaphoric&quot;)) + ggtitle(&quot;Comprehensibility ratings&quot;) To finish up, lets make similar changes to our combined plot, which will require some further adjustments. Lets use the exact same code from above, replacing type with study on the y-axis and bringing the legend back. Lets also center the title of our legend and give the plot the right name. rat_combi %&gt;% ggplot(aes(rat, study, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;metonymic&quot; = &quot;Metonymic&quot;, &quot;metaphoric&quot; = &quot;Metaphoric&quot;)) + ggtitle(&quot;Transparency ratings&quot;) Much like we did earlier, lets adjust the distance between the boxes and the groups in the geom_boxplot() call. We should also fix the labels, both on the y-axis and in the legend. We already know how to change the distance between our boxes, in fact we can just recover our code from earlier. As for the labels, notice the new arguments in the scale_fill_brewer() call. rat_combi %&gt;% ggplot(aes(rat, study, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_fill_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) One more thing to do: lets play with transparency so as to highlight the contrast between the two plotted measures. Here were using a trick which is to specify that alpha should refer to an interaction between our two grouping variables, type and study, and then to set alpha values manually for the different combinations of the two variables. Lets make the relation boxes look bright and the comprehensibility ones look dark. rat_combi %&gt;% ggplot(aes(rat, study, fill = type, alpha = interaction(type, study))) + geom_boxplot(lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_fill_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + scale_alpha_manual(values = c(1, 1, .4, .4), guide = FALSE) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) This worked as intended. In order to achieve our end result and thus highlight the contrasts of interest, we made use of color and transparency, combined with a vertical collocation of the two contrasted groups. We could have used other combinations of techniques, though. As weve seen, these can include, among other things, changing the color of our geoms, changing the geoms themselves, facetting groups, but also changing the size or orientation of the plotted elements. As an example of such possible variations, lets use color instead of fill to highlight type. Lets also change the color of the boxes to gray, to increase the visual contrast with the white background. This should also help us understand the difference between fill and color. rat_combi %&gt;% ggplot(aes(rat, study, color = type)) + geom_boxplot(fill = &quot;light gray&quot;, lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) What we did was to color the lines in the boxplot according to the expression type and to fill the boxes with a specified color. Notice how color varies as a function of the variable type while color, which is manually coded, remains constant across both groups. We can now export our plot, say if wed like to save it locally on our machine as an image file. Lets export the plot with transparency added to it as a .png file, which can be done using the ggsave() function. You might need to adjust the width and height arguments within ggsave() to get your plot to be exported with the right dimensions. rat_combi %&gt;% ggplot(aes(rat, study, fill = type, alpha = interaction(type, study))) + geom_boxplot(lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_fill_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + scale_alpha_manual(values = c(1, 1, .4, .4), guide = FALSE) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) ggsave(&quot;ratings-transparency.png&quot;) ## Saving 7 x 5 in image 3.4 Session summary As weve seen in our last two sessions, geoms are what shape the type of graph one ends up with when using ggplot2. The single most important aspect of a geom is what type and number of variables its aesthetics support. Weve seen examples of geoms that take one or two variables. In the case of geoms that support two variables, like geom_boxplot(), we saw that these can consist either of two variables of the same type, be it continuous or discrete, or a combination of both. In order to produce a plot that renders the data correctly, one needs to understand how the plotted variables are coded in R, so that they match the variable type supported by the selected geom. One also needs to understand what the data corresponds to in the wild, that is, in terms of its original measurement type. This might impact how the data is plotted regardless of its actual representation in R. Knowing what variables a geom requires and whether ones data matches those requirements is a good recipe for plotting data correctly; In order to explore data for the purposes of data analysis, we saw that a good starting point is with plots and visual representations of the data. Often times, however, one will need to wrangle the data in order to produce certain plots. In addition to wrangling, which can be done with the help of many specialized packages like tidyverses dplyr, one usually needs to summarize the data in different ways or to calculate different descriptive statistics on the basis of the raw data, which themselves can also added to plots in different ways. Exploring a data set will usually involve a combination of visualization, wrangling, and numerical/ statistical summarizing; In order to enrich ggplot2s base visualizations, one can make use of different aesthetic specifications. As weve seen, these include specifications which affect major elements of a plot, such as fill, color, and alpha, geom-specific arguments like lwd or linetype, but also arguments which affect minor elements of the plot such as the legend, the title, and the axes, usually specified within the theme() layer. Importantly, as we will see later, where an aesthetic is specified can impact how the plots looks like: whatever is called within the ggplot() function will affect all layers in a plot, while aesthetics called within specific geoms will only affect that geoms layer. In addition to specifying aesthetics, using facets or varying the mapping of variables onto the x, y coordinates can also help highlight certain contrasts in a graph; Plots can be exported from R using the function ggsave(), which by default saves the last plotted graph. ggsave() takes the arguments width and height which can be modified in order to change the dimensions of the saved plot. "],["exploring-reaction-time-data.html", "Session 4 Exploring reaction time data 4.1 Laying out the data 4.2 Exploring the data 4.3 Session summary", " Session 4 Exploring reaction time data 4.1 Laying out the data In the last two sessions, we went over the steps of producing and refining visualizations in R using ggplot2. So far, weve been working with ratings, which we plotted using geom_bar() and geom_boxplot(), both well suited for plotting discrete variables, like ordinal and nominal data. Starting today, we will be working with a different type of data, namely reaction times, which are continuous in nature. For that, and building on the foundations we already have, we will explore some new visualization techniques as well as some new assumptions about data visualization itself. Lets get started by getting to know our data and what it refers to. Whereas in the first study we looked at the question of interest was how metaphors and metonomies are perceived in terms of their non-literal meaning, which we explored in terms of one of the properties investigated in the study, namely transparency, in the study were looking at today the question of interest is whether the semantic structure of an idiom, whether its metaphorical or metonymic or maybe even literal, might affect how it is processed in real time. Studies with a focus on online, or real time, processing investigate how people understand linguistic expressions as they are processed, for instance, in listening or reading, as opposed to how they might be interpreted in offline tasks where the time taken to react to or make a decision based on the linguistic stimuli is not the primary measure of interest. In order to investigate their research question, Michl (2019b) used a reading task where participants were asked to read different types of idioms, presented to them in chunks. In addition to metaphors and metonomies, participants also read non-idiomatic expressions, which acted as controls for idiomaticity, as well as literal idiomatic expressions, that is, constructions which fall under a definition of idiomatic language but not under a definition of non-literal language. Lets take a look at what the stimuli looked liked: Metonomy - Diese Bücher | bezahlt Julia aus eigener Tasche | und das ist ärgerlich These books | pays Julia out of her own pocket (= Julia pays out of her own pocket) | and that is annoying Control - Diese Bücher | schleppt Eva in der eigenen Tasche | und das ist ärgerlich These books | drags Eva insider her own bag | and that is annoying Metaphor - Wieder einmal | beißt Katrin in den sauren Apfel | und stimmt allem zu Once more | bites Katrin the sour apple (= Katrin bites the bullet) | and agrees to everything Control - Wieder einmal | wählt Katrin einen sauren Apfel | und stimmt allem zu Once more | picks Katrin a sour apple | and agrees to everything Literal idiom - Bei dem Ausflug | bringt Stefan die Kinder in Gefahr | aber alle haben Spaß At the trip | brings Stefan the children in danger (= Stefan puts the children in danger) | but everyone has fun Control - Bei dem Ausflug | sieht Stefan die Kinder in Gefahr | aber alle haben Spaß At the trip | sees Stefan the children in danger (= Stefan thinks the children are in danger) | but everyon has fun Reading times were measured at each chunk of the read stimuli, such that the time taken to read the idiomatic chunks could be compared across different idiom types. The participants who took part in the study were also asked to complete an offline interpretation task, namely a rating task similar to the ones from the previous study. We will come back to the ratings next session. For now, lets look at the reading times. Unlike the ratings study, which was exploratory in nature, in the reading study a specific hypothesis was tested. Lets look at the predictions described in the paper, so that we can assess whether the data supports or contradicts the formulated hypothesis. The original text reads: A general processing advantage is expected for idioms. If only idiomaticity (i.e. being an idiom and recognizable as such) matters, literal or nonliteral structure should not be reflected in processing speed; consequently, all idioms should be processed equally fast, regardless of whether they are literal, metonymic, or metaphoric, as long as length, familiarity, and comprehensibility are controlled for. If, however, the differing nonliteralness is processed or at least accessed in the idiom, literal idioms should be easiest to process, hence read the fastest, metaphors should be read the most slowly, while reading times for metonymies should be in-between. (Michl, 2019b, p. 1289) In other words, we should expect to find reading time differences between the idiom types, such that the time taken to read the idiomatic chunk of an expression should vary as a function of its semantic structure, more literal idioms being processed faster than less literal ones. Notice that were in the case of a confirmatory analysis: either the data supports the prediction that more literal idioms are read faster, with systematic differences between all three idiom types, or it supports the alternative prediction that all idiom types are read alike, without any systematic differences between groups. All in all, we should expect a baseline difference between idioms and controls (non-idioms). Lets then plot our raw reading times (RTs from now on). Lets start by plotting the entire response distribution, which might be helpful to understand what the underlying data actually consists of. First, lets see how the corresponding variable is represented in R. str(rts$RT) ## num [1:7022] 937 808 1249 1085 826 ... As would be expected, our variable is a numeric vector containing the RTs recorded in the experiment. Each value in the vector corresponds to one measurement, that is, one sentence chunk of one particular item, as read by a particular participant in a given trial. Since we are dealing with interval data, lets start with geoms which might be more appropriate for plotting continuous variables. One of these geoms is geom_density(), which plots a density distribution, a useful way of visualizing the shape of a distribution. Lets call geom_density(), mapping the RTs onto the x-axis. rts %&gt;% ggplot(aes(RT)) + geom_density() This plot only provides us with limited information, but looking at the raw distribution and understanding what it represents is a good starting point before we start grouping the data according to our variables of interest. As we can see, the RT distribution is right-skewed, which means that the bulk of responses is found closer to zero, the absolute minimum RT. While the mode of the distribution, its most common value, lies around 1000 ms, we can see that responses become rarer and more extreme as we move away from around 2000 ms towards larger values. We can assume that extreme values are not particularly representative of the actual population-level reading patterns, which is what we are interested in. Another important point to take away from this plot, also related to its right skew, is that no response actually ever reaches zero, which has to do with what was measured in the experimental task, namely reaction times. In this particular case, were dealing with the time needed for a participant to trigger a reading segment and then to read whatever they were presented with in that segment. As such, RT measurements include the time needed to prepare and execute the motor action of pressing a button or key in order to trigger a reading chunk. What that means, in practical terms, is that even if a participant is extremely fast in reading a given sentence segment, the resulting measurement can only ever be so short, as it includes not only the time taken to read and process the linguistic stimulus per se but also the motor routines involved in initiating the presentation of the stimulus. With those caveats in mind, lets now use another geom, geom_histogram(), to produce a plot where we can inspect what our distribution is composed of. A histogram shows a distribution in terms of bins of responses, where each bin contains a fixed number of observations. If we check the warning messaged produced by our geom_histogram() call, we see that ggplot2 gives us extra information about the binning of the data. rts %&gt;% ggplot(aes(RT)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. In this plot, the RT distribution is shown in terms bins of responses, where by default we get 30 bins, each containing whatever number of data points fall within that specific RT range. As the geom_histogram() documentation will tell us, we should always override the default width of the bins, exploring widths that best fit whatever were trying to explore and/ or communicate in our data. Lets then set the binwidth argument within geom_histogram() to 100, meaning that were discretizing our empirical response distribution into response bins of 100 ms, which sounds like a reasonable range for now. rts %&gt;% ggplot(aes(RT)) + geom_histogram(binwidth = 100) As we can see, our histogram now shows bins that contain data points that fill within a range of 100 ms, starting from zero. In order to highlight the binning of the data, lets add a contour to each bin and lets also increase the transparency of the plot. rts %&gt;% ggplot(aes(RT)) + geom_histogram(binwidth = 100, color = &quot;black&quot;, alpha = .7) This plot shows us roughly the same as our original density plot but in a way that its clearer what the actual number of values per fraction of the measurement scale is. Lets now calculate the mean of the distribution, which is our preferred measure of central tendency for the data at hand, and lets then overlay it to the plot to serve as a reference point. mean(rts$RT) ## [1] 1408.883 Given that the mean response value is about 1400 ms, we could break our histogram into bins of 200 ms, such that each bin reflects a 1/7 step from the mean value. Both 100 and 200 ms are appropriate values, in fact practically any positive divisor of the mean would be, the important thing to notice here is that different bin widths are more or less reasonable depending on what we are trying to visualize. Since were just trying to get an overview of the data, either 100 or 200 ms will do. rts %&gt;% ggplot(aes(RT)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .7) + geom_vline(aes(xintercept = mean(RT, na.rm = TRUE)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) Our plot now shows both the grand mean, marked in red, as well the amount of responses that lie within +/- 200 ms increments of the mean, which is what is shown by the bins. Now that we have an overview of our response distribution, lets move on to add information to the plot so that we can learn more about our main question of interest, which is whether there are differences between the means of the different idiom types. Lets try to add that to our plot by specifying fill within our ggplot() call. rts %&gt;% ggplot(aes(RT, fill = cond)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .7) + geom_vline(aes(xintercept = mean(RT, na.rm = TRUE)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) Here, we see the response distribution plotted in terms of the different expression types. At first glance, the graph seems to suggest that metaphors, in blue, were read more slowly than both metonomies and literal idioms, in purple and green, respectively. However, remember that what matters in this plot is not only the height of the stacked bars, which indicate the amount of responses for a given numerical range, but also where on the scale the bars lie. So, in short, by looking at this plot its practically impossible to tell, on average, which idiom type was read the slowest. One thing we can do to help us visualize what were interested in is to instead plot the RTs on the y-axis, which allows us to map higher concentrations of bars high up on the y-axis to larger RTs. Lets also plot our grand mean as a horizontal line. rts %&gt;% ggplot(aes(y = RT, fill = cond)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .7) + geom_hline(aes(yintercept = mean(RT, na.rm = TRUE)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) This is more helpful than what we had before, but perhaps instead of plotting a single response distribution, we might want to unstack the bars to help us a clearer visualization of our groups. Lets then specify the position argument within our geom_histogram() call to plot the individual distributions of each expression type. Recall that in earlier sessions we used position_dodge to split stacked bars into horizontally aligned bars. Now, were using position_identity() to split the entire response distribution into overlapping distributions. rts %&gt;% ggplot(aes(y = RT, fill = cond)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .7, position = position_identity()) + geom_hline(aes(yintercept = mean(RT, na.rm = TRUE)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) Note that the position adjustment argument takes both a function, as in the code above, or a string, as in the code below. rts %&gt;% ggplot(aes(y = RT, fill = cond)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .7, position = &quot;identity&quot;) + geom_hline(aes(yintercept = mean(RT, na.rm = TRUE)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) We now see all four expression distributions plotted one on top of the other. However, this is hardly more informative than our previous graph, given that with the visual overlap its hard to see the distributions plotted in the back. Lets increase the transparency to see if it helps decluttering the plot. rts %&gt;% ggplot(aes(y = RT, fill = cond)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .3, position = &quot;identity&quot;) + geom_hline(aes(yintercept = mean(RT, na.rm = TRUE)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) Even if we increase the transparency, its still hard to tell apart the overplotted groups. Lets then try some alternative ways of plotting this data. First, lets call our old acquaintance, facet_wrap(). rts %&gt;% ggplot(aes(y = RT, fill = cond)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .3, position = &quot;identity&quot;) + geom_hline(aes(yintercept = mean(RT, na.rm = TRUE)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) + facet_wrap( ~ cond) Here we can see, much better than before at least, what the actual group distributions look like. By eyeballing the data, it seems that metaphors are the idiom type that is read the slowest, followed by metonomies and literal idioms. We can tell that by roughly estimating the concentration of data points higher up on the RT scale, were in the realm of rough estimations here. Lets calculate the actual group means and add those to the plot instead of the grand mean so as to have clearer reference points. Notice the usage of the group argument within geom_hline(). rts %&gt;% group_by(cond) %&gt;% summarize(mean(RT)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## cond `mean(RT)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 control 1429. ## 2 literal 1384. ## 3 metaphoric 1394. ## 4 metonymic 1385. rts &lt;- rts %&gt;% group_by(cond) %&gt;% mutate(mean = mean(RT)) rts %&gt;% ggplot(aes(y = RT, fill = cond)) + geom_histogram(binwidth = 200, color = &quot;black&quot;, alpha = .3, position = &quot;identity&quot;) + geom_hline(aes(yintercept = mean, color = cond, group = cond), linetype = &quot;dashed&quot;, size = 1) + facet_wrap( ~ cond) Even with the group means added to plot, its hard to tell whats going on, and that is because the differences between means are quite small, as can be seen by inspecting the actual calculated values. What this means is that our plot is not particularly informative, despite our attempts to improve it. Lets try plotting the data using another geom then. First, lets use geoms we already know, like geom_boxplot(). Well keep mapping the RT to the y-axis and were now mapping idiom type to the x-axis. rts %&gt;% ggplot(aes(cond, RT, fill = cond)) + geom_boxplot() This plot is not very informative either: it is hard to notice whether there are systematic differences between the groups, even though at this point we know already that there are small differences between the means. Also, as we know boxplots give us medians and interquartile ranges, and not what we are after, which are the means. We could add horizontal lines to the plot, much like we did above, indicating the respective group means, but what would only contribute to making the plot more convoluted and therefore less transparent. Instead, lets try filtering out some of the data to see if we can notice any potential differences. Earlier we said that values above 2000 ms seemed to be outliers, which of course was a very rough estimation based on eyeballing the data. If we look at our boxplot, we see that the upper box whiskers reach around 3000 ms, meaning that anything above that is beyond 1.5 times the interquartile range. We can be quite certain that any value that far off from the median is not very representative, so lets focus on RTs that are shorter than 3000 ms. rts %&gt;% filter(RT &lt; 3000) %&gt;% ggplot(aes(cond, RT, fill = cond)) + geom_boxplot() Even having filtered the data, the resulting plot is not much better: with some squinting it seems that the median for the controls is higher than that of the other groups, though we cant really tell if there are any differences between the idioms. Lets try a different approach: lets use a geom_violin(), which, as we saw, was a useful way of combining a density plot with a boxplot. Instead of always having to filter out the relevant RT range using filter(), lets create a new data frame called rts_filter which includes the filtered RTs. rts_filter &lt;- rts %&gt;% filter(RT &lt; 3000) rts_filter %&gt;% ggplot(aes(cond, RT, fill = cond)) + geom_violin() Again, not the most informative of plots, but here we get more detail about the density of responses at each section of the RT scale. If we look at the section between 1000 and 2000 ms, we see that controls and metaphors seem to have more of a spread, whereas literal idioms and metonomies seem to have a thicker concentration of data points around 1000 ms. Since weve already noticed that the differences were interested in are quite minimal, we might want to use a graph that allows us to look at single points of interest, namely the means themselves, as opposed to whole distributions of response values. When reporting a summary statistic like a sample mean its also customary to show a measure of variability in the data, that is, a measure of how much the values in the sample diverge from the value represented by the summary statistic. In our case, we can rely on the middle 95% of the data, which we can then use to calculate a confidence interval. A confidence interval tells us, to a certain degree of certainty, what the plausible range of values for the population mean might be. That way, when plotting our means, we can also plot their confidence intervals, so that when comparing means we can have a better idea of whether their likely ranges overlap or not. Ultimately, this is important as were interested in inferring something about the larger population our empirical sample refers to. Lets then start by creating a new data frame called mean_RT which contains the summaries we want to plot, namely the group means. To that data frame we will eventually add the lower and upper limits of our confidence interval, which by convention usually correspond to the 2.5th and the 97.5th percentiles of the distribution. What we are going to do, however, is that we are going to simulate more samples based on our empirical distribution, so that the percentiles are based not only on the original sample but rather on an approximation of what the population distribution might be. This is called bootstrapping, and it is based on sampling the distribution with value replacement. Here were using a function called rep_sample_n() from the external package infer. What this function allows us to do is to resample our empirical distribution so that we can generate a number of new samples based on which were calculating our percentiles. Lets draw 1000 samples. Lets then plot the bootstrapped distribution to make sure it resembles, somewhat at least, our original response distribution. mean_RT &lt;- rts_filter %&gt;% group_by(cond) %&gt;% summarize(mean = mean(RT)) ## `summarise()` ungrouping output (override with `.groups` argument) bootstrapped_samples &lt;- rts_filter %&gt;% infer::rep_sample_n(size = nrow(rts_filter), replace = TRUE, reps = 1000) rts_filter %&gt;% ggplot(aes(RT)) + geom_density() + ggtitle(&quot;Empirical&quot;) bootstrapped_samples %&gt;% ggplot(aes(RT)) + geom_density() + ggtitle(&quot;Bootstrapped&quot;) The two distributions have similar shapes. Still, you might be wondering why the bootstrapped distribution, which is based on 1000 samples, looks worse than the empirical distribution, which is based on a single sample. That has to do with the fact that in the case of the bootstrapped distribution, given that we have many samples, were much more certain about the actual shape of the underlying response distribution, which is what a density plot allows us to visualize, that is, an estimation of the data generating distribution based on observed data. Now, lets calculate the means of our samples, and on the basis of that lets create our desired confidence intervals, which were going to store as bootstrapped_CI. Well then combine mean_RT and bootstrapped_CI, and we should then inspect the summaries before feeding the data to a ggplot() call. Notice that, differently from what we did before in our course with bind_rows(), were not joining two data frames which have the exact same variables, where wed simply be combining observations, but rather were joining two data frames which have different variables which need to be linked over a common variable, in our case cond. For that, were using the left_join() function from dplyr. bootstrapped_mean_RT = bootstrapped_samples %&gt;% group_by(replicate, cond) %&gt;% summarize(boot_mean = mean(RT)) ## `summarise()` regrouping output by &#39;replicate&#39; (override with `.groups` argument) bootstrapped_CI = bootstrapped_mean_RT %&gt;% group_by(cond) %&gt;% summarize(CILow = quantile(boot_mean, .025), CIHigh = quantile(boot_mean, .975)) ## `summarise()` ungrouping output (override with `.groups` argument) mean_CI_RT &lt;- left_join(mean_RT, bootstrapped_CI) ## Joining, by = &quot;cond&quot; mean_CI_RT ## # A tibble: 4 x 4 ## cond mean CILow CIHigh ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 control 1340. 1320. 1359. ## 2 literal 1305. 1269. 1341. ## 3 metaphoric 1326. 1297. 1355. ## 4 metonymic 1303. 1270. 1337. mean_CI_RT %&gt;% ggplot(aes(cond, mean)) + geom_point(aes(color = cond)) What we have plotted here are the group means, without the respective confidence intervals for now, just so its clear that the two are plotted in different layers. Its quite hard to notice the points, so lets increase their size, and lets add the confidence intervals using geom_errobar(), also with slightly thicker line size. Notice how we specify the limits of our intervals using the ymin and ymax arguments within geom_errorbar(). mean_CI_RT %&gt;% ggplot(aes(cond, mean)) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond), size = 1) + geom_point(aes(color = cond), size = 5) Now we have the plot we wanted: we can see the group means with their respective bootstrapped confidence intervals. Visual inspection of the plot suggests that there are differences between the controls and the idiomatic expressions. Moreover, there seem to be differences between the different idiomatic expressions, literal idioms and metonomies being, on average, read faster than metaphors, which seem to be read slightly faster than controls. Now, one thing to notice here is that were plotting all RTs together, even though the measurements refer to different sentence segments. Recall that in the task participants read the expressions in chunks, as in Once more | Katrin bites the bullet | and agrees to everything, the second chunk, the one containing an idiom, being the main region of interest. Lets replot our data in terms of both the idiom chunk and the region following it, the post-idiom chunk. For that, well need to re-calculate the means grouping the data not only by expression type but also by reading chunk. Then, in the new plot, well also need to dodge our points and their confidence intervals, otherwise they will be overplotted, as has happened before to us. Notice the argument in the position_dodge() calls. mean_RT &lt;- rts_filter %&gt;% group_by(cond, chunk) %&gt;% summarize(mean = mean(RT)) ## `summarise()` regrouping output by &#39;cond&#39; (override with `.groups` argument) bootstrapped_mean_RT &lt;- bootstrapped_samples %&gt;% group_by(replicate, cond, chunk) %&gt;% summarize(boot_mean = mean(RT)) ## `summarise()` regrouping output by &#39;replicate&#39;, &#39;cond&#39; (override with `.groups` argument) bootstrapped_CI &lt;- bootstrapped_mean_RT %&gt;% group_by(cond, chunk) %&gt;% summarize(CILow = quantile(boot_mean, .025), CIHigh = quantile(boot_mean, .975)) ## `summarise()` regrouping output by &#39;cond&#39; (override with `.groups` argument) mean_CI_RT &lt;- left_join(mean_RT, bootstrapped_CI) ## Joining, by = c(&quot;cond&quot;, &quot;chunk&quot;) mean_CI_RT %&gt;% ggplot(aes(chunk, mean)) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond), size = 1, position = position_dodge(.7)) + geom_point(aes(color = cond), size = 5, position = position_dodge(.7)) This is closer to what we might actually want as a final product: we can see the mean RTs plotted in terms of both the sentence regions and the expression types. Visual inspection of the plot suggests that there are descriptive differences between the different expression types at each of the two plotted sentence regions. The pattern found in the idiom region seems to map rather directly onto the pattern found in the aggregated data, whereas in the post-idiom region there is considerably more overlap between the conditions, indicating that any early effects of idiomaticity, especially in the case of metonomies, are evened out at later stages of processing, once the idiomatic chunk has been processed. Note, however, that so far we have plotted a single control condition, which is how the cond variable is represented in our data frame, with four distinct levels, three idiomatic and one control. We do know, though, that each idiom type has its own set of non-idiomatic control expressions, which is why we might expect differences between each control set, as the items in each set have been designed to match, as closely as possible, their idiomatic counterparts, whether literal, metaphoric, or metonymic. Lets then create a new variable, which we can call cond2, where we have information about all control sets, resulting in a factor with six levels instead of four. For that were going to need to know which controls match which idioms. Fortunately, the items are coded using the same unique IDs for both idioms and their respective controls. Lets inspect our data set to see what that looks like. Were simply printing our items as a function of our conditions, accessing the entire data. table(rts_filter$cond, rts_filter$itemnr) ## ## 3 8 17 18 22 29 31 36 38 44 51 52 55 58 60 62 65 69 71 74 76 82 ## control 42 39 38 33 42 41 39 39 39 42 42 40 41 37 40 38 38 33 40 42 39 38 ## literal 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## metaphoric 42 0 38 0 39 41 41 41 38 42 39 41 0 38 0 0 0 35 0 41 0 0 ## metonymic 0 42 0 41 0 0 0 0 0 0 0 0 41 0 42 39 39 0 39 0 40 39 ## ## 83 85 92 96 98 99 101 104 106 108 115 118 125 130 138 140 141 142 ## control 33 41 41 42 39 41 40 40 37 37 36 35 41 39 38 40 39 41 ## literal 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## metaphoric 36 42 39 41 42 0 41 41 35 40 0 0 42 0 41 39 0 40 ## metonymic 0 0 0 0 0 38 0 0 0 0 41 31 0 41 0 0 40 0 ## ## 143 144 146 150 153 154 180 188 192 195 204 206 207 208 211 215 ## control 42 42 39 42 42 41 38 35 41 39 41 39 35 41 42 41 ## literal 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## metaphoric 41 0 42 0 42 0 42 0 0 40 0 0 0 42 40 41 ## metonymic 0 39 0 37 0 40 0 34 41 0 37 39 39 0 0 0 ## ## 216 217 219 224 229 233 235 242 250 251 252 253 261 264 265 268 ## control 32 41 36 41 41 41 34 41 38 39 32 38 40 38 41 37 ## literal 0 0 0 0 0 0 0 0 40 38 39 42 32 40 41 39 ## metaphoric 37 40 41 40 42 0 36 0 0 0 0 0 0 0 0 0 ## metonymic 0 0 0 0 0 42 0 39 0 0 0 0 0 0 0 0 ## ## 272 279 282 287 292 296 297 298 302 303 304 308 314 318 ## control 41 41 41 41 39 35 40 40 36 40 38 40 41 39 ## literal 41 39 41 39 42 41 42 39 36 40 40 41 41 42 ## metaphoric 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## metonymic 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Here we can see how many instances of a given item, indicated by the columns, are found in the data set for each condition, indicated by the rows. As we can see, a given item only ever appears as a control and as a single idiom type, which means that those are control-target pairs. On the basis of this information, we can create a new variable which tell us whether a given item is a control for a literal idiom, for a metaphor, or for a metonomy. We can then use that information to create cond2, as explained above. Lets wrangle our data and then try to plot it again, using our newly created wrangled information about the controls. In order to increase the legibility of the graph, lets map controls and idioms to different shapes, namely circles and squares. Notice the usage of scale_shape_manual(), where we manually specify which levels from cond2 are to be mapped onto which shapes. rts_filter &lt;- rts_filter %&gt;% mutate(control2 = case_when(itemnr %in% c(250:318) ~ &quot;control_literal&quot;, itemnr %in% c(3, 17, 22:52, 58, 69, 74, 83:98, 101:108, 125, 138:140, 142:143, 146, 153, 180, 195, 208:229, 235) ~ &quot;control_metaphoric&quot;, itemnr %in% c(8, 18, 55, 60:65, 71, 76:82, 99, 115:118, 130, 141, 144, 150, 154, 188:192, 204:207, 233, 242) ~ &quot;control_metonymic&quot;)) %&gt;% mutate(cond2 = case_when(cond == &quot;literal&quot; ~ &quot;literal&quot;, cond == &quot;metaphoric&quot; ~ &quot;metaphoric&quot;, cond == &quot;metonymic&quot; ~ &quot;metonymic&quot;, control2 == &quot;control_literal&quot; ~ &quot;control_literal&quot;, control2 == &quot;control_metaphoric&quot; ~ &quot;control_metaphoric&quot;, control2 == &quot;control_metonymic&quot; ~ &quot;control_metonymic&quot;)) rts_filter$cond2 &lt;- factor(rts_filter$cond2, levels = c(&quot;control_literal&quot;, &quot;literal&quot;, &quot;control_metaphoric&quot;, &quot;metaphoric&quot;, &quot;control_metonymic&quot;, &quot;metonymic&quot;)) mean_RT_control &lt;- rts_filter %&gt;% group_by(cond2) %&gt;% summarize(mean = mean(RT)) ## `summarise()` ungrouping output (override with `.groups` argument) bootstrapped_samples &lt;- rts_filter %&gt;% infer::rep_sample_n(size = nrow(rts_filter), replace = TRUE, reps = 1000) bootstrapped_mean_RT_control = bootstrapped_samples %&gt;% group_by(replicate, cond2) %&gt;% summarize(boot_mean = mean(RT)) ## `summarise()` regrouping output by &#39;replicate&#39; (override with `.groups` argument) bootstrapped_CI_control = bootstrapped_mean_RT_control %&gt;% group_by(cond2) %&gt;% summarize(CILow = quantile(boot_mean, .025), CIHigh = quantile(boot_mean, .975)) ## `summarise()` ungrouping output (override with `.groups` argument) mean_CI_RT_control &lt;- left_join(mean_RT_control, bootstrapped_CI_control) ## Joining, by = &quot;cond2&quot; mean_CI_RT_control %&gt;% ggplot(aes(cond2, mean)) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), size = 1) + geom_point(aes(color = cond2, shape = cond2), size = 5) + scale_shape_manual(values = c(&quot;circle&quot;, &quot;square&quot;, &quot;circle&quot;, &quot;square&quot;, &quot;circle&quot;, &quot;square&quot;)) The resulting plot is quite informative: as we can see, each set of controls behaves differently. Importantly, we can see that the baseline effect of idiomaticity is still there, that is, controls are read, on average, more slowly than their idiomatic counterparts, despite differences between each control set. Lets now plot the graph showing the split between the idiom and the post-idiom sentence regions. rts_filter &lt;- rts_filter %&gt;% mutate(control2 = case_when(itemnr %in% c(250:318) ~ &quot;control_literal&quot;, itemnr %in% c(3, 17, 22:52, 58, 69, 74, 83:98, 101:108, 125, 138:140, 142:143, 146, 153, 180, 195, 208:229, 235) ~ &quot;control_metaphoric&quot;, itemnr %in% c(8, 18, 55, 60:65, 71, 76:82, 99, 115:118, 130, 141, 144, 150, 154, 188:192, 204:207, 233, 242) ~ &quot;control_metonymic&quot;)) %&gt;% mutate(cond2 = case_when(cond == &quot;literal&quot; ~ &quot;literal&quot;, cond == &quot;metaphoric&quot; ~ &quot;metaphoric&quot;, cond == &quot;metonymic&quot; ~ &quot;metonymic&quot;, control2 == &quot;control_literal&quot; ~ &quot;control_literal&quot;, control2 == &quot;control_metaphoric&quot; ~ &quot;control_metaphoric&quot;, control2 == &quot;control_metonymic&quot; ~ &quot;control_metonymic&quot;)) rts_filter$cond2 &lt;- factor(rts_filter$cond2, levels = c(&quot;control_literal&quot;, &quot;literal&quot;, &quot;control_metaphoric&quot;, &quot;metaphoric&quot;, &quot;control_metonymic&quot;, &quot;metonymic&quot;)) mean_RT_control &lt;- rts_filter %&gt;% group_by(cond2, chunk) %&gt;% summarize(mean = mean(RT)) ## `summarise()` regrouping output by &#39;cond2&#39; (override with `.groups` argument) bootstrapped_mean_RT_control &lt;- bootstrapped_samples %&gt;% group_by(replicate, cond2, chunk) %&gt;% summarize(boot_mean = mean(RT)) ## `summarise()` regrouping output by &#39;replicate&#39;, &#39;cond2&#39; (override with `.groups` argument) bootstrapped_CI_control &lt;- bootstrapped_mean_RT_control %&gt;% group_by(cond2, chunk) %&gt;% summarize(CILow = quantile(boot_mean, .025), CIHigh = quantile(boot_mean, .975)) ## `summarise()` regrouping output by &#39;cond2&#39; (override with `.groups` argument) mean_CI_RT_control &lt;- left_join(mean_RT_control, bootstrapped_CI_control) ## Joining, by = c(&quot;cond2&quot;, &quot;chunk&quot;) mean_CI_RT_control %&gt;% ggplot(aes(chunk, mean)) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), size = 1, position = position_dodge(.7)) + geom_point(aes(color = cond2, shape = cond2), size = 5, position = position_dodge(.7)) + scale_shape_manual(values = c(&quot;circle&quot;, &quot;square&quot;, &quot;circle&quot;, &quot;square&quot;, &quot;circle&quot;, &quot;square&quot;)) Again, we see that there are differences between our control sets. The overall patterns are similar to what weve seen before though: at the idiom region, controls seem to be read more slowly than idioms, metaphors being read the slowest among idioms, and metonomies being read the fastest. At the post-idiom region, conditions largely overlap, indicating no late effect of either idiomaticity nor semantic structure. Lets explore the data some more to see if there are other relevant patterns worth highlighting in our final plot. 4.2 Exploring the data Lets try plotting the means of each participant who took part in the study, to see the degree of individual variability we have in the data. First, we need to calculate the participant means. Then, can plot them using the code from above. mean_RT_par &lt;- rts_filter %&gt;% group_by(subj, cond2, chunk) %&gt;% summarize(mean = mean(RT)) ## `summarise()` regrouping output by &#39;subj&#39;, &#39;cond2&#39; (override with `.groups` argument) bootstrapped_samples_par &lt;- bootstrapped_samples %&gt;% group_by(subj, replicate, cond2, chunk) %&gt;% summarize(boot_mean = mean(RT)) ## `summarise()` regrouping output by &#39;subj&#39;, &#39;replicate&#39;, &#39;cond2&#39; (override with `.groups` argument) bootstrapped_CI_par &lt;- bootstrapped_samples_par %&gt;% group_by(subj, cond2, chunk) %&gt;% summarize(CILow = quantile(boot_mean, .025), CIHigh = quantile(boot_mean, .975)) ## `summarise()` regrouping output by &#39;subj&#39;, &#39;cond2&#39; (override with `.groups` argument) mean_CI_RT_par &lt;- left_join(mean_RT_par, bootstrapped_CI_par) ## Joining, by = c(&quot;subj&quot;, &quot;cond2&quot;, &quot;chunk&quot;) mean_CI_RT_par %&gt;% ggplot(aes(chunk, mean)) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7)) + geom_point(aes(color = cond2, shape = cond2), position = position_dodge(.7)) + scale_shape_manual(values = c(&quot;circle&quot;, &quot;square&quot;, &quot;circle&quot;, &quot;square&quot;, &quot;circle&quot;, &quot;square&quot;)) We can see that theres variability in the data, but the plot is highly cluttered, so well have to declutter it. This will involve reducing the width of the error bars as well as changing the size and color contrast of the points. Changing the width of the error bars as well as changing the size of the plotted points requires specifying arguments in ways were already familiar with. Now, in order to strike a good visual contrast between the points and the bars, lets change the color argument within geom_point() to a fill and lets add color separately, so as to make the contours of the points black. For that, we will need to specify shapes which allow us to change both their fill and their color. Notice also the usage of the group argument within the geom_point() call, which tells ggplot() that each plotted point is to be filled with a color according to its respective group of origin. mean_CI_RT_par %&gt;% ggplot(aes(chunk, mean)) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22, 21, 22, 21, 22)) This is much better: there is still a lot of overlap between points but at least we can tell roughly where each individual mean is located. As was expected, participants vary considerably from one another in terms of their responses. Still, its hard to tell, for each participant, whether there are differences between expression types. In order to highlight any potential differences, lets add lines connecting each group, which we will map back onto the x-axis. Lets use facets to show the different reading segments. mean_CI_RT_par %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22, 21, 22, 21, 22)) + facet_wrap(~ chunk) We can see that some people read the controls more slowly, with small differences between the idiomatic expressions, while others read certain types of idioms more slowly than the controls. It would be ideal if we could tag our data, such that we can highlight the presence or absent of an effect, as indicated in terms of the means, by contrasting the tagged data. First, lets wrangle our data so as to create new variables which indicate, both for idiomaticity and semantic structure, whether the data suggests an effect or not. Lets then color the lines connecting each group in terms of whether an effect is found or not. Were plotting two graphs: one for idiomaticity and one for semantic structure. mean_RT_par_effect &lt;- mean_RT_par %&gt;% pivot_wider(names_from = cond2, values_from = mean) %&gt;% mutate(bl_effect = case_when(control_literal &gt; literal &amp; control_metaphoric &gt; metaphoric &amp; control_metonymic &gt; metonymic ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;), sem_str = case_when(metaphoric &gt; literal &amp; metaphoric &gt; metonymic &amp; metonymic &gt; literal ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;)) %&gt;% pivot_longer(cols = control_literal:metonymic, names_to = &quot;cond2&quot;, values_to = &quot;mean&quot;) mean_CI_RT_par_effect &lt;- left_join(mean_RT_par_effect, bootstrapped_CI_par) ## Joining, by = c(&quot;subj&quot;, &quot;chunk&quot;, &quot;cond2&quot;) mean_CI_RT_par_effect %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = bl_effect, group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22, 21, 22, 21, 22)) + facet_wrap(~ chunk) + theme(legend.position = &quot;bottom&quot;) + ggtitle(&quot;Idiomaticity&quot;) mean_CI_RT_par_effect %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = sem_str, group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22, 21, 22, 21, 22)) + facet_wrap(~ chunk) + theme(legend.position = &quot;bottom&quot;) + ggtitle(&quot;Semantic structure&quot;) Here we catch a glimpse of our expected effects, coded in terms of the original predictions: on average, participants seem not to take longer reading controls compared to idiomatic expressions. Moreover, on average, there seems to be no effect of semantic structure, at least not in terms of metaphors being read more slowly than both literal idioms and metonomies, and literal idioms being read the fastest. Even though the predictions are not borne out in the data, we do know at this point that the original hypothesis is not altogether unsupported by the data, according to what we learned from our visualizations. Lets then replot the comparisons above, coding the data according to each control-target comparison, so as to check whether the predictions are confirmed or disconfirmed by the data. Lets start by looking at the potential effects of idiomaticity. mean_RT_par_effect &lt;- mean_RT_par %&gt;% pivot_wider(names_from = cond2, values_from = mean) %&gt;% mutate(bl_effect_literal = case_when(control_literal &gt; literal ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;), bl_effect_metaphoric = case_when(control_metaphoric &gt; metaphoric ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;), bl_effect_metonymic = case_when(control_metonymic &gt; metonymic ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;)) %&gt;% pivot_longer(cols = control_literal:metonymic, names_to = &quot;cond2&quot;, values_to = &quot;mean&quot;) %&gt;% mutate(control = case_when(grepl(&quot;literal&quot;, cond2) ~ &quot;literal&quot;, grepl(&quot;metaphoric&quot;, cond2) ~ &quot;metaphoric&quot;, grepl(&quot;metonymic&quot;, cond2) ~ &quot;metonymic&quot;)) mean_CI_RT_par_effect &lt;- left_join(mean_RT_par_effect, bootstrapped_CI_par) ## Joining, by = c(&quot;subj&quot;, &quot;chunk&quot;, &quot;cond2&quot;) mean_CI_RT_par_effect %&gt;% filter(control == &quot;literal&quot;) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = bl_effect_literal, group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22)) + scale_fill_manual(values = c(&quot;#F8766D&quot;, &quot;#B79F00&quot;)) + facet_wrap( ~ chunk) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Idiomaticty - Literal idioms&quot;) mean_CI_RT_par_effect %&gt;% filter(control == &quot;metaphoric&quot;) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = bl_effect_metaphoric, group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22)) + scale_fill_manual(values = c(&quot;#00BA38&quot;, &quot;#00BFC4&quot;)) + facet_wrap( ~ chunk) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Idiomaticity - Metaphors&quot;) mean_CI_RT_par_effect %&gt;% filter(control == &quot;metonymic&quot;) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = bl_effect_metonymic, group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22)) + scale_fill_manual(values = c(&quot;#619CFF&quot;, &quot;#F564E3&quot;)) + facet_wrap( ~ chunk) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Idiomaticity - Metonomies&quot;) Lets concentrate on the idiom region, for simplicity. As we can see, the prediction that controls would be read more slowly than idioms seems to be at least somewhat borne out in the data, with considerable differences between idiom types. While metonomies seem to be read on average faster than their controls, metaphors and controls seem to be read more slowly than their respective controls. We would have not been able to spot this fine-grained distinction by looking at a composite plot like the one we had earlier, although the original plot with aggregated means already suggested differential effects of idiomaticity. Categorically speaking, however, we might have inferred that there is no effect. In summary, then, the original prediction seems to be supported in the case of metonomies but not in the case of the other two idiom types. Lets now look at the potential effects of semantic structure. mean_RT_par_effect &lt;- mean_RT_par %&gt;% pivot_wider(names_from = cond2, values_from = mean) %&gt;% mutate(sem_str_met_lit = case_when(metaphoric &gt; literal ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;), sem_str_met_met = case_when(metaphoric &gt; metonymic ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;), sem_str_met_lit = case_when(metonymic &gt; literal ~ &quot;effect&quot;, TRUE ~ &quot;no-effect&quot;)) %&gt;% pivot_longer(cols = control_literal:metonymic, names_to = &quot;cond2&quot;, values_to = &quot;mean&quot;) mean_CI_RT_par_effect &lt;- left_join(mean_RT_par_effect, bootstrapped_CI_par) ## Joining, by = c(&quot;subj&quot;, &quot;chunk&quot;, &quot;cond2&quot;) mean_CI_RT_par_effect %&gt;% filter(cond2 == &quot;metaphoric&quot; | cond2 == &quot;literal&quot;) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = sem_str_met_lit, group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22)) + scale_fill_manual(values = c(&quot;#B79F00&quot;, &quot;#00BFC4&quot;)) + facet_wrap( ~ chunk) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Semantic structure\\nMetaphors x Literal idioms&quot;) mean_CI_RT_par_effect %&gt;% filter(cond2 == &quot;metaphoric&quot; | cond2 == &quot;metonymic&quot;) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = sem_str_met_met, group = subj, linetype = sem_str_met_met), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22)) + scale_fill_manual(values = c(&quot;#00BA38&quot;, &quot;#F564E3&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;)) + facet_wrap( ~ chunk) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Semantic structure\\nMetaphors x Metonomies&quot;) mean_CI_RT_par_effect %&gt;% filter(cond2 == &quot;metonymic&quot; | cond2 == &quot;literal&quot;) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(color = sem_str_met_lit, group = subj, linetype = sem_str_met_lit), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2), size = 2, color = &quot;black&quot;, position = position_dodge(.7)) + scale_shape_manual(values = c(21, 22)) + scale_fill_manual(values = c(&quot;#F564E3&quot;, &quot;#B79F00&quot;)) + scale_color_manual(values = c(&quot;#F564E3&quot;, &quot;#B79F00&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;)) + facet_wrap( ~ chunk) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Semantic structure\\nMetonomies x Literal idioms&quot;) For the sake of brevity, lets again focus only on the idiom region. We now see the effects of semantic structure. While it was predicted that more literal idioms would be read faster, literal idioms being read the fastest, followed by metonomies, and then metaphors, we see that the data supports the hypothesis only partially. Metonomies are indeed read faster than metaphors, and so are literal idioms, however, metonomies are read faster than literal idioms, an unexpected response pattern. We thus see an overall effect of semantic structure, although one of the response patterns contradicts the formulated literalness account. In order to get an altogether different picture of what the patterns might be for each participant, lets facet them. Lets aggregate the two sentence regions again, for simplicity. Since we know were going to end up with more facets than we can fit within a single plot, lets plot several graphs, each with seven participants, one per facet. mean_RT_par_nochunk &lt;- rts_filter %&gt;% group_by(subj, cond2) %&gt;% summarize(mean = mean(RT)) ## `summarise()` regrouping output by &#39;subj&#39; (override with `.groups` argument) bootstrapped_samples_par_nochunk &lt;- bootstrapped_samples %&gt;% group_by(subj, replicate, cond2) %&gt;% summarize(boot_mean = mean(RT)) ## `summarise()` regrouping output by &#39;subj&#39;, &#39;replicate&#39; (override with `.groups` argument) bootstrapped_CI_par_nochunk &lt;- bootstrapped_samples_par_nochunk %&gt;% group_by(subj, cond2) %&gt;% summarize(CILow = quantile(boot_mean, .025), CIHigh = quantile(boot_mean, .975)) ## `summarise()` regrouping output by &#39;subj&#39; (override with `.groups` argument) mean_CI_RT_par_nochunk &lt;- left_join(mean_RT_par_nochunk, bootstrapped_CI_par_nochunk) ## Joining, by = c(&quot;subj&quot;, &quot;cond2&quot;) pd &lt;- position_dodge(.7) shapes_cond &lt;- scale_shape_manual(values = c(21, 22, 21, 22, 21, 22)) mean_CI_RT_par_nochunk %&gt;% filter(subj &lt;= 7) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = pd) + shapes_cond + facet_wrap(~ subj) mean_CI_RT_par_nochunk %&gt;% filter(subj &gt; 7 &amp; subj &lt;= 14) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = pd) + shapes_cond + facet_wrap(~ subj) mean_CI_RT_par_nochunk %&gt;% filter(subj &gt; 14 &amp; subj &lt;= 21) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = pd) + shapes_cond + facet_wrap(~ subj) mean_CI_RT_par_nochunk %&gt;% filter(subj &gt; 21 &amp; subj &lt;= 28) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = pd) + shapes_cond + facet_wrap(~ subj) mean_CI_RT_par_nochunk %&gt;% filter(subj &gt; 28 &amp; subj &lt;= 35) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = pd) + shapes_cond + facet_wrap(~ subj) mean_CI_RT_par_nochunk %&gt;% filter(subj &gt; 35 &amp; subj &lt; 42) %&gt;% ggplot(aes(cond2, mean)) + geom_line(aes(group = subj), size = .6, alpha = .6) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = position_dodge(.7), width = .1) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = pd) + shapes_cond + facet_wrap(~ subj) Again, we get quite a detailed picture of the amount of individual variation in the data. Participants vary considerably not only in their qualitative patterns but also on the average time taken to read the stimuli, as a whole. With these plots, however, its hard to tell if there are any systematic differences between groups. We might want to combine information about individual variation with information about the aggregated means. Lets overlay the group means to our plot showing the individual means. For that, well need to duplicate our geom_errorbar() and geom_point() layers, feeding the data with the aggregated means to the new layers. Lets also change the transparency of the individual means to highlight the group means. mean_CI_RT_par %&gt;% ggplot(aes(chunk, mean)) + geom_errorbar(aes(ymin = CILow, ymax = CIHigh, color = cond2), position = pd, width = .1, alpha = .2) + geom_errorbar(data = mean_CI_RT_control, aes(ymin = CILow, ymax = CIHigh, color = cond2), width = .3, size = 1, alpha = .8, position = pd) + geom_point(aes(fill = cond2, shape = cond2, group = cond2), size = 2, color = &quot;black&quot;, position = pd, alpha = .2) + geom_point(data = mean_CI_RT_control, aes(fill = cond2, shape = cond2, group = cond2), size = 3.5, color = &quot;black&quot;, position = pd, alpha = .9) + shapes_cond With this plot we get the whole picture: we can see the variation in the group means as well as well in the individual participant means. Still, the plot with only the aggregated means is more informative when it comes to our main question of interest, which is whether there are systematic differences, at the population level, between our different expression types. Individual variation seems to be random enough not to afford itself to being plotted with the aggregated means. Indeed, given the large degree of individual variation and the small differences between aggregated means, it is hard to interpret any population patterns based on the plot containing both. 4.3 Session summary Today we learned that we should start by plotting whole distributions or ranges of data before focusing on any groupings of interest when dealing with continuous data. That should give us, as analysts, an overview of what our data  and even more specifically, our main variable of interest  looks like. We can inspect the whole distribution of responses/ measurements and see if anything looks irregular or unexpected. From there, we can start digging deeper into the data in order to plot those patterns that help us answer our questions of interest; If we are interested in groups or clusters of data, we should eventually group it as necessary, ideally breaking the data into sub-distributions. This will usually involve introducing more variables into our visualizations and wrangling the data back and forth between different formats. If we are plotting point estimates, like sample means, we might need to calculate those values first; we will usually also want to have a measure of variability in the data, for instance a confidence interval. As weve seen, with ggplot2 we can plot point estimates using geom_point() and confidence intervals using geom_errobar(); If we are testing hypotheses, the first thing we should do after having calculated the comparisons of interest is to plot exactly the necessary comparisons, as formulated in the original predictions. We will want to check whether our predictions are borne out in the data, and, more specifically, for what combinations of groups/ comparisons. This might involve further exploring the data, however, the main plot used to communicate the results should clearly and unambiguously reflect the contrasts originally formulated in the study predictions. "]]
