[
["basics-of-data-wrangling-in-r.html", "Basics of data visualization for behavioral scientists Session 1 Basics of data wrangling in R 1.1 Installing R and RStudio 1.2 Creating data structures 1.3 Subsetting and manipulating data 1.4 Session summary 1.5 Exercises", " Basics of data visualization for behavioral scientists Vinicius Macuch Silva Session 1 Basics of data wrangling in R 1.1 Installing R and RStudio We will start by installing R – which is the programming language we will be using to learn how to visualize data – as well as RStudio – which is an integrated development environment for R. In case you’re not sure what the difference between one thing and the other is, think of R as the actual computer or engine behind all the calculations and operations you will be performing, while RStudio is the beautiful and very helpful interface that makes it all much more enjoyable and streamlined as a programming/ analysis experience. To install R, go to the following webpage &gt; https://cloud.r-project.org/ &lt; and select your operating system as well as your R version. As the page will tell you, you want to go for base R if this is your first install. Once you’ve installed R, open it and let’s get familiarized with some basic concepts and operations, as well as with the way R syntax generally works. R serves first and foremost as a computing tool, that is, as a tool for calculating things. To get started, type in your R console 2 + 2, as shown in the code box below, and see what happens. To run the code in R, either press Enter or select the bit of code and press Ctrl + Enter. Below the inputted code you should see the result of your operation, which in this case is straightforward and not in any sense unexpected. 2 + 2 ## [1] 4 In R you are able to perform the same arithmetic operations you’d be able to perform in any programming language, such as subtraction 2 - 2, multiplication 2 * 2, or division 2 / 2. You are also able to use logical operators to check whether a given relation between two or more elements is true or false. Suppose you want to know whether 2 is larger than 3, you can check that by running the code below. 2 &gt; 3 ## [1] FALSE Unsurprisingly, the output of the code above tells us that 2 is not greater than 3. Other important logical operations include checking whether two elements are equal to one another 2 == 3, not equal to another 2 != 3, or whether the relation between multiple sets of elements or multiple individual operations is true or false. For instance, notice the difference between the output of the two code snippets below. 2 &gt;= 3 | 3 &gt;= 3 ## [1] TRUE 2 &gt;= 3 &amp; 3 &gt;= 3 ## [1] FALSE While in the first case the output of the joint logical operation is true, in the second case it is false, and this has to do with the usage of the operators | and &amp;. What these operators do is to evaluate different combinations of individual outputs. In both cases, the individual operations which get evaluated consist of checking whether a given value is equal to or higher than 3. | indicates that the output of either one or the other individual operation should be equal to or greater than 3, while &amp; indicates that the output of both individual operations should be equal to or greater than 3. These sorts of logical operations will come in very handy later when we manipulate data structures consisting of lists of several values. Now, other than performing basic operations such as the ones above, more importantly for our purposes we will use R to manipulate and organize data in ways which help us discover and highlight patterns which we deem interesting as data analysts. More than that, we will use R to create visual representations of data so as to help us communicate any interesting data patterns to an external audience. In this course, we will learn how to communicate data to an audience which is academic or to some extent or another familiar with the goals and interests of scientific communication. But first let’s learn some more about the basics in R. This is a good point to bring in RStudio, which will only enhance our experience as R data analysts. You can download RStudio here &gt; https://rstudio.com/products/rstudio/download/. Note that you need to have base R installed to be able to run RStudio. Once you’ve installed RStudio, let’s have a look at the interface it offers us. RStudio interface In a nutshell, RStudio has 4 panels or regions of interest, as shown in the picture above. At the top left you have the region where you will usually have an R script, which is the document in which you type and record your R commands, the bits of code that will generate an output after the R engine has done its magic. This panel need not necessarily house a plain code script, as we will see it can also contain a document which gets converted into a reader-friendly file, such as a PDF or an HTML. But that’s where you will type things and organize the logic of your analyses/ programs. Right below that is the RStudio console, which, much like the console in base R, showcases the output of whatever chunks of code you run. Any time you ask R to output some data summary or to run an operation this is where you will see it, usually in a condensed format given the running window character of the panel. Use the console to inspect what your code does but do not type your commands straight into it as the console will not save anything that is displayed in it. At the bottom right you have a multi-purpose panel which is usually used to view any graphical representations of your data, your plots, but also to view and locate any files or to read the always-handy entries from the different sources of R documentation. Last, at the top right you will find a panel where you can see any data structures you create or load during your R sessions, including data sets, R variables, temporary functions and shortcuts you create and much more. This is where most of your data management will occur, so eventually you will learn to keep your environment minimally clean and tidy. 1.2 Creating data structures Now that we know where to write code in RStudio and where to look for the output and representation of our operations, let’s have a look at how we can create and manipulate different types of data in R. Let’s bring back our basic arithmetic operation from above, 2 + 2. It’s fine simply running the operation as this chunk of code currently does, but usually you will want to access the output of one operation and feed it to another operation, or maybe you will want to store it locally for purposes of record-keeping and data management. Let’s then store the result of 2 + 2 in a new variable, which we can call op1, short for operation one. You can do that in R by assigning an operation or an already existing code output to a label, an unique identifier which stands for your newly created variable. Simply type the name of your new variable followed by an arrow and the operation or code output you wish to assign to it. op1 &lt;- 2 + 2 Now, in order to check if the assignment was sucessful, you can try calling your new variable. Type in op1 and run the code to see what happens. You should see the result of the operation 2 + 2, which is what you stored under the label op1. Notice that calling op1 only shows the actual output of the operation, and not the source of that output. This is an important aspect of variable assignment. op1 ## [1] 4 Before we move on to more interesting cases of data manipulation, let’s have a look at what happens if you try to manipulate the variable you created. There’s two things we can do here. We can use op1 in a new operation without modifying the variable itself, which works by using the value associated with op1 without changing its representation: op1 + 2 ## [1] 6 Alternatively, we can store the result of our manipulation of op1 in a new variable, which we can call op2. Now we have two variables which we can manipulate and call back whenever we want. Notice how those two variables are now visible in your environment panel on the right, which means that they are retrievable within your current R session. If you wish to store your progress for future work, however, you should save an R script with the code used to create op1 and op2. When you open that script in a future R session and run the respective code, the variables will be created anew and will become visible in your environment panel. op2 &lt;- op1 + 2 op2 ## [1] 6 As you can imagine, storing the output of simple operations like the one above might not be that useful in itself. Usually, you will want to manipulate more complex variables consisting of lists of several elements, for instance several numbers or tokens of a given category. Let’s create a list of numbers and do some more basic calculations with it. To create a list of elements, called a vector in R, you can use the command c() as shown below: numbers &lt;- c(2, 3, 5, 9) numbers ## [1] 2 3 5 9 As per usual, calling your variable shows the elements stored in it. In our toy example, this is a vector with four numbers. You can inspect any R object, whether a single variable or more complex data structures as we will construct later, by using the function str(). Try that yourself and see what it tells you about the vector numbers. str(numbers) ## num [1:4] 2 3 5 9 str() is useful in two ways: first, it tells you what type of an object your variable is. This is something that you will usually know if you create the variable yourself – in fact, it is something that you should know, as it will make your life easier later on – but in case you don’t know or can’t remember, str() is a good method of finding it out. Then, str() will also show you the length of your object, in this case a vector of length 4, i.e. a vector composed of 4 elements. Let’s turn our focus to object types. In the case of numbers, str() diagnoses the vector as being numeric, which of course means that it is composed of numbers. But numeric is not the only vector type in R, there is in fact a number of other vector types, all of which are primitive data structures in R, which is to say that they act as building blocks for more complex data structures. The most important primitives for us are: Logical vectors, which contain elements that are marked as either true or false, most likely stored as the output of some logical operation or as a reference to a binary category (lv &lt;- c(TRUE, FALSE, FALSE, TRUE)); Numeric vectors, which contain elements that are numbers, including numbers with decimals (nv &lt;- c(10, 15.6, 4, 7.9)); Integer vectors, which contain elements that are whole numbers (iv &lt;- c(7, 15, 2, 57)); Character vectors, which contain elements that are strings of text, including orthographic representations of numbers and Arabic numerals without the underlying number representation in R (cv &lt;- c(\"butterfly\", \"5\", \"four\", \"false\")). One important and useful characteristic of vectors in R is that they can be converted into a nominal variable regardless of the actual vector type. Factors as they are called in R are vectors that have labels associated with each unique element stored in them. This might not seem particularly useful at this point, but it will be when we start dealing with meaningful data. For now let’s just remember that a factor consists of a mapping between nominal values, which are arbitrary labels, and a set of elements. Notice that there might be more elements than unique labels, depending on the number of repeated elements. The code below converts numbers into a factor and checks the structure of the converted vector. numbers &lt;- as.factor(numbers) str(numbers) ## Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;5&quot;,&quot;9&quot;: 1 2 3 4 Vectors can be combined to create larger R objects. If multiple vectors of the same type are combined, the result is a matrix, a two-dimensional data structure with multiple rows and columns. If multiple vectors of different types are combined, the result is a data frame, which is also a two-dimensional data structure. Another useful type of data structure is an array, which is similar to a matrix, i.e. it consists of a single element type, but can have an n number of dimensions. The code below creates a new vector composed of letters, a character vector called letters. When you create a character vector, you need to use quotation marks to demarcate each individual element, as shown below. We then check the structure of the newly created vector in order to make sure that the object type is correct. Finally, we combine numbers and letters into a data frame which we call df1, and use str() to get some more details about the newly created data frame. letters &lt;- c(&quot;a&quot;, &quot;f&quot;, &quot;m&quot;, &quot;t&quot;) str(letters) ## chr [1:4] &quot;a&quot; &quot;f&quot; &quot;m&quot; &quot;t&quot; df1 &lt;- data.frame(numbers, letters) str(df1) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ numbers: Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;5&quot;,&quot;9&quot;: 1 2 3 4 ## $ letters: Factor w/ 4 levels &quot;a&quot;,&quot;f&quot;,&quot;m&quot;,&quot;t&quot;: 1 2 3 4 By inspecting our new data frame we can see that letters, originally a character vector, is now a factor, each individual letter consisting of a label, or as R calls it, a level, of its own. There is an explanation for this: when creating a data frame R converts character vectors to factors by default. This can lead to confusion later on when you manipulate your variable, now part of a data frame. In case the automatic conversion is unintended, you can avoid it by specifying the argument stringsAsFactors in the data.frame() function. NOTE: Starting with R version 4.0, released in June 2020, data.frame() does not convert character vectors to factors by default anymore. If you have installed R from scratch right before working on this session, you will be using R version 4.0 or newer, which means your default is stringsAsFactors = FALSE. Functions in R, like data.frame(), have arguments, which are slots that get filled with both mandatory and optional information. Think of arguments as switches that can be manipulated in order to produce slightly different outputs with the same function call. In the code below, we change stringAsFactors from its default TRUE to FALSE in order to avoid the conversion of our character vector. Notice the difference in the output. df1 &lt;- data.frame(numbers, letters, stringsAsFactors = FALSE) str(df1) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ numbers: Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;5&quot;,&quot;9&quot;: 1 2 3 4 ## $ letters: chr &quot;a&quot; &quot;f&quot; &quot;m&quot; &quot;t&quot; 1.3 Subsetting and manipulating data Having created our toy data frame df1 let’s play around with it for a little while. Let’s start by calling our data frame, this time without using the usual str() command. Given that df1 is a minuscule data set, it should be no problem visualizing it straight in the console. Alternatively, you could click on the data frame object in your environment panel to have a full view of the data, which should come in handy when we deal with larger data sets with dozens or hundreds of rows/ columns. df1 ## numbers letters ## 1 2 a ## 2 3 f ## 3 5 m ## 4 9 t As you can see, our two variables, numbers and letters, are organized as columns in the data frame, with the individual values organized vertically bellow their respective variable names. You have probably noticed this before, but data frames have a tabular format, which is only possible given that each of our variables has the exact same number of elements or values. Now suppose we want to add more information to our data set, suppose we want to record whether the values associated with our two existing variables are presented in the right order, numbers in ascending order and letters in alphabetical order. We can start by creating a variable called order_numb, which indicates whether the number value of a given observation is given in the correct ascending order. Let’s then merge that with df1 and save the resulting data frame with the same name, df1. order_numb &lt;- c(TRUE, TRUE, TRUE, TRUE) df1 &lt;- data.frame(df1, order_numb) df1 ## numbers letters order_numb ## 1 2 a TRUE ## 2 3 f TRUE ## 3 5 m TRUE ## 4 9 t TRUE Now let’s create a fourth variable called order_let, which indicates whether the letter value of a given observation is given in the correct alphabetical order. This time, however, let’s use an alternative to merging the vector and the data frame together, which is what data.frame() does, and so let’s add the vector order_let straight to the already existing df1 data frame. Notice the usage of the operator $. order_let &lt;- c(TRUE, TRUE, TRUE, TRUE) df1$order_let &lt;- order_let df1 ## numbers letters order_numb order_let ## 1 2 a TRUE TRUE ## 2 3 f TRUE TRUE ## 3 5 m TRUE TRUE ## 4 9 t TRUE TRUE We now have order_numb and order_let, two logical vectors, indicating whether the number and letter values of a given observation are ordered properly. However, if we think about it, these two variables are in a sense instances of the same superordinate variable, something we could call order. This is a very good moment to introduce the concept of tidy data, which will be very important for us as we get our hands on real data. The picture below illustrates what tidy data looks like. Tidy data As we can see, aside from each row reflecting an individual observation and each cell consisting of an individual value, in a tidy data set the columns relate to individual variables. That is not really the case in df1, if we consider that order_numb and order_let both relate to the same variable. Let’s then change how our data frame is organized. For that, we will need special tools, which we will borrow from a very handy R toolkit known as the tidyverse. The tidyverse is a collection of packages – small extensions to base R which feature extended or improved function(alities)s – which greatly improve the tasks of everyday data analysis. Some of its packages, including dplyr, tidyr, and ggplot2, will be the bread and butter of practically any data analytic endeavor in R. To install the tidyverse, call install.packages(\"tidyverse\"). Once installed, any R package can be loaded by calling library(). Packages need only be installed once, however, each new session they need to be loaded anew if to be used in that session. A best practice in R programming is to load any necessary packages right at the beginning of an R session/ script, before they are actually needed later in the body of the script. For now, we will need the function pivot_longer() from the tidyr package, but we will load the tidyverse as a whole, which will prevent us from having to load other individual tidyverse packages later. Let’s then use pivot_longer() to reshape our data frame, moving our two order_ variables and their respective values into a new arrangement that reflects the common basis between order_numb and order_let. pivot_longer() allows us to elongate our data set, increasing the number of rows and decreasing the number of columns. In this particular case, we end up with the same number of columns as we started, but had we had more than two order_ variables the resulting number of columns in df1_long would have been smaller than in df1. library(tidyverse) ## -- Attaching packages -------- tidyverse 1.3.0 -- ## v ggplot2 3.3.2 v purrr 0.3.4 ## v tibble 3.0.3 v dplyr 1.0.2 ## v tidyr 1.1.2 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.5.0 ## -- Conflicts ----------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() df1_long &lt;- pivot_longer(df1, cols = starts_with(&quot;order&quot;), names_to = &quot;order&quot;, values_to = &quot;correct&quot;) df1_long ## # A tibble: 8 x 4 ## numbers letters order correct ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 2 a order_numb TRUE ## 2 2 a order_let TRUE ## 3 3 f order_numb TRUE ## 4 3 f order_let TRUE ## 5 5 m order_numb TRUE ## 6 5 m order_let TRUE ## 7 9 t order_numb TRUE ## 8 9 t order_let TRUE As you will notice as you start coding in R, usually there are multiple alternative ways of achieving the same desired end result. One alternative to our reshaping operation as coded above involves using the so-called pipe operator %&gt;%, which is one of the many handy tools that come with the tidyverse. If we inspect our code, more specifically the arguments in the pivot_longer() function, we see that the original data frame, df1, is called inside the function as its first argument. What that means is that pivot_longer needs to know which data frame is to be reshaped, the specific instructions of how exactly to reshape it being given in the arguments names_to and values_to. Now, in many functions you will need to specify which data source is the object of whatever operation the function is supposed to perform. If you are performing a single operation at a time, specifying the data source in the function argument is fine, however, if you are to perform several distinct operations, all sharing the same data source, chaining those operations might be a more convenient way of doing things. This is where the pipe operator comes in. Using %&gt;% to ‘pipe’ a data source into one or more operations is a great way of saving yourself typing and writing a more fluid, though arguably less explicit, chunk of code. Run the code below and check whether the output differs from the code chunk above. df1_long &lt;- df1 %&gt;% pivot_longer(cols = starts_with(&quot;order&quot;), names_to = &quot;order&quot;, values_to = &quot;correct&quot;) df1_long ## # A tibble: 8 x 4 ## numbers letters order correct ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 2 a order_numb TRUE ## 2 2 a order_let TRUE ## 3 3 f order_numb TRUE ## 4 3 f order_let TRUE ## 5 5 m order_numb TRUE ## 6 5 m order_let TRUE ## 7 9 t order_numb TRUE ## 8 9 t order_let TRUE Now let’s learn how to subset our data frame. Depending on what it is we want to do, we might want to extract a whole variable, a certain number of observations, or a single data value. Suppose we want to inspect the numbers variable without calling the whole data frame. We can that do by using either of the options below. df1_long$numbers ## [1] 2 2 3 3 5 5 9 9 ## Levels: 2 3 5 9 df1_long[, 1] ## # A tibble: 8 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 ## 3 3 ## 4 3 ## 5 5 ## 6 5 ## 7 9 ## 8 9 Note how the subsetted values are the same, though the output is presented in a slightly different way. In the first case we resorted to the $ operator, which we had used before already to add a variable to our data frame. Here, the $ operator is used to single out an individual variable from the data frame. In the second case we resorted to squared brackets [,], which are used to specify which rows and columns from our data frame, a tabular data structure, we want to extract. The 1 after the comma stands for the column: that tells R to extract the 1st column, i.e. the numbers column, from df1_long. Notice how before the comma nothing is specified: that tells R to extract all rows from the data frame. An alternative to that would be to explicitly specify rows 1 to 8 df1_long[1:8, 1], which would result in all 8 rows being extracted, exactly the same result as above. Now, suppose you only want to extract the first two rows of numbers. For that, we would specify our rows [1:2, x] and then our column [x, 1]. df1_long[1:2, 1] ## # A tibble: 2 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 What if we only want to extract a single value from df1_long? For that, we should specify the precise coordinates of that value, say 1st column and 4th row [4, 1]. df1_long[4, 1] ## # A tibble: 1 x 1 ## numbers ## &lt;fct&gt; ## 1 3 These are all the basic ways of extracting bits of data from a data table in R. In fact, these are the base R ways of doing it. But we also have the tidyverse tools at our disposal, which make some of these operations more streamlined. Let’s repeat the same operations from the code chunks above but now using functions from the dplyr package. First, let’s extract the numbers variable from df1_long, incorporating the %&gt;% operator as well. Notice how you can do that by either specifying the column number or the column name. df1_long %&gt;% select(1) ## # A tibble: 8 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 ## 3 3 ## 4 3 ## 5 5 ## 6 5 ## 7 9 ## 8 9 df1_long %&gt;% select(numbers) ## # A tibble: 8 x 1 ## numbers ## &lt;fct&gt; ## 1 2 ## 2 2 ## 3 3 ## 4 3 ## 5 5 ## 6 5 ## 7 9 ## 8 9 You can, of course, also select more than a single variable at a time. Suppose we wanted to extract both numbers and letters. For that we could use either of the options below. df1_long %&gt;% select(1:2) ## # A tibble: 8 x 2 ## numbers letters ## &lt;fct&gt; &lt;chr&gt; ## 1 2 a ## 2 2 a ## 3 3 f ## 4 3 f ## 5 5 m ## 6 5 m ## 7 9 t ## 8 9 t df1_long %&gt;% select(numbers, letters) ## # A tibble: 8 x 2 ## numbers letters ## &lt;fct&gt; &lt;chr&gt; ## 1 2 a ## 2 2 a ## 3 3 f ## 4 3 f ## 5 5 m ## 6 5 m ## 7 9 t ## 8 9 t Now let’s use a dplyr function to extract only values that meet a certain criterion. Say we want to extract only the observations that have a number value above 2. We can try the code below, using the filter() function. df1_long %&gt;% filter(numbers &gt; 2) ## Warning in Ops.factor(numbers, 2): &#39;&gt;&#39; not meaningful for factors ## # A tibble: 0 x 4 ## # ... with 4 variables: numbers &lt;fct&gt;, letters &lt;chr&gt;, order &lt;chr&gt;, ## # correct &lt;lgl&gt; This didn’t work the way we wanted, but luckily R throws error warnings to help us understand what might have gone awry. According to the warning, the greater-than operator &gt; is not meaningful for factors. Let’s try to make sense of that. First, recall that the numbers variable is a factor, which means that the numbers are only stored as labels and not as actual numeric representations. Now the operation we are trying to perform only makes sense when applied to numeric or integer vectors, given that a nominal value cannot be greater than another nominal value. Let’s then try to convert numbers back to a numeric vector and attempt to perform the operation again. as.numeric(df1_long$numbers) ## [1] 1 1 2 2 3 3 4 4 Even before we have the chance to try the extraction operation again, we can see that something went wrong with the factor conversion. The resulting vector does not seem to show the original numeric values but rather the labels associated with them in the factor representation. If we look for help in the documentation on factor, we find the following note: as.numeric applied to a factor is meaningless, and may happen by implicit coercion. To transform a factor f to approximately its original numeric values, as.numeric(levels(f))[f] is recommended Let’s then do what the R documentation recommends before we attempt to perform our original extraction operation again. as.numeric(levels(df1_long$numbers))[df1_long$numbers] ## [1] 2 2 3 3 5 5 9 9 df1_long$numbers &lt;- as.numeric(levels(df1_long$numbers))[df1_long$numbers] df1_long %&gt;% filter(numbers &gt; 2) ## # A tibble: 6 x 4 ## numbers letters order correct ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 3 f order_numb TRUE ## 2 3 f order_let TRUE ## 3 5 m order_numb TRUE ## 4 5 m order_let TRUE ## 5 9 t order_numb TRUE ## 6 9 t order_let TRUE Now the factor conversion works seamlessly, and so does the subsequent extraction operation. Our failed attempts should serve as a reminder that one should always know what type of data they are trying to manipulate. In many cases, we won’t exactly know, for example, why a given operation won’t work on a vector of type factor or numeric. Such situations will usually require some research about the exact operator or function we’re trying to use, but overall knowing how our variables of interest are stored and represented in R is a must to any process of data manipulation and analysis. This will be particularly true when the data we deal with have meaningful counterparts in a study design, such as a group of participants or a particular type of measurement. Let’s try one more trick before summarizing what we’ve learned so far. Currently, numbers is organized in an ascending order. But what if we wanted to display our number values in a descending order? We could create a new numbers vector with the reverse order, and then replace the old vector with the new one, but that is a pretty roundabout solution. Instead, we can use the dplyr function arrange(), as shown below. df1_long %&gt;% arrange(desc(numbers)) ## # A tibble: 8 x 4 ## numbers letters order correct ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 9 t order_numb TRUE ## 2 9 t order_let TRUE ## 3 5 m order_numb TRUE ## 4 5 m order_let TRUE ## 5 3 f order_numb TRUE ## 6 3 f order_let TRUE ## 7 2 a order_numb TRUE ## 8 2 a order_let TRUE This seems to work just fine. Of course not always will we have a handy tidyverse function like arrange() to quickly solve our problems, but many recurring wrangling operations have been turned into functions to simplify data analysis in R. This is the exact reason why we use the tidyverse, as its packages make our job as data analysts much easier in many instances. This is not to say that one needs to rely on these packages and functions, in fact we can even create our own functions or use coding solutions that we might know from other programming languages. The point is to realize which options work best for our individual purposes, considering also the constrains of the data we have at hand. In the next session of our course, before we learn how to plot data, we will talk about how to map data reflecting some of the measures typically found in behavioral research onto the data structures we can create and manipulate in R. For now, let’s recapitulate the key points that we learned in today’s session. 1.4 Session summary In order to create a variable or assign something to an already existing variable we use the arrow operator &lt;-, e.g. op1 &lt;- 2 + 2; In order to create a vector we can use the function c(), where c() contains the values or elements to be created, e.g. nv &lt;- c(10, 15.6, 4, 7.9); In order to quickly gather information about a vector or any other data structure we can use the function str(), e.g. str(nv); In order to convert any vector into a factor, which stores individual values as unique labels, we can use the function as.factor(), e.g. as.factor(nv); In order to create a data frame, a tabular data structure made up of vectors of various types, we can use the function data.frame(), e.g. data.frame(nv, cv); We learned that we should strive to keep our data tidy. In order to do that, we can perform several operations, such as elongating a data table with pivot_longer, selecting and filtering variables with select() and filter(), or better arranging variables with arrange(); Finally, we learned that we can feed a data source into one or more operations using the pipe operator %&gt;%, e.g. nv %&gt;% filter( &gt; 4). 1.5 Exercises Convert df1_long back to its original, wide format. Try using the pivot_wider() function from the tidyr package. Remove the variable letters from df1. Try using select(), and for that inspect the function documentation using the Help tab on the lower right panel of RStudio. Find observations that have a numbers value above 4 and then add 4 to those values. Chain those two operations together using the pipe %&gt;%. Add a new variable to the data frame called order_both. This should be a logical vector that indicates whether both order_numb and order_let are true. If so, order_both should be TRUE; if not, then FALSE. Check the function mutate() and use it to create the new variable. "],
["plotting-and-visualizing-data-in-r.html", "Session 2 Plotting and visualizing data in R 2.1 Measurement levels in behavioral research 2.2 Why visualize data? 2.3 Plotting data using ggplot2 2.4 Session summary 2.5 Exercises", " Session 2 Plotting and visualizing data in R 2.1 Measurement levels in behavioral research Before we start plotting data using R, it is important to discuss how the different types of data objects found in a programming language such as R might relate to the levels of measurement usually found in behavioral research. Widely diverse strands of research, conducted across different fields of the social and cognitive sciences, rely on behavioral data as their main object of measurement and analysis. Depending on the specific research area and methodology employed, such data can be collected using experiments, questionnaires and surveys, as well as a myriad of other methods. Regardless of the specific data collection method employed, the considerations we discuss below apply to various measures found in behavioral research, from decision-making to information and linguistic processing, encompassing both measures of real-time processing as well as measures of offline judgment and interpretation. The examples we will be using in the remainder of this course come from psycholinguistics, more specifically from two studies on non-literal language, and feature both online and offline measures. In behavioral research, traditionally a distinction has been made between four basic levels of measurement, namely nominal, ordinal, interval, and ratio. Just by looking at the names of the measures themselves, we can see that these levels have rather direct counterparts in the data types found in R (and in other programming languages for that matter). In practice, however, it is important to realize that the mapping between data objects in R and different levels of measurement is not necessarily straightforward, as these four basic measure types can be extended to many other categories and sub-types, which may bring with them assumptions of their own. One must always understand what measure scale their data best corresponds to, so as to encode it appropriately both when creating data, for example for simulation purposes, and when working with data imported from external sources. Let’s start by distinguishing between the four basic types of measures. Nominal measures are best understood as categories or instances of any sort of classification where each value represents one category or class. The most basic type of nominal measures consists of dichotomous data, where there are only two categories and every token must be classified according to either one or the other category. There can also be multiple categories, each of them identified by a unique identifier, a label which usually consists of a name or a string of letters or numbers. Example in study design: In an experimental study, individual participants as well as individual stimuli items are understood as nominal measures for the purposes of data and statistical analysis. Individual conditions reflecting a certain experimental manipulation also consist of nominal measures, as well as questions in a questionnaire or survey. Representation in R: Nominal measures should be coded as unordered factors, the labels reflecting unique strings. No arithmetic operations can be performed on nominal measures, yet, one can perform logical operations like checking whether two values are the same or whether a certain token is included in a given category. Ordinal measures are similar to nominal insofar as they represent categories, however, these categories are naturally ordered or ranked, as in a grading or rating scale. Even though the order of the categories is important, only the actual points on the scale are meaningful, the intervals between them not being captured by the scale. Despite ongoing discussion about the matter as well as variation across disciplines, the median is regarded as the most appropriate measure of central tendency to be applied to ordinal data. Example in study design: In an experimental study, any sort of scale presented to a participant or respondent consists of an ordinal measure. Each point on the scale is represented by a label, the labels being organized in an order which is either theoretically or methodologically meaningful. Scales need not necessarily consist of several points, they can also be dichotomous, as in judgments of truth value. Representation in R: Ordinal measures should be coded as ordered factors, the labels following a specific order which can be specified by the analyst. The same sorts of operations that can be performed on nominal measures can be performed on ordinal measures. Interval measures reflect an underlying scale where the degree of difference between the individual points on the scale is meaningful. Intervals may contain arbitrary reference points, such as a zero/ starting point, which can be considered an absolute minimum. Example in study design: In an experimental study, reaction times are interval measures. Zero is the absolute minimum, as the measurement begins only when a certain stimulus or motor/ verbal action is triggered and the measured reaction times cannot be negative. Representation in R: Interval measures should be coded as numeric. Any sort of arithmetic operation can be performed on interval measures. The mean is the most commonly applied measure of central tendency to interval measures. Ratio measures reflect the relationship between two units of measurement, for example a count of a certain type of token in a given time window or its proportion to another type of token. Ratios contain non-arbitrary zero values, which makes it meaningful to say, for instance, that there are are twice as as many tokens of event or category x compared to event or category y. Example in study design: In an experimental study, counts or proportions of responses are ratio measures. Representation in R: Ratio measures can be coded as either numeric or integer depending on the corresponding variable in the study design. These four basic types of measurement illustrate only a portion of the possible measure scales found in behavioral study designs. The coarse-grained distinction presented here suffices for the purposes of our course, however, these considerations should also serve to highlight how important it is to understand how the variables and measurements of a study relate to the objects used in a programming language to represent and analyze the data originating from the measurements made in the study. So, in a nutshell, knowing your data is crucial to any attempt to analyze. 2.2 Why visualize data? Our main goal for today is to learn how to produce visual representations of data. As researchers concerned with analyzing empirical data, we are interested, primarily, in two things: first, we want to understand what the meaningful and relevant patterns in a given data set are; then, we want to be able to highlight and communicate those patterns to an audience who is not directly involved in the process of analyzing the data, be it our peers or other parties involved in the larger process of scientific communication. These two aspects of the data analytic process can be regarded as distinct steps or stages in any analysis pipeline, as they involve different goals and practices. Let’s call the first of them data exploration and the second data communication. Data exploration involves wrangling and plotting the data in different ways, according to different dimensions, in an attempt to uncover as many facets of the data as possible. In the case of confirmatory research, it might involve investigating the data beyond the scope of the originally formulated hypotheses. The main point at this stage is to gather as much and as detailed information about the data as possible, without placing much emphasis on making plots visually or conceptually transparent nor on refining them aesthetically. As a result of exploring the data, one should be be able to tell, qualitatively at least, what the main aspects of interest of a data set are, and which of these should be further analyzed by means of statistical analyses or techniques other than visualization proper. Data communication, on the other hand, involves taking the insights gained from the exploration of the data – and possibly too from any statistical modeling – and packaging it in such a way that people not involved in the analysis of the data are able to understand what it says about the investigated research questions. This involves highlighting the most important aspects of the data and making sure the main (differences between) measurements are presented in a visually salient and understandable way. Communicating data visually, however, is not an easy task: there will always be a trade-off between clarity and fidelity to the data, especially in scenarios where the underlying data is complex and the data patterns of interest are convoluted or non-trivial. Often enough, complex study designs will necessitate complex visualizations, though any data set, no matter how simple, will demand optimizing and refining visualizations. Having said that, making decisions about what aspects of the data to highlight or omit in a visualization is an integral part of the data analytic process, and as such should be regarded as one of many possible degrees of freedom that a researcher has, along with aspects related to the design of the study, the choice of a data elicitation and data collection method, as well as any decisions related to the statistical modeling of the data. Let’s keep in mind these considerations as we get familiar with the data we are starting to plot in the section below. 2.3 Plotting data using ggplot2 Now that we’ve discussed what we might be aiming for as researchers visualizing data, let’s get familiar with the actual data we are going to be using throughout our course. The data we are visualizing comes from two psycholinguistic studies on metonymy and metaphor. Both metonomy and metaphor are forms of non-literal language which, despite their common idiomatic nature, differ from one another conceptually as well as in terms of their degree of non-literalness. To begin with, let’s take a look at what such idiomatic expressions might look like. As the author of the studies has put it: In a metaphor such as to swim against the current, what is said (current in a body of water) stands for something literally unrelated (a ‘general trend’). In a metonymy such as to have an eye for detail, eye refers to something that is literally or immanently related or part of the same concept (i.e., ‘ability to see details’). (Michl, 2019a, pp. 98-99) The studies we are concerned with thus include two types of idiomatic expressions: Metonymic expressions etw. unter vier Augen besprechen (to discuss something among four eyes = to discuss something in private) mit leeren Händen kommen (to come with empty hands = to arrive empty-handed, without contribution) Metaphoric expressions die Hand für jmd. ins Feuer legen (to put one’s hand in fire for somebody = to vouchsafe for somebody’s virtue) noch feucht hinter den Ohren sein (to still be moist behind the ears = to be young ad inexperienced) In the first of the two studies, Michl (2019a) investigated how non-literalness might be linked to other properties relevant to the linguistic and cognitive characterization of idioms, such as transparency and familiarity. For that, she had participants rate different German idioms along a number of dimensions, later using those ratings to predict idiom type in an inferential statistical model. In the second study, Michl (2019b) had participants read some of the idioms used in her first study in a task that involved reading the stimuli one chunk at a time. The idioms used in the second study were first rated on their non-literalness by an independent sample of participants, and that is what we turn our attention to now. In order to understand the extent to which the expressions presented to participants in the reading experiment were perceived as non-literal, Michl (2019b) first had an independent pool of participants rate the experimental items according to their degree of non-literalness. As such, the author asked participants to rate both literal expressions as well as the target idiomatic expressions on a scale from 1 to 4, where 1 stood for an expression that was not at all literal and 4 for an expression that was completely literal. There were two actual studies: pre-study 1 focused on the metaphoric expressions while pre-study 2 focused on the metonymic expressions. Let’s get started with plotting the results, first the ones from pre-study 1. For that, we are using the tidyverse package ggpplot2. In order to produce a visual object with ggplot2, all we have to do is to provide a data source, then tell ggplot2 how to map the variables of interest to the plot aesthetics, and finally tell ggplot2 which graphical primitives to use. You can think of ggplot2 plotting as a layered process which requires all these basic elements just described. We start with the blank canvas onto which we will be adding elements to build our graphical representations of the data. Let’s call an empty ggplot() function and see what it does. ggplot() You might be surprised at the output, but this is as much as our chunk of code does: it lays out a blank canvas. What we need to do in order to add content to this blank plot is to specify our coordinate system, or in other words, which variables should be plotted where. Let’s then try to add the ratings to the plot. For that, let’s add our data source to the code above and let’s specify which axis we want to assign the rating variable to – let’s say the x-axis. In order to do this, we need to specify some aesthetics within the ggplot() call. The aesthetics tell ggplot() which variable to map onto which coordinate. ggplot(data = pre1, aes(x = rat)) As we can see, we now have the x-coordinate in place – we can already see the rating scale with the variable name rat as the axis title, as well as some of the points on the scale plotted beneath the axis, with vertical lines extending from them. Now let’s add the last essential element to the plot, a geometric object, or geom in R lingo. Aside from aesthetic mappings, ggplot() needs geoms in order to plot data onto the canvas. Once all three elements are in place, the basic requirements for the call are met, and the plot is built. ggplot(data = pre1, aes(x = rat)) + geom_bar() Here we used the geom geom_bar(), which plots the data using a bar chart. Notice how ggplot() generated the chart by using the counts of responses for each point on the scale. However, as it stands, the plot only tells us about the responses as an overall function of the points on the scale, aggregated over all data, and not about their relation to the two types of expressions we are interested in. Remember that what we were interested in knowing was what the distribution of responses would look like when taking into account the type of expression rated, either metaphoric or literal. Let’s then add that information to the plot. What we are going to do is to specify another aesthetic, this time within our geom_bar() call. More specifically, we will add color to our plot to differentiate between the two expression types, and we will do so by using fill. All there is to do here is to map a variable onto the fill aesthetic, which is embedded within the geom_bar geom. ggplot(data = pre1, aes(x = rat)) + geom_bar(aes(fill = typ)) We now see the same response distribution as above with the addition of color sections to the count bars, each color referring to one expression type. This allows us to distinguish what the relevant patterns in the data are, namely that people rated the metaphoric expressions, plotted in red/ pink, mostly as not at all literal, while rating the literal expressions, plotted in blue, mostly as completely literal. Despite now being more nuanced and informative than before, arguably the graph has also become somewhat convoluted: one needs to parse both the ‘height’ of the bars as well as their color composition in order to understand what their relation to each point on the rating scale is. One simple way to modify this default rendition while keeping the relevant color distinction in place is to plot the individually colored sections of the bars as individual bars. Let’s tell ggplot() to do exactly that by ‘dodging’ the position of the bars. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(fill = typ), position = position_dodge()) In this re-designed plot, we now see the initial bars split in terms of the type of expression they represent. This way, it is easy to inspect not only the overall distribution of responses per expression type but also the differences in terms of particular points on the scale. Next, what we’ll do to this already quite informative plot is to change the y-axis. Instead of showing the overall frequency of responses, let’s plot their relative frequency, that is, as proportional to the number of responses for each expression type. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;, position = position_dodge()) This seems to have worked the way we intended: the proportions are being calculated relative to each expression type. In this plot, one can associate a visual pattern to a quantity which is immediately meaningful. Just by glancing at the plot one can quite easily see that people rated metaphoric expressions as not at all literal in about 80% of the cases. We could also have plotted the ratings proportional to the overall number of responses, though this is not what we were interested in, from the point of view of our empirical and data analytic goals. This might be useful, however, in other cases. Notice the differences in distribution if we had opted for this alternative plotting. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = (..count..)/sum(..count..), fill = typ), position = position_dodge()) Now, usually there are several ways of achieving the same, or at least similar, ends in R. In the graph sequence above, we used color as well ‘dodged’ or split bars to visualize the ratings in terms of our two groups of interest, the metaphoric and the literal expressions. We opted for plotting the responses in a single graph, which means that the data points are plotted sharing the same axes as wel as the same visual real estate at the center of the plot. Notice how we also relied on a legend to be able to tell which expression type was which. One alternative to this rendition is to use facets, an option provided in ggplot2 for splitting observations or groups of observations into different windows or panels within a single graph. facets work in a very simple way, you simply specify what variable(s) the data should be split by, and how the facets are to be plotted, horizontal or vertical to one another. Here we tell ggplot() to split the data by the expression type and to plot the facets horizontal to one another. pre1 %&gt;% ggplot(aes(rat)) + geom_bar() + facet_wrap( ~ typ) This rendition offers a different way of visualizing the same results as above: we can see the distribution of ratings for both the metaphoric and the literal expressions. With the facets it is easy to see that the metaphoric expressions are rated mostly as not at all literal while the literal expressions are rated mostly as completely literal. Although the two facets share the same y-axis, they are clearly divided in terms of expression type, which is indicated on top of each facet by means of a label. Even if we are not exploring any customization options right now, you can probably already tell that with ggplot2 virtually any aspect of a visualization can be altered or enhanced: here, for example, we could change the position and aesthetics of the facet labels, or the range and aesthetics of the axes and ticks. More importantly for now, notice how we are facetting our data in terms of a single variable, type, though facetting is also possible with two variables, in which case there are more degrees of freedom in terms of how exactly to render the visualization in spatial terms. Much like we did before, let’s convert the y-axis to represent the proportion of responses by expression type instead of the raw response frequency. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop..), stat = &quot;count&quot;) + facet_wrap( ~ typ) Currently, our graph does not contain any color coding, which we replaced earlier with the facetting. This is not to say, however, that color cannot be added to a facetted plot. Let’s map the variable type onto a fill aesthetic, much like we did before. pre1 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;) + facet_wrap( ~ typ) Let’s now turn our attention to the data from pre-study 2, which compared metonymic and literal expressions. Let’s plot the ratings using our latest ggplot() code. pre2 %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ, group = typ), stat = &quot;count&quot;) + facet_wrap( ~ typ) The plot looks just like the one we had for pre-study 1, except, of course, that the data patterns are different here: while still rating the literal expressions mostly as completely literal, people rated the metonymic expressions mostly as fairly literal, in contrast to metaphors which were rated mostly as not at all literal. Now, in order to get a global picture of the combined results of the two norming studies, we would like to compare the results of pre-study 2 to those of pre-study 1. Let’s then plot the data together. In order to do that, we will have to merge the two individual data sets together, something which we can easily do in R using, for example, tools from the tidyverse package dplyr. We are using a function called bind_rows, which adds one data set right on top of the other. Notice how in the function call, in addition to the two data frames, we specify the argument .id, which gives us a label we can refer back to to know which data set each individual observation comes from. norms &lt;- bind_rows(pre1, pre2, .id = &quot;study&quot;) We successfully joined the two data sets, but we also got an error that due to unequal factor levels one of our resulting factors was coerced into character. Luckily for us, we know where this error might come from: while in the data set from pre-study 1 the factor type had the levels meta and woe, in the date set from pre-study 2 it had the levels meto and woe, which means that the resulting data frame has a factor with three levels as opposed to two. That is all fine, we just have to convert the variable type in the the new data frame to factor again. norms$typ &lt;- as.factor(norms$typ) Now that we have a single data set with observations from both the pre-study on metaphors and and the pre-study on metonomies, let’s plot the combined results. Let’s go back to facet_wrap() and try to change our code from earlier to produce a plot with four facets. norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;) + facet_wrap(study ~ typ) We were able to achieve what we wanted by instructing ggplot2 to facet the data by both the original data set, study, and the expression type, type. Notice how the order of the arguments in the facet_wrap() call impacted the resulting plot: study was plotted along the x-axis while type was plotted along the y-axis. The graph is already good as it is, but let’s redesign the facet labels to aid visualization. For that, we’ll create two vectors with the new names we want to assign to the labels. Then, we’ll use the labeller() function from ggplot2 to apply those new labels to the already existing plot. study_labels &lt;- c( `1` = &quot;Study 1&quot;, `2` = &quot;Study 2&quot; ) type_labels &lt;- c( meta = &quot;Metaphoric&quot;, meto = &quot;Metonymic&quot;, woe = &quot;Literal&quot; ) norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), stat = &quot;count&quot;) + facet_wrap(study ~ typ, labeller = labeller(study = study_labels, typ = type_labels)) In order to conclude our introduction to ggplot2, let’s attempt to visualize the combined results with our split bar technique from earlier, exploring an alternative visualization possibility. norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, labeller = labeller(study = study_labels, typ = type_labels)) This is an interesting plot: it subsumes the ratings for each expression type within a single facet which tells us which study the ratings come from. Despite the usage of the split bars though, the plot is somewhat convoluted, as there is a lot of visual information in each facet. Let’s try to modify this graph so as to keep both distributions but only focus on a single expression type per facet. For that, we’ll actually plot the graph twice, once highlighting the literal results and once highlighting the non-literal results. First, let’s create two vectors with the color highlight information: for the literal facets, we color the literal responses in their original color and the non-literal responses in gray; for the non-literal facets, we color the non-literal responses in their original color and the literal responses in gray. In both cases, we follow the original coloring scheme, plotting our graph once with each coloring information. literal &lt;- c( &quot;woe&quot; = &quot;grey&quot;, &quot;meta&quot; = &quot;#F8766D&quot;, &quot;meto&quot; = &quot;#00BA38&quot;) non_literal &lt;- c( &quot;woe&quot; = &quot;#619CFF&quot;, &quot;meta&quot; = &quot;grey&quot;, &quot;meto&quot; = &quot;grey&quot;) literal_norms &lt;- norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, ncol = 1, labeller = labeller(study = study_labels, typ = type_labels)) + scale_fill_manual(values = non_literal) print(literal_norms) non_literal_norms &lt;- norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, ncol = 1, labeller = labeller(study = study_labels, typ = type_labels)) + scale_fill_manual(values = literal) print(non_literal_norms) Now that we have two plots, each with their own color highlight, we merge them together into a single plot. This time, we will use a function from an external package, namely the cowplot package, as ggplot2 doesn’t offer us exactly the tools we need. We use a function called plot_grid(), which is being called from the package cowplot via cowplot::plot_grid(), and to it we add the two plots we just created. Notice how within the call we specify that no legend should be plotted. This is the case as we’ll generate a common legend below. plot &lt;- cowplot::plot_grid( non_literal_norms + theme(legend.position = &quot;none&quot;), literal_norms + theme(legend.position = &quot;none&quot;), nrow = 1) print(plot) Now, all we need to do is to produce a common legend for our new plot, and then to combine the plot with its legend. In order to do that, we generate the plot we started with, where there was no custom color information, only so we can get the legend with the right colors in it. We then use the useful get_legend() function from cowplot to extract the legend. Finally, we produce our final plot using plot_grid() again, now with the legend included. legend_norms &lt;- norms %&gt;% ggplot(aes(rat)) + geom_bar(aes(y = ..prop.., fill = typ), position = position_dodge()) + facet_wrap( ~ study, labeller = labeller(study = study_labels, typ = type_labels)) print(legend_norms) legend &lt;- cowplot::get_legend(legend_norms) cowplot::plot_grid(plot, legend, rel_widths = c(2, .3)) 2.4 Session summary In order to produce a plot with ggplot2 three basic elements are needed: the ggplot() call which sets the canvas, a data source and the basic coordinate aesthetics, and a geometric object, also known as a geom; geoms are graphical primitives which are used to plot different types of graphs. In today’s session, we only used geom_bar(), which plots bar plots, but ggplot2 offers a myriad of other geoms, some of the most useful for our purposes being geom_point() for scatter plots, geom_box() for box plots, and geom_violin() for violin plots. We will use some of these other types of geoms later in our course. As we will see, different geoms allow us to plot different types of variables as well as their corresponding interactions; aesthetics allow us to add more elements to a plot, on top of the base geom specifications. They also allow us to customize our plots in different ways. We saw how to add color to a plot using fill but also how to create subplots using facet_wrap(). We also made some adjustments to our base plots specifying the position and labeller aesthetics; We learned that with ggplot2 there are usually several ways of achieving the same desired ends. We will talk more about clarity and relevance in data visualization in the next sessions, but already at this point it’s good to realize that the same data, with the same intended visualization goal, can be visualized in a variety of ways, often to the same utility to the target audience. Although there are some principles which are generally preferred across different visualization contexts, ultimately many of the design choices are dependent on the personal goals and taste of the analyst. Standards also vary from one field of research to another, but the main takeaway is that we should aim at a trade-off between clarity and functionality that best suits our particular needs as analysts visualizing a specific data sets. You can find a summary of the essential ggplot2 functions on this Cheatsheet. 2.5 Exercises fill is not the only way to add color to plots using ggplot2. Using the documentation, check what the aesthetic color does and then try to replace fill with color in one of the plots above. If you’re feeling curious, try using both fill and color. In the plots where we plotted proportions, the scale on the y-axis shows ticks at every increment of 0.2, starting at 0. Replot one of the graphs above such that there are ticks at every increment of 0.1 instead. Try using the function scale_y_continuous, and read on the options for the break aesthetic. There are two functions used for facetting in ggplot2: facet_wrap(), which we’ve already used, and facet_grid(). Read on facet_grid() and try to use it to replot one of the graphs from above. "],
["exploring-and-communicating-patterns-in-rating-data.html", "Session 3 Exploring and communicating patterns in rating data 3.1 Laying out the data 3.2 Exploring the data 3.3 Enriching the base visualizations 3.4 Modifying major elements of the plot 3.5 Refining minor elements of the plot 3.6 Session summary", " Session 3 Exploring and communicating patterns in rating data 3.1 Laying out the data Last session we got an introduction on how to use ggplot2 to plot data in R. We saw that with the basics in place, which are the data to be plotted as well as at least one geom, a graphical primitive in R, we can create graphs which allow us to visualize our data, being up to us as analysts to decide what specific format and what design choices best suit our visualization needs. Today we will dig deeper into how to visualize data using gpplot2, first by seeing some examples of how to explore different patterns of interest in a data set and then by creating visualizations which not only highlight the most relevant patterns in the data but which also look refined and visually pleasing. We will be visualizing data from the first of the two studies introduced last session, in which the primary question of interest was whether metaphors and metonomies, both of which forms of non-literal language, differ in their perceived degree of literalness. Recall that in this study the author asked groups of people to rate both types of idiomatic expressions along a number of dimensions tied to literalness, namely familiarity, transparency, and non-literalness. Let’s focus our attention on the ratings for transparency, which encompasses two distinct properties, comprehensibility and relation. As the author of the study puts it, comprehensibility, as a feature of transparency, relates to “[…] the ease with which the meaning of an idiomatic unit can be recovered” (Michl, 2019a). In addition to comprehensibility, the relatedness between the literal and non-literal meaning of an idiom also matters for its perceived transparency. Indeed, as the author explains in the paper, the notion of relation “[…] defines the strength of the semantic or conceptual link between the literal and the idiomatic meaning of an idiom”. With these two notions in mind, let’s then turn to the ratings themselves. As we can see, comprehensibility and relation are theoretical constructs which capture the nature of idiom transparency from two different angles, yet they may or may not overlap with one another in the ratings measured empirically. In the study we are concerned with, people were asked to rate 244 different idioms. As a reminder of what these idioms might look like, let’s take a look at an example of each type of expression, taken from the sample used in the study. Metaphor - das Eis brechen - to break the ice = to relax a socially stiff/ uncomfortable situation Metonomy - die Nase rümpfen - to wrinkle one’s nose = to be contemptuous or disgusted Now, let’s start plotting the comprehensibility ratings, which range from 1, extremely difficult to understand, to 5, extremely easy to understand. For that, let’s recycle some code from last session, where we already plotted similar ratings using colored bars to indicate the different expression types. Instead of plotting the raw numbers, let’s make sure to plot the proportion of ratings as a function of the expression types, which is a more reader-friendly plotting option compared to using raw counts. rat_comp %&gt;% ggplot(aes(x = rat)) + geom_bar(aes(y = ..prop.., fill = type, group = type), stat = &quot;count&quot;, position = position_dodge()) As the plot shows us, at least on the surface, the two types of expression look very similar in terms of their perceived comprehensibility. Given the selected sample of idioms, respondents seem to find, on average, both metaphors and metonomies quite easy to understand. And yet this finding is is not necessarily transparent when one looks at the plot. On the one hand, it is obvious by looking at it that most of the rating mass is concentrated on the upper end of the scale, over points 4 and 5. On the other hand, however, it is not immediately evident what the central tendency of the response distribution is. Since we are talking about ordinal data here, our preferred measure of central tendency is the median, which seems to be around 4, but this is a guess based on eyeballing the data and roughly estimating it. Before assuming that 4 is the actual median rating, let’s calculate the median directly. For that we will turn to our trusted dplyr package from the tidyverse, which we’ve already used to wrangle data before. So what we want to do here is to calculate the median for each group of interest. Medians can be calculated in R using the function median(), which according to the documentation can be used to compute the sample median and is to be applied to a numeric vector containing the values whose median is to be computed. Now, since we want the median of each group, and not just the overall sample median, we need to group the data. Thanks to dplyr, that is easily done using group_by(), inside of which we specify our grouping variable, in this case type. Next, what we need to do is to call our summary statistic function, median(), inside of a summarize() call, which creates a new data frame with our summary of interest. Let’s try that. rat_comp %&gt;% group_by(type) %&gt;% summarize(median = median(rat)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## type median ## &lt;chr&gt; &lt;dbl&gt; ## 1 metaphoric 4 ## 2 metonymic 4 The calculated summary statistic confirms our rough guess that the median rating of both metaphors and metonomies is 4. But this still leaves us with the question of how to improve our visualization such that this pattern of interest becomes more noticeable. There might be ways of changing the current plot so as to make the medians stand out, but one more practical solution is to completely re-think our choice of a graph. Notice how we’ve been using the geom_bar() geom so far, but maybe there are better alternatives in the geoms available in ggplot2. Let’s explore a little. What we have done so far is to plot one variable, namely rat, the resulting graph showing bars either with the raw response counts or with different proportions of responses according to each expression type, which we achieved using fill(). What if we tried to assign type to one of the axes instead of using fill() to add it to the graph? Let’s try that, and let’s use another geom instead, since geom_bar() only plots one variable. We can start by giving geom_point() a try, which plots each response as a point. Let’s assign type to the x-axis and rat to the y-axis. rat_comp %&gt;% ggplot(aes(type, rat)) + geom_point() Our code does exactly what we tell it to do: it plots the ratings on the y-axis and the groups as two distinct ‘columns’ on the x-axis. But why does the plot look so weird, as if there was only one data point for each point on the rating scale? Well, the reason is that geom_point() is useful for displaying the relationship between two continuous variables, as would be the case for interval or ratio data. What we have at hand is two discrete variables, one ordinal and one dichotomous, so our data points are in fact all there, but, since our rating scale consists of discrete points, the responses are overplotted. Let’s use another geom to declutter the overplotted graph and reveal the underlying response points. We’ll use geom_jitter(), which according to the documentation adds a small amount of random variation to the location of each plotted point, such that it is a useful way of handling overplotting caused by discreteness. rat_comp %&gt;% ggplot(aes(type, rat)) + geom_jitter() With geom_jitter() we’re able to see all the responses, which are clustered around each point on the rating scale. Remember that we were interested in being able to spot where most of the rating mass lies on the scale, so it might make sense to plot the scale horizontally, so one can easily notice the data spread for each expression type as well any differences between the two groups, which will be plotted one on top of the other. Let’s try to achieve that by reversing our x and y arguments in the ggplot() function. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_jitter() This jittered plot looks more intuitive than the previous one, but even after reversing the axes we still end up with a plot which is not particularly informative; if anything our original barplot was more informative. Let’s try another solution then, namely another geom, geom_count(). According to the documentation, geom_count() is a variant of geom_point() which counts the number of observations of each location and then maps the count to a point area. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_count() This again somewhat resembles our original barplot insofar as we get a visual correlate of the amount of responses for each point on the rating scale. Still, this plot is arguably much less informative than the original barplot. So do we settle for a barplot or do we try some more alternatives? Let’s try some more alternatives. So far we’ve tried geoms which take either two continuous variables or two discrete variables. ggplot2 also has geoms which take one discrete and one continuous variable. Even though the ratings we are plotting are ordinal in nature, they are represented in R as a numeric variable, which we can double-check via a str() call. str(rat_comp$rat) ## num [1:16492] 4 2 2 1 1 2 3 3 2 1 ... R represents vectors containing numbers as numeric by default, regardless of what these numbers actually represent. We could have converted rat to a factor to reflect the nature of its original measurement type, but keeping it as a numeric vector was useful for calculating the median earlier on. Still, what that means is that we are plotting rat as a continuous variable, which is useful for the geom we’re using next, geom_boxplot(). Boxplots, or box-and-whisker plots as they are also called, are a very useful way of graphically representing numeric data when one is interested in the quartiles or percentiles of the data distribution. The median itself is the 50th percentile, that is, the point which divides the data distribution in two. The lower and uppers quartiles represent the 25th and the 75th percentiles, respectively. Let’s take a look at our graph and see how all that is represented in a boxplot. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot() What this plot tells us is that for both expression types 4 corresponds to the 50th percentile of the data, that is, to the median point in the rating distribution. This is marked by a thick vertical black line in the plot. Moreover, the plot tells us that 3 and 5 are the 25th and 75th percentiles, respectively, which means that half of the data mass lies between these two points. These percentiles are marked by thin vertical black lines, which enclose the area colored in white and halved by the median line. The lines extending from the ‘boxes’ in the graph, the so-called ‘whiskers’, indicate values that lie within a deviation of 1.5 x the range marked by the 25th and 75th percentiles, which is called the interquartile range. Importantly for our current purposes, such a plot gives us both our desired measure of central tendency as well as information about the spread of the data, all readily available as part of the geom itself. Before we move on, let’s look at one last geom, geom_violin(). A plot produced with geom_violin() resembles a boxplot except that it has the shape of a density distribution, which may or may not come in handy. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_violin() In our case, such a violin plot is not particularly informative, and that is because our data, despite being represented as continuous, actually consists of discrete response bins clustered around each of the five points of the rating scale. We will therefore stick with our boxplot from above, which was the most informative graph for our purposes, perhaps alongside the barplot. Now that we have settled on the type of graph we will be using for our visualization, let’s plot the other transparency measure, the relation ratings. rat_rel %&gt;% ggplot(aes(rat, type)) + geom_boxplot() Given the selected sample of idioms, respondents seem to find that both in the case of metaphors and metonomies their literal and non-literal meanings are neither extremely distantly or not at all related nor extremely closely related. That is to say that, on average, the literal and non-literal meanings of both types of expressions is perceived as not particularly closely or distantly related, which may be expected given the large number of items in the study and inherent variability between items. Now that we have both the comprehensibility and the relation ratings, we might want to combine them in a single plot, given that both measures relate to the notion transparency. In order to do that, we will create a graph that shows the correlation between the two response distributions on the basis of visual overlap between the data. First, we need to combine the two data sets, so we can have a single data frame to feed to ggplot(). rat_combi &lt;- bind_rows(rat_comp, rat_rel, .id = &quot;study&quot;) Now that we have a combined data frame, let’s plot it using a boxplot, just like we did above. In order to highlight the different measures, we will need to add the variable study to the plot, which we created in our bind_rows() call above. If we inspect rat_combi, we’ll see that study has 1 and 2 as default IDs for our two transparency measures, so we should probably change that to something more meaningful, to make our lives easier. We’re using a function called fct_recode() from the tidyverse package forcats. Note that study is not actually coded as a factor in our data frame but rather as character, which is the R default for vectors containing strings. That is not a problem in itself as we can still use fct_recode() to change the labels associated with the values recorded in study. Let’s call the comprehensibility measure comp and the relation measure rel. str(rat_combi$study) ## chr [1:37138] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... rat_combi$study &lt;- fct_recode(rat_combi$study, comp = &quot;1&quot;, rel = &quot;2&quot;) str(rat_combi$study) ## Factor w/ 2 levels &quot;comp&quot;,&quot;rel&quot;: 1 1 1 1 1 1 1 1 1 1 ... Now let’s plot the combined ratings, just like we did above with only one measure. Here, we will add the information about the measures as a fill() within our geom_boxplot() call. Note how we are using color here to highlight the source of the ratings, or in other words, what measure they refer to, while we are mapping our expression type directly onto the y-axis. rat_combi %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = study)) With this plot, we can directly compare our two transparency measures. Of course, we could also reverse the current visual scheme and map the study source to the y-axis and use color to indicate the expression type, like below. This second rendition might be more informative if we would like to highlight both the contrast between the expressions types and the one between the measures themselves. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type)) One thing left to do before we move on is to make some minor visual improvements to the plot. Later today we will make further changes to these plots to increase their legibility and clarity, but for now let’s simply dodge the boxes a little bit, so they’re not so close to one another. Within our geom, we can specify the same argument we specified last session when working with barplots, namely position = position_dodge(). Since the boxes in boxplots are already slightly ‘dodged’ by default, we can increase the distance between them by modifying the argument width within position_dodge(). rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), position = position_dodge(width = .9)) We’ve increased the distance between the boxes in each group, but what if we also wanted to increase the distance between the two groups? Well, we can then modify the width argument within geom_boxplot(), which is a separate argument as the one nested within position_dodge(). Let’s try that out. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), width = .9, position = position_dodge(width = .9)) This didn’t work the way we wanted to, and that’s because we’re now dealing with two width parameters, so we need to adjust them accordingly until we find a balance that suits our needs. Let’s reduce the distance between groups. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), width = .5, position = position_dodge(width = .9)) We’re getting there, but we should probably reduce the distance between the paired boxes a tiny bit as well. rat_combi %&gt;% ggplot(aes(rat, study)) + geom_boxplot(aes(fill = type), width = .5, position = position_dodge(width = .7)) This plot looks good. By inspecting it, one can see that there are no differences in how the two idiom types are perceived, neither in terms of their comprehensibility nor in terms of the relation between their literal and non-literal meanings. There is, however, a difference between the two transparency measures, such that both metaphors and metonomies are perceived, on average, as easy to understand despite their literal and non-literal meanings not being perceived as particularly related. Let’s now explore the data a bit more to see if there are other interesting patterns worth highlighting and communicating. 3.2 Exploring the data Before working on improving our plots, we’ll explore our data some more. A detailed account of the hows and whys of exploring data, as part of a data analytic pipeline, is something that goes much beyond the scope of this course and which is directly tied to the specific practices and standards of a research field as well as to specific methods employed to measure and analyze data. Having said that, it’s important to understand what the goal of data exploration might be, and how it might fit within the larger picture of data analysis. Analysis of empirical data can be roughly divided into exploratory and confirmatory strands. With exploratory analyses one attempts to describe and summarize the main characteristics of a data set, as a representative snapshot of a given phenomenon, possibly with the intention of generating hypotheses for further empirical testing. In a study which is exploratory in nature, the researchers do not state specific hypotheses about the data they are analyzing, despite having a research question and specific empirical and possibly theoretical goals. With confirmatory analyses, on the other hand, also known as hypothesis testing, one attempts to answer a research question which is formulated in terms of a specific hypothesis or sets of hypotheses, usually accompanied by an [explicit] alternative hypothesis or sets of alternative hypotheses. In a study which is confirmatory in nature, the researchers attempt to confirm hypotheses which they formulate before collecting and analyzing the data. In practice, data exploration and hypothesis testing are complementary aspects of empirical research, such that exploring one data set might lead to generating hypotheses, and possibly building theories, which can be tested explicitly using another data set. While testing hypotheses which are different from the ones formulated beforehand is considered bad practice in confirmatory research, data exploration is something that can and is expected to happen in addition to hypothesis testing, either cumulatively across different studies which rely on different data sets or as part of a single study before any hypotheses are generated and tested. The study we are focusing on today can be characterized as exploratory, and as such exploring the data beyond the main contrasts of interest might be particularly fruitful. Given that we are not experts in the particular empirical/ theoretical domain the study is situated in, generating hypotheses for future empirical testing would probably be wishful thinking, and it also escapes the purpose of this exercise. We can, nevertheless, look at the data and see if there’s any interesting descriptive pattern worth highlighting. Given that we have already explored the transparency ratings to some degree in the previous section, let’s look at some of the other variables which are recorded in the data set. Let’s use our trusted str() function to generate an overview of the data frame. Alternatively, we can also view the data by clicking on it in our environment panel on the top right of RStudio. str(rat_combi) ## tibble [37,138 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ study : Factor w/ 2 levels &quot;comp&quot;,&quot;rel&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ part_id: num [1:37138] 460 460 460 460 460 460 460 460 460 460 ... ## $ gender : chr [1:37138] &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ age : num [1:37138] 31 31 31 31 31 31 31 31 31 31 ... ## $ state : chr [1:37138] &quot;Niedersachen&quot; &quot;Niedersachen&quot; &quot;Niedersachen&quot; &quot;Niedersachen&quot; ... ## $ educ : chr [1:37138] &quot;Hoch/Fachhochschul&quot; &quot;Hoch/Fachhochschul&quot; &quot;Hoch/Fachhochschul&quot; &quot;Hoch/Fachhochschul&quot; ... ## $ item : num [1:37138] 1 2 3 4 5 6 7 8 9 10 ... ## $ rat : num [1:37138] 4 2 2 1 1 2 3 3 2 1 ... ## $ type : chr [1:37138] &quot;metaphoric&quot; &quot;metaphoric&quot; &quot;metaphoric&quot; &quot;metaphoric&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. part_id = col_double(), ## .. gender = col_character(), ## .. age = col_double(), ## .. state = col_character(), ## .. educ = col_character(), ## .. item = col_double(), ## .. rat = col_double(), ## .. type = col_character() ## .. ) In addition to our main variables of interest rat and type, which we are already familiar with, we also see other variables which relate to the study per se, such as part_id and item, as well as basic demographic information about the respondents, like age, gender, and educ. Even though we don’t know what a theory of non-literal language might say about the relation between perceived idiom transparency and traits like gender or age of a language user, we might expect there to be differences in how, for example, young and older language users perceive the comprehensibility of metaphors, or in how users with a higher or lower level of formal education perceive the relation between the literal and non-literal meaning of such expressions. Let’s then start by plotting our ratings from above in terms of different sub-populations of our sample. To simplify our job for now, let’s plot only the comprehensibility ratings, and let’s use color to help us visualize any differences between our two expression types. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) To this plot, we’re adding a facet_wrap() with gender as a grouping variable. Let’s also move the legend to underneath the plot, so it doesn’t take up so much space on the right-hand side. For that we need to add a new layer to our ggplot() call, one in which we can specify details about the plot. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ gender) + theme(legend.position = &quot;bottom&quot;) As the graph suggests, there seem to be no systematic differences in terms of how men and women perceive the comprehensibility of both metaphors and metonomies. Let’s now plot the ratings in terms of age. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ age) + theme(legend.position = &quot;bottom&quot;) This is an interesting plot: what we see is the ratings facetted in terms of all the 34 different ages reported by respondents. Just by glancing at the graph, we see that there is variability in terms of how people of different ages rated the idioms, on average metaphors and metonomies being rated similarly, despite different patterns in some of the facets. Let’s now collapse the ages into more representative age groups, such as how they are usually presented in surveys. For that, we will need to wrangle our data some more. What we’ll do is that we’ll create a new variable, called age_group, which is based on groupings of younger, middle-aged, and older respondents. For that, we’re using the function mutate() from dplyr, which we first encountered in the exercises from session 1. Within our mutate() call we’re using another function called case_when(), which basically allows us to vectorize multiple if-else statements. Let’s see how that works. rat_comp &lt;- rat_comp %&gt;% mutate(age_group = case_when(age &gt;= 18 &amp; age &lt;= 24 ~ &quot;18-24&quot;, age &gt;= 25 &amp; age &lt;= 54 ~ &quot;25-54&quot;, age &gt;= 55 &amp; age &lt;= 64 ~ &quot;55-64&quot;, age &gt;= 65 ~ &quot;65+&quot;)) What we did was to create age_group on the basis of grouping of values from age, such as specified in the call. We can now replace age with age_group in our code and see how the new plot looks like. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ age_group) + theme(legend.position = &quot;bottom&quot;) This plot shows that, despite variability in some of the facets in the plot above, the ratings are the same across three of the four binned age groups, the exception being adults who are older than 65. All in all, the median comprehensibility of both metaphors and metonomies is still 4 on the 5-point scale. Let’s now try to combine this plot with the earlier plot where we grouped participants by gender. For that, we need to add gender to our facet_wrap() call. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap(gender ~ age_group) + theme(legend.position = &quot;bottom&quot;) This plot confirms what we had already seen separately in the gender and age plots, namely that, on average, the ratings are consistent across men and women as well as across different age groups. The outliers seem to be men who are older than 65. The current plot works well for the purposes of exploring the data, but in case we would like to have something which is a bit more transparent to other viewers, we might want to line the facets for each gender in their own rows. Let’s change that by specifying the number of rows in our facet_wrap() call. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap(gender ~ age_group, nrow = 2) + theme(legend.position = &quot;bottom&quot;) Note that if we had used facet_grid(), which is an alternative to facet_wrap(), we would have had achieved a similar end result without having to specify the number of desired rows, as the two functions work differently and have different arguments. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_grid(gender ~ age_group) + theme(legend.position = &quot;bottom&quot;) Just before we move on to improving the aesthetics of our plots, let’s try to explore the data in terms of one last variable, namely the specific idioms that participants rated. Before we plot the data, let’s double-check how many items there were in total. Let’s use the function unique(), which can be applied to our vector item to single out all unique values. unique(rat_comp$item) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 ## [217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 ## [235] 235 236 237 238 239 240 241 242 243 244 That means that we have 244 items in total, which might be complicated to plot. Let’s give it a go in any case and see what we get. We’re adding item as a grouping variable within facet_wrap(). rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot(aes(fill = type)) + facet_wrap( ~ item) + theme(legend.position = &quot;bottom&quot;) The resulting plot is, as expected, simply impossible to parse. We will have to find some alternative way of exploring the items. What we are interested in, ultimately, is in having an overview of the amount of variability in the rated idioms. Since we know that the median rating is 4, perhaps we can use that to break down the responses into smaller groups. Let’s calculate, for each expression type, the relative number of items rated as more than 4 and less than 4. For that, we first group the items by type. Then, we create a variable called median_split, which indicates whether the rating for a given item is equal to or smaller/ larger than the median. We then group our data by type and our new variable median_split and summarize the relative number of occurrences. rat_comp %&gt;% group_by(type) %&gt;% mutate(median_split = case_when(rat &gt; 4 ~ &quot;5&quot;, rat &lt; 4 ~ &quot;1,2,3&quot;, rat == 4 ~ &quot;4&quot;)) %&gt;% group_by(type, median_split) %&gt;% summarize(n = n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` regrouping output by &#39;type&#39; (override with `.groups` argument) ## # A tibble: 6 x 4 ## # Groups: type [2] ## type median_split n freq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 metaphoric 1,2,3 3768 0.355 ## 2 metaphoric 4 2950 0.278 ## 3 metaphoric 5 3908 0.368 ## 4 metonymic 1,2,3 2064 0.352 ## 5 metonymic 4 1648 0.281 ## 6 metonymic 5 2154 0.367 The information we generated tells us, in numbers, what we had already inferred from our graphical representation of the data, namely that most of the rating mass lies on the upper end of the scale, over points 4 and 5. In order to get more detailed information about each item, we’d have to add item to our grouping call. rat_comp %&gt;% group_by(type) %&gt;% mutate(median_split = case_when(rat &gt; 4 ~ &quot;5&quot;, rat &lt; 4 ~ &quot;1,2,3&quot;, rat == 4 ~ &quot;4&quot;)) %&gt;% group_by(item, type, median_split) %&gt;% summarize(n = n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` regrouping output by &#39;item&#39;, &#39;type&#39; (override with `.groups` argument) ## # A tibble: 732 x 5 ## # Groups: item, type [244] ## item type median_split n freq ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 metaphoric 1,2,3 3 0.0469 ## 2 1 metaphoric 4 13 0.203 ## 3 1 metaphoric 5 48 0.75 ## 4 2 metaphoric 1,2,3 11 0.169 ## 5 2 metaphoric 4 26 0.4 ## 6 2 metaphoric 5 28 0.431 ## 7 3 metaphoric 1,2,3 26 0.4 ## 8 3 metaphoric 4 19 0.292 ## 9 3 metaphoric 5 20 0.308 ## 10 4 metaphoric 1,2,3 12 0.185 ## # ... with 722 more rows What we now get is a detailed view of how each item was rated according to our tripartite division of the data around the median. From here on, in order to further explore the items, we’d have to conduct a more thorough inspection of the numbers in the table. We could look, for instance, for those items that received similar ratings, and then inspect the actual idioms to see if there are any commonalities between them. This is, of course, something that requires not only a qualitative view of the data but also expert knowledge about the actual target domain. Because of that, we’re not exploring this data any further, but this should serve to show that data exploration consists of a mix between visualization, wrangling, and numerical/ statistical summarizing. The best place to start is, just liked we did, with plots and graphical representations of the data, but usually several iterations between different techniques will be required to get at interesting and relevant data patterns. 3.3 Enriching the base visualizations Having laid out some base plots and explored the data along some of its secondary dimensions, we’re now in the position to decide what aspects of our data we want to highlight and communicate to our audience. In order to do that, we are going to go through the steps of producing camera-ready visualizations of the data, taking into account what we learned about it and what we deem interesting to be communicated. As a reminder, we are dealing with categorical data, where each ‘value’ consists of a category, in our case one point on a 5-point psychometric scale measuring properties related to idiom literalness. More specifically, our data is ordinal, meaning that the different categories are ranked, as the task participants were asked to perform consisted of rating, for example, the comprehensibility of each idiom they read using the provided 5-point scale. Now, our main goal is to highlight any potentially systematic differences between the participant ratings as a function of the experimental conditions. As we saw, the main contrast between metaphors and metonomies seems to hold across different sub-populations of our sample, the minor differences found in terms of age perhaps not being relevant to the point that they deserve being communicated in a plot of its own. Because of that, let’s focus on communicating the main results, namely the lack of systematic difference in comprehensibility between metaphors and metonomies, and the fact that both were rated, on average, as quite easy to understand, a 4 on the 5-point scale. Before we plot our data, let’s double-check how it’s represented in R. str(rat_comp$rat) ## num [1:16492] 4 2 2 1 1 2 3 3 2 1 ... As we can see, the ratings are coded as numeric, although we know that they represent, in fact, ordinal data. We managed to plot the data before without any problems, but let’s see what happens if we convert rat to a factor and try to plot it then. rat_comp$rat &lt;- as.factor(rat_comp$rat) Let’s start by re-plotting the comprehensibility ratings using our basic and straightforward ggplot2 code from above. Remember that all we have to do to create a basic graphical representation of our data is to specify the required arguments in the ggplot() function, namely aes(), which requires a variable to be plotted to the x-axis – our experimental conditions, that is, the two idiom types – and one variable to be plotted to the y-axis – our ratings. We subsequently tell ggplot() to plot the data using boxplots, which we decided was going to be our geom of choice to highlight the patterns of interest in the data. rat_comp %&gt;% ggplot(aes(type, rat)) + geom_boxplot() That does not look good at all. The reason why we get this odd result is because we’re feeding geom_boxplot(), which takes a continuous and a discrete variable, with two discrete variables, somewhat of a similar problem to what happened to us earlier when we tried feeding geom_point() with the ratings. Let’s change the ratings back to numeric and try again. rat_comp$rat &lt;- as.numeric(as.character(rat_comp$rat)) rat_comp %&gt;% ggplot(aes(type, rat)) + geom_boxplot() This works just fine. Now that we have the basic data patterns laid out, let’s start refining our plot, little by little, so it looks increasingly better, both in terms of transparency and effectiveness but also in terms of aesthetics. The first thing we are going to do is to flip the boxplots, so they are plotted horizontally as opposed to vertically. Since our main goal with this visualization is to allow our audience to notice where most of the rating mass lies on the scale, it makes sense to plot the scale horizontally, so one can easily notice the data spread for each group but also any differences between groups, which will be plotted one on top of the other. Let’s do that by reversing our x and y arguments in the ggplot() function, just like we did earlier. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot() This works just as intended. In some cases, however, reverting the axes in the ggplot() function will not produce the intended result. Luckily, we can also flip the x, y coordinates without changing the order of the arguments within the ggplot() call. To understand how flipping coordinates works, notice what happens if we add the coord_flip() call to our code. rat_comp %&gt;% ggplot(aes(rat, type)) + geom_boxplot() + coord_flip() 3.4 Modifying major elements of the plot Now that we have our data plotted the way we would like it to be, let’s work on some further adjustments to the base plot which will greatly improve the effectiveness of our data communication. Remember that there are a couple of useful ways of highlight and contrasting visual elements. Relevant for this case is the manipulation of color and fill scales. First, let’s assign different fills to each experimental condition, which is the variable assigned to the y-axis. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot() Now each condition is represented by a different color. Let’s change the color palette and slightly increase the transparency of the filled colors. For the color itself, we need to specify a new palette in a function that refers to our fill scale, namely scale_fill_brewer(). For the transparency, we modify the alpha parameter within our geom. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7) + scale_fill_brewer(palette = &quot;Set2&quot;) Now let’s remove the gray background so as to make the plot somewhat cleaner. Let’s also thicken the lines which surround and extend from the boxplots, the boxplot whiskers, in order to create a sharper contrast between the boxes and the white background. For the lines, we need to adjust the lwd argument within geom_boxplot(), while for the background we’re applying a new theme to the plot, namely them_minimal(), which, among other things, removes the gray background. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() 3.5 Refining minor elements of the plot As it is, the plot looks pretty good already, the most important aspects of the underlying data appropriately represented and well clearly visible. Now we can fix minor issues which arguably only contribute cosmetically to the plot, though when considered together with the previous changes make an astounding impact to the overall quality of the resulting visualization. Firs, let’s get rid of unnecessary elements. Let’s remove the grid lines on the y-axis extending horizontally from the condition names. Let’s also get rid of the axis titles, which only clutter the plot, as well as the legend that appeared when we added fills to the plot. We’re doing all of those things using the theme() function, which we had already encountered before. theme() takes several arguments which relate to, for example, the legend of the plot, the scales, as well as the labels and titles. We’re specifying that we’d like to remove the title axes, as in axis.title.x = element_blank(), and also telling ggplot() to remove the major grids of the y-axis, which are always plotted by default and not always necessary. Finally, we’re also calling legend_position(), and specifying that the legend should be removed altogether. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;) Now let’s improve the quality of the labels on the different axes. Let’s make them more visible by increasing their size and making them bold. Notice how we specify axis.text.x = element_text(), and within that function we modify the relevant parameters. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14), axis.text.x = element_text(face = &quot;bold&quot;, size = 14)) The next step is to increase the distance between the labels and the actual graph. Again notice the usage of the arguments in the element_text() functions. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0))) Now, let’s capitalize the labels on the y-axis. There’s at least two options to do that. We could recode the underlying factor in rat_comp, though that would change the actual data representation in R. A less permanent solution is to tell ggplot() to replace the existing labels with new ones, which will not change the underlying data representation. rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0))) + scale_y_discrete(labels = c(&quot;metonymic&quot; = &quot;Metonymic&quot;, &quot;metaphoric&quot; = &quot;Metaphoric&quot;)) Last, let’s add a title to the plot. Note that in many cases titles need not be added to plots, as that can be done straight in the text/ presentation editor one is using (including R, as we will see later). rat_comp %&gt;% ggplot(aes(rat, type, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;metonymic&quot; = &quot;Metonymic&quot;, &quot;metaphoric&quot; = &quot;Metaphoric&quot;)) + ggtitle(&quot;Comprehensibility ratings&quot;) To finish up, let’s make similar changes to our combined plot, which will require some further adjustments. Let’s use the exact same code from above, replacing type with study on the y-axis and bringing the legend back. Let’s also center the title of our legend and give the plot the right name. rat_combi %&gt;% ggplot(aes(rat, study, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;metonymic&quot; = &quot;Metonymic&quot;, &quot;metaphoric&quot; = &quot;Metaphoric&quot;)) + ggtitle(&quot;Transparency ratings&quot;) Much like we did earlier, let’s adjust the distance between the boxes and the groups in the geom_boxplot() call. We should also fix the labels, both on the y-axis and in the legend. We already know how to change the distance between our boxes, in fact we can just recover our code from earlier. As for the labels, notice the new arguments in the scale_fill_brewer() call. rat_combi %&gt;% ggplot(aes(rat, study, fill = type)) + geom_boxplot(alpha = .7, lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_fill_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) One more thing to do: let’s play with transparency so as to highlight the contrast between the two plotted measures. Here we’re using a trick which is to specify that alpha should refer to an interaction between our two grouping variables, type and study, and then to set alpha values manually for the different combinations of the two variables. Let’s make the relation boxes look bright and the comprehensibility ones look dark. rat_combi %&gt;% ggplot(aes(rat, study, fill = type, alpha = interaction(type, study))) + geom_boxplot(lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_fill_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + scale_alpha_manual(values = c(1, 1, .4, .4), guide = FALSE) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) This worked as intended. In order to achieve our end result and thus highlight the contrasts of interest, we made use of color and transparency, combined with a vertical collocation of the two contrasted groups. We could have used other combinations of techniques, though. As we’ve seen, these can include, among other things, changing the color of our geoms, changing the geoms themselves, facetting groups, but also changing the size or orientation of the plotted elements. As an example of such possible variations, let’s use color instead of fill to highlight type. Let’s also change the color of the boxes to gray, to increase the visual contrast with the white background. This should also help us understand the difference between fill and color. rat_combi %&gt;% ggplot(aes(rat, study, color = type)) + geom_boxplot(fill = &quot;light gray&quot;, lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) What we did was to color the lines in the boxplot according to the expression type and to fill the boxes with a specified color. Notice how color varies as a function of the variable type while color, which is manually coded, remains constant across both groups. We can now export our plot, say if we’d like to save it locally on our machine as an image file. Let’s export the plot with transparency added to it as a .png file, which can be done using the ggsave() function. You might need to adjust the width and height arguments within ggsave() to get your plot to be exported with the right dimensions. rat_combi %&gt;% ggplot(aes(rat, study, fill = type, alpha = interaction(type, study))) + geom_boxplot(lwd = 1.2, width = .5, position = position_dodge(width = .7)) + scale_fill_brewer(palette = &quot;Set2&quot;, name = &quot;Idiom type&quot;, labels = c(&quot;metaphoric&quot; = &quot;Metaphoric&quot;, &quot;metonymic&quot; = &quot;Metonymic&quot;)) + scale_alpha_manual(values = c(1, 1, .4, .4), guide = FALSE) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), legend.title.align = 0.5, axis.text.y = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 0, r = 10, b = 0, l = 0)), axis.text.x = element_text(face = &quot;bold&quot;, size = 14, margin = margin(t = 10, r = 0, b = 0, l = 0)), plot.title = element_text(face = &quot;bold&quot;, size = 24, hjust = .5, margin = margin(t = 10, r = 0, b = 40, l = 0))) + scale_y_discrete(labels = c(&quot;rel&quot; = &quot;Relation&quot;, &quot;comp&quot; = &quot;Comprehensibility&quot;)) + ggtitle(&quot;Transparency ratings&quot;) ggsave(&quot;ratings-transparency.png&quot;) ## Saving 7 x 5 in image 3.6 Session summary As we’ve seen in our last two sessions, geoms are what shape the type of graph one ends up with when using ggplot2. The single most important aspect of a geom is what type and number of variables its aesthetics support. We’ve seen examples of geoms that take one or two variables. In the case of geoms that support two variables, like geom_boxplot(), we saw that these can consist either of two variables of the same type, be it continuous or discrete, or a combination of both. In order to produce a plot that renders the data correctly, one needs to understand how the plotted variables are coded in R, so that they match the variable type supported by the selected geom. One also needs to understand what the data corresponds to “in the wild”, that is, in terms of its original measurement type. This might impact how the data is plotted regardless of its actual representation in R. Knowing what variables a geom requires and whether one’s data matches those requirements is a good recipe for plotting data correctly; In order to explore data for the purposes of data analysis, we saw that a good starting point is with plots and visual representations of the data. Often times, however, one will need to wrangle the data in order to produce certain plots. In addition to wrangling, which can be done with the help of many specialized packages like tidyverse’s dplyr, one usually needs to summarize the data in different ways or to calculate different descriptive statistics on the basis of the raw data, which themselves can also added to plots in different ways. Exploring a data set will usually involve a combination of visualization, wrangling, and numerical/ statistical summarizing; In order to enrich ggplot2’s base visualizations, one can make use of different aesthetic specifications. As we’ve seen, these include specifications which affect major elements of a plot, such as fill, color and alpha, geom-specific arguments like lwd or linetype but also arguments which affect minor elements of the plot such as the legend, the title, and the axes, usually specified within the theme() layer. Importantly, as we will see later, where an aesthetic is specified can impact how the plots looks like: whatever is called within the ggplot() function will affect all layers in a plot, while aesthetics called within specific geoms will only affect that geom’s layer. In addition to specifying aesthetics, using facets or varying the mapping of variables onto the x, y coordinates can also help highlight certain contrasts in a graph; Plots can be exported from R using the function ggsave(), which by default saves the last plotted graph. ggsave() takes the arguments width and height which can be modified in order to change the dimensions of the saved plot. "]
]
